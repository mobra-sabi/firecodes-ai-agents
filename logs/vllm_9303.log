/home/mobra/aienv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 11-11 17:02:41 [__init__.py:241] Automatically detected platform cuda.
[1;36m(APIServer pid=215354)[0;0m INFO 11-11 17:02:45 [api_server.py:1805] vLLM API server version 0.10.1.1
[1;36m(APIServer pid=215354)[0;0m INFO 11-11 17:02:45 [utils.py:326] non-default args: {'model_tag': 'Qwen/Qwen2.5-7B-Instruct', 'host': '0.0.0.0', 'port': 9303, 'model': 'Qwen/Qwen2.5-7B-Instruct', 'dtype': 'float16', 'max_model_len': 4096, 'gpu_memory_utilization': 0.85, 'max_num_seqs': 8}
[1;36m(APIServer pid=215354)[0;0m INFO 11-11 17:02:57 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM
[1;36m(APIServer pid=215354)[0;0m WARNING 11-11 17:02:57 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
[1;36m(APIServer pid=215354)[0;0m INFO 11-11 17:02:57 [__init__.py:1750] Using max model len 4096
[1;36m(APIServer pid=215354)[0;0m INFO 11-11 17:02:58 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
/home/mobra/aienv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 11-11 17:03:06 [__init__.py:241] Automatically detected platform cuda.
[1;36m(EngineCore_0 pid=216423)[0;0m INFO 11-11 17:03:10 [core.py:636] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=216423)[0;0m INFO 11-11 17:03:10 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":16,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=216423)[0;0m INFO 11-11 17:03:11 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_0 pid=216423)[0;0m WARNING 11-11 17:03:12 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_0 pid=216423)[0;0m INFO 11-11 17:03:12 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen2.5-7B-Instruct...
[1;36m(EngineCore_0 pid=216423)[0;0m INFO 11-11 17:03:12 [gpu_model_runner.py:1985] Loading model from scratch...
[1;36m(EngineCore_0 pid=216423)[0;0m INFO 11-11 17:03:12 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700] EngineCore failed to start.
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700] Traceback (most recent call last):
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 492, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 80, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self._init_executor()
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 49, in _init_executor
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.collective_rpc("load_model")
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3007, in run_method
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 212, in load_model
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1986, in load_model
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.model = model_loader.load_model(
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 44, in load_model
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     model = initialize_model(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 465, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 183, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 316, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 641, in make_layers
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 318, in <lambda>
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     lambda prefix: decoder_layer_type(config=config,
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 229, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.mlp = Qwen2MLP(
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                ^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 72, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 649, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     super().__init__(input_size=input_size,
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 508, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     self.quant_method.create_weights(
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 193, in create_weights
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]   File "/home/mobra/aienv/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m ERROR 11-11 17:03:14 [core.py:700] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 226.38 MiB is free. Process 38525 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 10.14 GiB memory in use. Of the allocated memory 9.81 GiB is allocated by PyTorch, and 44.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_0 pid=216423)[0;0m Process EngineCore_0:
[1;36m(EngineCore_0 pid=216423)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_0 pid=216423)[0;0m     self.run()
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_0 pid=216423)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 704, in run_engine_core
[1;36m(EngineCore_0 pid=216423)[0;0m     raise e
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[1;36m(EngineCore_0 pid=216423)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 492, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 80, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_0 pid=216423)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     self._init_executor()
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 49, in _init_executor
[1;36m(EngineCore_0 pid=216423)[0;0m     self.collective_rpc("load_model")
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=216423)[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3007, in run_method
[1;36m(EngineCore_0 pid=216423)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 212, in load_model
[1;36m(EngineCore_0 pid=216423)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1986, in load_model
[1;36m(EngineCore_0 pid=216423)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_0 pid=216423)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 44, in load_model
[1;36m(EngineCore_0 pid=216423)[0;0m     model = initialize_model(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=216423)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 63, in initialize_model
[1;36m(EngineCore_0 pid=216423)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_0 pid=216423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 465, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(EngineCore_0 pid=216423)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 183, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 316, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_0 pid=216423)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 641, in make_layers
[1;36m(EngineCore_0 pid=216423)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_0 pid=216423)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 318, in <lambda>
[1;36m(EngineCore_0 pid=216423)[0;0m     lambda prefix: decoder_layer_type(config=config,
[1;36m(EngineCore_0 pid=216423)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 229, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     self.mlp = Qwen2MLP(
[1;36m(EngineCore_0 pid=216423)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 72, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_0 pid=216423)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 649, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     super().__init__(input_size=input_size,
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 508, in __init__
[1;36m(EngineCore_0 pid=216423)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 193, in create_weights
[1;36m(EngineCore_0 pid=216423)[0;0m     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(EngineCore_0 pid=216423)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
[1;36m(EngineCore_0 pid=216423)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=216423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=216423)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 226.38 MiB is free. Process 38525 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 10.14 GiB memory in use. Of the allocated memory 9.81 GiB is allocated by PyTorch, and 44.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1111 17:03:15.379023152 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=215354)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/bin/vllm", line 7, in <module>
[1;36m(APIServer pid=215354)[0;0m     sys.exit(main())
[1;36m(APIServer pid=215354)[0;0m              ^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 54, in main
[1;36m(APIServer pid=215354)[0;0m     args.dispatch_function(args)
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 50, in cmd
[1;36m(APIServer pid=215354)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
[1;36m(APIServer pid=215354)[0;0m     return __asyncio.run(
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
[1;36m(APIServer pid=215354)[0;0m     return runner.run(main)
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[1;36m(APIServer pid=215354)[0;0m     return self._loop.run_until_complete(task)
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
[1;36m(APIServer pid=215354)[0;0m     return await main
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1850, in run_server
[1;36m(APIServer pid=215354)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1870, in run_server_worker
[1;36m(APIServer pid=215354)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=215354)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=215354)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 178, in build_async_engine_client
[1;36m(APIServer pid=215354)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=215354)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=215354)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 220, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=215354)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=215354)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 1557, in inner
[1;36m(APIServer pid=215354)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 174, in from_vllm_config
[1;36m(APIServer pid=215354)[0;0m     return cls(
[1;36m(APIServer pid=215354)[0;0m            ^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 120, in __init__
[1;36m(APIServer pid=215354)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=215354)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[1;36m(APIServer pid=215354)[0;0m     return AsyncMPClient(*client_args)
[1;36m(APIServer pid=215354)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 767, in __init__
[1;36m(APIServer pid=215354)[0;0m     super().__init__(
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 446, in __init__
[1;36m(APIServer pid=215354)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=215354)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[1;36m(APIServer pid=215354)[0;0m     next(self.gen)
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 706, in launch_core_engines
[1;36m(APIServer pid=215354)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=215354)[0;0m   File "/home/mobra/aienv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
[1;36m(APIServer pid=215354)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=215354)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
