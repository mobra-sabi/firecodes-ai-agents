#!/usr/bin/env python3
"""
Competitive Strategy Generator - FoloseÈ™te DeepSeek pentru evaluare È™i strategie
AnalizeazÄƒ datele agentului È™i genereazÄƒ strategii de cercetare a concurenÈ›ei
"""

import asyncio
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
from pymongo import MongoClient
from llm_orchestrator import get_orchestrator
from bson import ObjectId
from dotenv import load_dotenv
import os

from tools.deepseek_client import reasoner_chat
from langchain_huggingface import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue

# â­ IMPORT: Modul pentru Ã®mbogÄƒÈ›ire context cu Qdrant vectori
from qdrant_context_enhancer import get_context_enhancer

load_dotenv(override=True)

logger = logging.getLogger(__name__)

MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
MONGO_DB = os.getenv("MONGO_DB", "ai_agents_db")
QDRANT_URL = os.getenv("QDRANT_URL", "http://127.0.0.1:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

class CompetitiveStrategyGenerator:
    """Generator de strategii competitive folosind DeepSeek"""
    
    def __init__(self):
        self.mongo_client = MongoClient(MONGO_URI)
        self.db = self.mongo_client[MONGO_DB]
        self.agents_collection = self.db.site_agents
        self.strategies_collection = self.db.competitive_strategies
        
        # Embeddings pentru search Ã®n Qdrant
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-en-v1.5",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # â­ FIX: NU folosim QdrantClient - folosim requests direct pentru a evita "illegal request line"
        self.qdrant_url = QDRANT_URL
        self.qdrant_api_key = QDRANT_API_KEY
        logger.info("âœ… Qdrant va fi accesat prin HTTP requests (evitÄƒ 'illegal request line')")
    
    async def analyze_agent_and_generate_strategy(self, agent_id: str) -> Dict[str, Any]:
        """
        AnalizeazÄƒ toate datele agentului È™i genereazÄƒ strategie competitive
        """
        try:
            logger.info(f"ğŸ” Analizez agentul {agent_id} pentru strategie competitive...")
            
            # 1. ObÈ›ine datele agentului din MongoDB
            agent = self.agents_collection.find_one({"_id": ObjectId(agent_id)})
            if not agent:
                raise ValueError(f"Agent {agent_id} nu existÄƒ")
            
            # 2. ObÈ›ine conÈ›inutul site-ului din Qdrant sau MongoDB
            collection_name = agent.get("vector_collection")
            
            # DacÄƒ nu existÄƒ vector_collection, Ã®ncearcÄƒ sÄƒ foloseascÄƒ numele standard sau sÄƒ obÈ›inÄƒ din MongoDB
            if not collection_name:
                # ÃncearcÄƒ numele standard pentru colecÈ›ie Qdrant
                collection_name = f"agent_{agent_id}"
                logger.warning(f"âš ï¸ Agent {agent_id} nu are vector_collection configurat. Folosesc numele standard: {collection_name}")
                
                # VerificÄƒ dacÄƒ existÄƒ colecÈ›ie Qdrant cu numele standard (folosind requests)
                try:
                    import requests
                    response = requests.get(f"{self.qdrant_url}/collections/{collection_name}", timeout=5)
                    if response.status_code == 200:
                        collection_info = response.json()
                        points_count = collection_info.get("result", {}).get("points_count", 0)
                        logger.info(f"âœ… ColecÈ›ie Qdrant gÄƒsitÄƒ: {collection_name} ({points_count} puncte)")
                except:
                    # DacÄƒ nu existÄƒ Ã®n Qdrant, obÈ›ine conÈ›inutul din MongoDB
                    logger.warning(f"âš ï¸ ColecÈ›ie Qdrant nu existÄƒ. ObÈ›in conÈ›inutul din MongoDB...")
                    site_content = await self._get_site_content_from_mongodb(agent_id)
                    if not site_content:
                        # â­ FALLBACK: DacÄƒ nu existÄƒ conÈ›inut Ã®n bazele de date, obÈ›ine-l direct de pe site
                        logger.warning(f"âš ï¸ Agent {agent_id} nu are conÈ›inut Ã®n bazele de date. ObÈ›in conÈ›inut direct de pe site...")
                        site_content = await self._fetch_content_from_site(agent)
                        if not site_content:
                            raise ValueError(f"Agent {agent_id} nu are conÈ›inut nici Ã®n Qdrant, nici Ã®n MongoDB, È™i nu s-a putut obÈ›ine de pe site. Te rog sÄƒ recreezi agentul.")
                    # ContinuÄƒ cu conÈ›inutul obÈ›inut
            else:
                # ObÈ›ine conÈ›inutul din Qdrant dacÄƒ existÄƒ vector_collection
                site_content = await self._get_site_content_from_qdrant(collection_name, agent_id)
                
                # Fallback la MongoDB dacÄƒ Qdrant este gol
                if not site_content or len(site_content) < 5:
                    logger.warning(f"âš ï¸ Qdrant collection goalÄƒ sau prea puÈ›ine date ({len(site_content) if site_content else 0} chunks). ObÈ›in conÈ›inutul din MongoDB...")
                    site_content_mongo = await self._get_site_content_from_mongodb(agent_id)
                    
                    # â­ ÃMBUNÄ‚TÄ‚ÈšIRE: DacÄƒ avem < 5 chunks TOTAL, scrapÄƒm direct site-ul pentru date fresh
                    if len(site_content_mongo) < 5:
                        logger.warning(f"âš ï¸ Prea puÈ›in conÈ›inut Ã®n baze de date ({len(site_content_mongo)} chunks). Scrapez direct site-ul...")
                        fresh_content = await self._fetch_content_from_site(agent)
                        if fresh_content and len(fresh_content) > 0:
                            logger.info(f"âœ… ObÈ›inut {len(fresh_content)} chunks FRESH de pe site")
                            site_content = fresh_content
                        else:
                            site_content = site_content_mongo if site_content_mongo else site_content
                            logger.warning(f"âš ï¸ Scraping eÈ™uat, folosesc ce am ({len(site_content)} chunks)")
                    else:
                        site_content = site_content_mongo
            
            # 3. GenereazÄƒ prompt pentru DeepSeek
            analysis_prompt = self._build_analysis_prompt(agent, site_content)
            
            # 3.3 â­ NOU: ÃmbogÄƒÈ›ire context cu vectori din Qdrant pentru Ã®nÈ›elegere profundÄƒ a industriei
            qdrant_context = ""
            try:
                logger.info(f"ğŸ¯ Extragere context Ã®mbogÄƒÈ›it din Qdrant pentru Ã®nÈ›elegere profundÄƒ...")
                enhancer = get_context_enhancer()
                
                # Extrage context complet pentru analiza industriei
                qdrant_context = enhancer.get_full_industry_analysis_context(
                    agent_id=str(agent_id),
                    analysis_focus="strategia competitivÄƒ È™i poziÈ›ionarea pe piaÈ›Äƒ"
                )
                
                if qdrant_context:
                    logger.info(f"âœ… Context Qdrant obÈ›inut: {len(qdrant_context)} caractere")
                else:
                    logger.warning(f"âš ï¸ Nu s-a obÈ›inut context din Qdrant")
            except Exception as e:
                logger.warning(f"âš ï¸ Eroare la obÈ›inere context din Qdrant: {e}")
                qdrant_context = ""
            
            # 3.5. GenereazÄƒ context pentru web search
            web_search_context = await self._get_web_search_context(agent, site_content)
            
            # ConstruieÈ™te prompt complet cu Qdrant context + web search context (ÃNAINTE de a-l folosi)
            enhanced_prompt = analysis_prompt
            
            # â­ PRIORITATE 1: Context semantic din Qdrant (pentru Ã®nÈ›elegere profundÄƒ)
            if qdrant_context:
                enhanced_prompt += f"\n\n{'='*70}\nâ­ CONTEXT SEMANTIC DIN BAZA DE DATE VECTORIALÄ‚ â­\n{'='*70}\n{qdrant_context}\n{'='*70}\n"
            
            # PRIORITATE 2: Web search context (pentru cercetare concurenÈ›i)
            if web_search_context:
                enhanced_prompt += f"\n\n**CONTEXT WEB SEARCH DISPONIBIL (foloseÈ™te aceste surse pentru cercetare):**\n{web_search_context}\n\nFoloseÈ™te aceste surse È™i sugestii pentru a genera o strategie mai completÄƒ cu Ã®ntrebÄƒri concrete de cÄƒutare web."
            
            # 4. FoloseÈ™te DeepSeek Reasoner pentru analizÄƒ cu acces la internet
            logger.info(f"ğŸ¤– Trimite analizÄƒ la DeepSeek Reasoner (cu acces la internet pentru cÄƒutare concurenÈ›i)...")
            logger.info(f"ğŸ“Š Prompt size: {len(enhanced_prompt)} caractere, Web search context: {len(web_search_context)} caractere")
            
            # â­ CRITIC: VerificÄƒ cÄƒ DeepSeek API key este setat
            from tools.deepseek_client import _get_deepseek_key
            try:
                deepseek_key = _get_deepseek_key()
                logger.info(f"âœ… DeepSeek API key este setat: {deepseek_key[:10]}...{deepseek_key[-4:]}")
            except Exception as e:
                logger.error(f"âŒ CRITIC: DeepSeek API key nu este setat: {e}")
                raise ValueError(f"DeepSeek API key nu este configurat. VerificÄƒ DEEPSEEK_API_KEY Ã®n .env")
            
            # AdaugÄƒ instrucÈ›iuni pentru web search Ã®n system prompt
            system_prompt = """EÈ™ti un expert Ã®n analizÄƒ competitivÄƒ È™i strategie de business. 
Analizezi site-uri web pentru a identifica serviciile/produsele oferite È™i generezi strategii 
de cercetare È™i Ã®nÈ›elegere a concurenÈ›ei pentru fiecare tip de serviciu.

IMPORTANT: 
- Ai acces la internet È™i poÈ›i folosi WEB SEARCH pentru a cÄƒuta informaÈ›ii despre concurenÈ›i.
- FoloseÈ™te web search pentru a identifica competitori, sÄƒ analizezi preÈ›uri, caracteristici È™i 
  strategii de marketing ale concurenÈ›ilor.
- FoloseÈ™te toate resursele disponibile (conÈ›inutul site-ului analizat + WEB SEARCH pentru concurenÈ›i)
  pentru a genera o strategie completÄƒ de analizÄƒ competitivÄƒ.
- Pentru fiecare serviciu identificat, genereazÄƒ Ã®ntrebÄƒri concrete de cÄƒutare web È™i sugereazÄƒ surse specifice
  (Google Search, industry directories, competitor websites, review platforms, etc.)
- RÄƒspunde STRICT Ã®n format JSON conform instrucÈ›iunilor din prompt
- Nu folosi markdown code blocks, doar JSON pur
- AsigurÄƒ-te cÄƒ JSON-ul este valid È™i complet"""
            
            
            # CalculeazÄƒ timeout dinamic bazat pe mÄƒrimea prompt-ului
            # Estimare: ~1 token/secundÄƒ pentru DeepSeek Reasoner
            estimated_tokens = len(enhanced_prompt) // 4  # Estimare aproximativÄƒ
            estimated_time = (estimated_tokens + 6000) // 10  # ~10 tokens/secundÄƒ conservativ
            timeout = max(180, min(estimated_time, 300))  # Min 3 min, max 5 min
            
            logger.info(f"â±ï¸ Estimated timeout: {timeout}s pentru ~{estimated_tokens} tokens input + 6000 tokens output")
            
            # â­ CRITIC: Apel DeepSeek cu logging detaliat
            analysis_result_raw = None
            try:
                logger.info("ğŸ”„ Apel DeepSeek API...")
                analysis_result_raw = reasoner_chat(
                    messages=[
                        {
                            "role": "system",
                            "content": system_prompt
                        },
                        {
                            "role": "user",
                            "content": enhanced_prompt
                        }
                    ],
                    max_tokens=6000,  # MÄƒreÈ™te pentru rÄƒspunsuri foarte detaliate
                    temperature=0.5,  # Mai precis pentru analizÄƒ detaliatÄƒ
                    timeout=timeout,  # Timeout dinamic
                    max_retries=3  # Retry pentru timeout-uri
                )
                logger.info("âœ… RÄƒspuns DeepSeek primit")
            except Exception as e:
                error_msg = str(e)
                logger.error(f"âŒ Eroare DeepSeek API: {error_msg}")
                import traceback
                logger.error(traceback.format_exc())
                
                # DacÄƒ e timeout, Ã®ncearcÄƒ cu max_tokens mai mic
                if "timeout" in error_msg.lower():
                    logger.warning("âš ï¸ Timeout detectat. ÃncearcÄƒ cu max_tokens redus...")
                    try:
                        analysis_result_raw = reasoner_chat(
                            messages=[
                                {
                                    "role": "system",
                                    "content": system_prompt
                                },
                                {
                                    "role": "user",
                                    "content": enhanced_prompt[:10000] + "\n\n[ConÈ›inut trunchiat pentru a evita timeout]"
                                }
                            ],
                            max_tokens=3000,  # Redus pentru a evita timeout
                            temperature=0.5,
                            timeout=180,
                            max_retries=2
                        )
                        logger.info("âœ… Succes cu max_tokens redus")
                    except Exception as e2:
                        logger.error(f"âŒ Eroare È™i cu max_tokens redus: {e2}")
                        raise Exception(f"DeepSeek API timeout chiar È™i cu setÄƒri reduse. VerificÄƒ conexiunea la internet sau Ã®ncearcÄƒ mai tÃ¢rziu. Eroare: {error_msg}")
                else:
                    raise Exception(f"DeepSeek API error: {error_msg}")
            
            # â­ CRITIC: VerificÄƒ cÄƒ rÄƒspunsul nu este gol
            if not analysis_result_raw:
                raise ValueError("DeepSeek nu a returnat niciun rÄƒspuns")
            
            # Extrage conÈ›inutul din rÄƒspunsul DeepSeek
            analysis_result = ""
            if isinstance(analysis_result_raw, dict):
                if "data" in analysis_result_raw:
                    choices = analysis_result_raw["data"].get("choices", [])
                    if choices and len(choices) > 0:
                        analysis_result = choices[0].get("message", {}).get("content", "")
                    else:
                        logger.error(f"âŒ CRITIC: Nu existÄƒ 'choices' Ã®n rÄƒspunsul DeepSeek")
                        logger.error(f"   RÄƒspuns complet: {analysis_result_raw}")
                        raise ValueError("DeepSeek nu a returnat 'choices' Ã®n rÄƒspuns")
                elif "content" in analysis_result_raw:
                    analysis_result = analysis_result_raw["content"]
                else:
                    logger.error(f"âŒ CRITIC: Nu existÄƒ 'data' sau 'content' Ã®n rÄƒspunsul DeepSeek")
                    logger.error(f"   RÄƒspuns complet: {analysis_result_raw}")
                    raise ValueError("DeepSeek nu a returnat 'data' sau 'content' Ã®n rÄƒspuns")
            else:
                analysis_result = str(analysis_result_raw)
            
            # â­ CRITIC: VerificÄƒ cÄƒ rÄƒspunsul nu este gol
            if not analysis_result or len(analysis_result.strip()) < 50:
                logger.error(f"âŒ CRITIC: RÄƒspuns DeepSeek este gol sau prea scurt ({len(analysis_result)} caractere)")
                logger.error(f"   RÄƒspuns: {analysis_result[:500]}")
                raise ValueError(f"DeepSeek nu a returnat un rÄƒspuns valid (doar {len(analysis_result)} caractere)")
            
            logger.info(f"âœ… RÄƒspuns DeepSeek primit ({len(analysis_result)} caractere)")
            logger.debug(f"   Primele 500 caractere: {analysis_result[:500]}")
            
            # 5. ParseazÄƒ rÄƒspunsul DeepSeek
            strategy = self._parse_deepseek_response(analysis_result, agent, site_content)
            
            # 6. SalveazÄƒ strategia Ã®n MongoDB
            # â­ VERIFICARE: AsigurÄƒ-te cÄƒ strategy este dict, NU string!
            if isinstance(strategy, str):
                logger.error(f"âŒ EROARE CRITICÄ‚: strategy este STRING, nu DICT!")
                logger.error(f"   Ãncerc sÄƒ parsez string-ul ca JSON...")
                try:
                    strategy = json.loads(strategy)
                    logger.info("âœ… Strategy parsat din string la dict")
                except:
                    logger.error("âŒ Nu pot parsa strategy ca JSON - folosesc fallback")
                    strategy = self._create_fallback_strategy(agent, site_content)
            
            strategy_doc = {
                "agent_id": agent_id,
                "domain": agent.get("domain"),
                "created_at": datetime.now(timezone.utc),
                "strategy": strategy,  # â­ Trebuie sÄƒ fie dict, NU string!
                "services": strategy.get("services", []),  # â­ Duplicate la nivel top pentru acces uÈ™or
                "analysis_metadata": {
                    "total_content_chunks": len(site_content),
                    "services_identified": len(strategy.get("services", [])),
                    "deepseek_model": "deepseek-chat"  # â­ FIX: deepseek-chat, nu deepseek-reasoner
                }
            }
            
            logger.info(f"ğŸ’¾ Salvez strategia Ã®n MongoDB (type={type(strategy).__name__}, services={len(strategy.get('services', []))})")
            
            # Upsert strategia (actualizeazÄƒ dacÄƒ existÄƒ)
            self.strategies_collection.update_one(
                {"agent_id": agent_id},
                {"$set": strategy_doc},
                upsert=True
            )
            
            logger.info(f"âœ… Strategie competitivÄƒ generatÄƒ pentru agent {agent_id}")
            logger.info(f"   Servicii identificate: {len(strategy.get('services', []))}")
            
            return strategy
            
        except Exception as e:
            logger.error(f"âŒ Eroare la generarea strategiei competitive: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def _get_site_content_from_qdrant(self, collection_name: str, agent_id: str) -> List[Dict[str, Any]]:
        """ObÈ›ine conÈ›inutul site-ului din Qdrant"""
        try:
            # VerificÄƒ dacÄƒ colecÈ›ia existÄƒ
            try:
                collection_info = self.qdrant_client.get_collection(collection_name)
            except:
                logger.warning(f"âš ï¸ ColecÈ›ie Qdrant '{collection_name}' nu existÄƒ")
                return []
            
            # ObÈ›ine toate punctele din colecÈ›ie
            # Folosim scroll pentru a obÈ›ine toate punctele
            scroll_result = self.qdrant_client.scroll(
                collection_name=collection_name,
                limit=1000,  # Limit mare pentru a obÈ›ine tot conÈ›inutul
                with_payload=True
            )
            
            content = []
            for point in scroll_result[0]:  # scroll_result este un tuple (points, next_offset)
                if point.payload:
                    content.append({
                        "content": point.payload.get("content", ""),
                        "url": point.payload.get("url", ""),
                        "metadata": point.payload.get("metadata", {})
                    })
            
            logger.info(f"âœ… ObÈ›inut {len(content)} chunks din Qdrant pentru agent {agent_id}")
            return content
            
        except Exception as e:
            logger.warning(f"âš ï¸ Eroare la obÈ›inerea conÈ›inutului din Qdrant: {e}")
            return []
    
    async def _get_web_search_context(self, agent: Dict, site_content: List[Dict]) -> str:
        """GenereazÄƒ context pentru web search bazat pe conÈ›inutul site-ului"""
        try:
            domain = agent.get("domain", "unknown")
            business_type = agent.get("business_type", "general")
            
            # Extrage servicii/produse cheie din conÈ›inut
            services_keywords = []
            for chunk in site_content[:10]:  # Primele 10 chunks
                content = chunk.get("content", "")
                # Simplificare - Ã®n realitate ar trebui NLP
                words = content.split()
                # CautÄƒ cuvinte cheie comune pentru servicii
                for word in words:
                    if len(word) > 5 and word.lower() not in services_keywords:
                        services_keywords.append(word.lower())
            
            # ConstruieÈ™te context pentru web search
            context = f"""Surse recomandate pentru cercetare concurenÈ›i pentru {domain}:

1. Google Search:
   - "{domain} competitors"
   - "{business_type} Romania"
   - "similar services {domain}"
   - Termeni din conÈ›inut: {', '.join(services_keywords[:10])}

2. Industry Directories:
   - CautÄƒ Ã®n directoare de industrie pentru {business_type}
   - AsociaÈ›ii de industrie relevante
   - Platforme B2B pentru sectorul {business_type}

3. Competitor Websites:
   - AnalizeazÄƒ site-urile competitorilor identificaÈ›i
   - ComparÄƒ preÈ›uri, caracteristici, strategii de marketing
   - IdentificÄƒ diferenÈ›iatorii cheie

4. Social Media & Reviews:
   - Platforme de review pentru servicii similare
   - Social media pentru branding È™i strategii de marketing
   - Forums È™i comunitÄƒÈ›i relevante

FoloseÈ™te aceste surse pentru a genera strategii concrete de cercetare."""
            
            return context
        except Exception as e:
            logger.warning(f"âš ï¸ Eroare la generarea web search context: {e}")
            return ""
    
    async def _get_site_content_from_mongodb(self, agent_id: str) -> List[Dict[str, Any]]:
        """ObÈ›ine conÈ›inutul site-ului din MongoDB ca fallback"""
        try:
            # â­ CRITIC: ConverteÈ™te agent_id la ObjectId pentru cÄƒutare corectÄƒ
            from bson import ObjectId
            
            # CautÄƒ Ã®n colecÈ›ia de site content
            site_content_collection = self.db.site_content
            
            # ÃncearcÄƒ cu ObjectId
            try:
                agent_id_obj = ObjectId(agent_id)
                content_docs = list(site_content_collection.find(
                    {"agent_id": agent_id_obj},
                    limit=200  # LimiteazÄƒ pentru a nu Ã®ncÄƒrca prea mult
                ))
            except:
                # Fallback: Ã®ncearcÄƒ cu string
                logger.warning(f"âš ï¸ Nu s-a putut converti agent_id la ObjectId, Ã®ncearcÄƒ cu string...")
                content_docs = list(site_content_collection.find(
                    {"agent_id": agent_id},
                    limit=200
                ))
            
            content = []
            for doc in content_docs:
                content.append({
                    "content": doc.get("content", ""),
                    "url": doc.get("url", ""),
                    "metadata": doc.get("metadata", {})
                })
            
            if content:
                logger.info(f"âœ… ObÈ›inut {len(content)} chunks din MongoDB pentru agent {agent_id}")
            else:
                logger.warning(f"âš ï¸ Nu s-a gÄƒsit conÈ›inut Ã®n MongoDB pentru agent {agent_id}")
            
            return content
            
        except Exception as e:
            logger.error(f"âŒ Eroare la obÈ›inerea conÈ›inutului din MongoDB: {e}")
            return []
    
    async def _fetch_content_from_site(self, agent: Dict) -> List[Dict]:
        """ObÈ›ine conÈ›inutul direct de pe site ca fallback dacÄƒ nu existÄƒ Ã®n bazele de date"""
        try:
            import requests
            from bs4 import BeautifulSoup
            from urllib.parse import urljoin, urlparse
            
            site_url = agent.get("site_url") or f"https://{agent.get('domain', '')}"
            if not site_url or not site_url.startswith(('http://', 'https://')):
                logger.warning(f"âš ï¸ URL invalid pentru agent: {site_url}")
                return []
            
            logger.info(f"ğŸŒ ObÈ›in conÈ›inut direct de pe {site_url}...")
            
            # Headers pentru a evita blocarea
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            try:
                response = requests.get(site_url, headers=headers, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # EliminÄƒ elemente nedorite
                for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    tag.decompose()
                
                # Extrage conÈ›inutul principal
                title = soup.find('title')
                title_text = title.get_text().strip() if title else ""
                
                # Extrage textul principal
                main_content = soup.get_text(separator=' ', strip=True)
                
                if len(main_content) < 100:
                    logger.warning(f"âš ï¸ ConÈ›inut prea scurt de pe {site_url}")
                    return []
                
                # CreeazÄƒ chunks din conÈ›inut
                chunk_size = 2000
                chunks = []
                for i in range(0, len(main_content), chunk_size):
                    chunk_text = main_content[i:i+chunk_size]
                    if len(chunk_text.strip()) > 50:
                        chunks.append({
                            "content": chunk_text,
                            "url": site_url,
                            "metadata": {
                                "chunk_index": len(chunks),
                                "source": "direct_fetch",
                                "title": title_text
                            }
                        })
                
                logger.info(f"âœ… ObÈ›inut {len(chunks)} chunks direct de pe site")
                return chunks
                
            except Exception as e:
                logger.error(f"âŒ Eroare la obÈ›inerea conÈ›inutului de pe site: {e}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ Eroare Ã®n _fetch_content_from_site: {e}")
            return []
    
    def _build_analysis_prompt(self, agent: Dict, site_content: List[Dict]) -> str:
        """ConstruieÈ™te prompt-ul pentru analizÄƒ DeepSeek"""
        
        # ConstruieÈ™te rezumatul conÈ›inutului - IMPORTANT: pÄƒstreazÄƒ terminologia exactÄƒ din site
        # AnalizeazÄƒ MAI MULTE chunks pentru a obÈ›ine context complet
        content_summary = []
        max_chunks = min(100, len(site_content))  # AnalizeazÄƒ pÃ¢nÄƒ la 100 chunks
        
        for idx, chunk in enumerate(site_content[:max_chunks], 1):
            chunk_content = chunk.get('content', '').strip()
            if not chunk_content:
                continue
            
            # PÄƒstreazÄƒ conÈ›inutul complet pentru context maxim (pÃ¢nÄƒ la 2000 caractere per chunk)
            chunk_url = chunk.get('url', 'N/A')
            content_summary.append(f"=== CHUNK {idx} (URL: {chunk_url}) ===\n{chunk_content[:2000]}\n")
        
        content_text = "\n\n".join(content_summary)
        
        # DacÄƒ nu avem destul conÈ›inut, avertizeazÄƒ
        if len(content_summary) < 5:
            logger.warning(f"âš ï¸ Doar {len(content_summary)} chunks disponibile - strategia poate fi mai genericÄƒ")
        
        # Extrage informaÈ›ii despre site pentru corecÈ›ii
        site_domain = agent.get('domain', 'unknown')
        site_name = agent.get('name', 'N/A')
        site_url = agent.get('site_url', 'N/A')
        
        prompt = f"""EÈ™ti un EXPERT Ã®n analizÄƒ competitivÄƒ È™i strategie de business. AnalizeazÄƒ ATENT urmÄƒtoarele date despre site-ul {site_domain} È™i genereazÄƒ o strategie COMPLETÄ‚, DETALIATÄ‚ È™i SPECIFICÄ‚ de cercetare a concurenÈ›ei.

**CRITICAL - ACURATETE TERMINOLOGIE:**
- CiteÈ™te ATENT TOATE chunks-urile È™i foloseÈ™te EXACT terminologia din site
- DacÄƒ site-ul foloseÈ™te "matari" (nu "mÄƒtÄƒÈ™uri"), foloseÈ™te "matari"
- DacÄƒ site-ul foloseÈ™te "treceri" (nu "trecere"), foloseÈ™te "treceri"
- VerificÄƒ Ã®n TOATE chunks-urile Ã®nainte de a identifica servicii
- Nu inventa termeni - foloseÈ™te DOAR ce gÄƒseÈ™ti Ã®n conÈ›inut
- Extrage NUME EXACTE de servicii/produse din conÈ›inut
- IdentificÄƒ TOATE serviciile oferite, nu doar cele generale

**INFORMAÈšII SITE:**
- Domeniu: {site_domain}
- Nume: {site_name}
- URL: {site_url}
- Tip business: {agent.get('business_type', 'general')}
- Total chunks analizate: {len(content_summary)}

**CONÈšINUT COMPLET SITE (CITEÈ˜TE ATENT TOATE CHUNKS-URILE PENTRU TERMINOLOGIE CORECTÄ‚):**
{content_text}

**INSTRUCÈšIUNI DETALIATE:**
1. ANALIZÄ‚ PROFUNDÄ‚:
   - CiteÈ™te TOATE chunks-urile È™i identificÄƒ TOATE serviciile/produsele oferite
   - Pentru fiecare serviciu, extrage NUMELE EXACT, descrierea detaliatÄƒ È™i caracteristicile
   - IdentificÄƒ termeni tehnici, certificÄƒri, standarde menÈ›ionate
   - NoteazÄƒ zone geografice de acoperire, tipuri de clienÈ›i, preÈ›uri (dacÄƒ sunt menÈ›ionate)

2. PENTRU FIECARE SERVICIU IDENTIFICAT:
   - Nume EXACT al serviciului (foloseÈ™te terminologia din site)
   - Descriere detaliatÄƒ bazatÄƒ pe conÈ›inutul site-ului
   - Termeni de cÄƒutare SPECIFICI pentru identificarea competitorilor
   - Strategie de cercetare DETALIATÄ‚ cu surse concrete
   - ÃntrebÄƒri-cheie SPECIFICE pentru acel serviciu
   - Query-uri web search CONCRETE È™i ACÈšIONABILE

3. STRATEGIE GENERALÄ‚:
   - Abordare competitivÄƒ SPECIFICÄ‚ pentru industria {site_domain}
   - PrioritÄƒÈ›i de cercetare CONCRETE È™i ACÈšIONABILE
   - Rezultate aÈ™teptate DETALIATE È™i MÄ‚SURABILE

**INSTRUCÈšIUNI:**
1. IdentificÄƒ TOATE tipurile de servicii/produse oferite de acest site
2. Pentru fiecare tip de serviciu:
   - DefineÈ™te serviciul/produsul clar
   - GenereazÄƒ termeni de cÄƒutare pentru identificarea competitorilor (foloseÈ™te web search dacÄƒ este necesar)
   - Propune strategii de cercetare a concurenÈ›ei (unde sÄƒ cauÈ›i, ce sÄƒ cauÈ›i)
   - SugereazÄƒ Ã®ntrebÄƒri-cheie pentru a Ã®nÈ›elege concurenÈ›a
   - Include surse pentru cercetare (Google Search, industry directories, competitor websites, social media, etc.)
3. CreeazÄƒ un plan general de analizÄƒ competitivÄƒ
4. FOLOSEÈ˜TE WEB SEARCH dacÄƒ ai nevoie de informaÈ›ii actualizate despre concurenÈ›i, preÈ›uri sau piaÈ›Äƒ

**FORMAT RÄ‚SPUNS (JSON):**
{{
    "services": [
        {{
            "service_name": "Nume serviciu/produs",
            "description": "Descriere detaliatÄƒ",
            "search_keywords": ["cuvinte", "cheie", "cÄƒutare"],
            "competitive_research_strategy": {{
                "where_to_search": [
                    "Google Search cu termeni specifici",
                    "Industry directories (ex: directory-industrie.ro)",
                    "Competitor websites",
                    "Social media platforms",
                    "Review platforms (ex: Google Reviews, Trustpilot)",
                    "Business directories (ex: YellowPages, 123firme.ro)",
                    "Trade shows È™i evenimente de industrie",
                    "Forums È™i comunitÄƒÈ›i online"
                ],
                "what_to_look_for": [
                    "PreÈ›uri È™i pachete",
                    "Caracteristici È™i beneficii",
                    "Strategii de marketing",
                    "PoziÈ›ionare pe piaÈ›Äƒ",
                    "DiferenÈ›iatorii cheie",
                    "Feedback È™i recenzii clienÈ›i",
                    "PrezenÈ›Äƒ online È™i branding"
                ],
                "key_questions": [
                    "Cine sunt principalii concurenÈ›i pentru acest serviciu?",
                    "Ce oferÄƒ concurenÈ›ii la acelaÈ™i preÈ›?",
                    "Cum se diferenÈ›iazÄƒ serviciul analizat?",
                    "Ce feedback primesc concurenÈ›ii de la clienÈ›i?",
                    "Ce strategii de marketing folosesc concurenÈ›ii?"
                ],
                "web_search_queries": [
                    "{{service_name}} competitors Romania",
                    "{{service_name}} alternative",
                    "best {{service_name}} providers",
                    "{{service_name}} pricing comparison"
                ]
            }},
            "priority": "high/medium/low"
        }}
    ],
    "overall_strategy": {{
        "competitive_analysis_approach": "Descrierea abordÄƒrii generale cu recomandÄƒri pentru web search",
        "research_priorities": [
            "Identificare principalilor concurenÈ›i (foloseÈ™te Google Search)",
            "Comparare preÈ›uri È™i pachete (web search + competitor websites)",
            "AnalizÄƒ diferenÈ›iatorii cheie (site-uri concurenÈ›i + review platforms)",
            "Evaluare strategii de marketing (social media + web search)"
        ],
        "expected_outcomes": "Ce ar trebui sÄƒ descoperim: lista concurenÈ›ilor, comparaÈ›ie preÈ›uri, analizÄƒ diferenÈ›iatorii, recomandÄƒri strategice",
        "web_search_enabled": true,
        "internet_access_required": true
    }}
}}

**IMPORTANT:** 
- RÄƒspunde DOAR Ã®n format JSON, fÄƒrÄƒ text suplimentar
- Fiecare serviciu trebuie sÄƒ aibÄƒ o strategie detaliatÄƒ cu surse concrete
- Include Ã®ntrebÄƒri de cÄƒutare web concrete pentru fiecare serviciu
- RecomandÄƒ surse specifice (Google Search, directories, platforms) pentru fiecare tip de serviciu"""
        
        return prompt
    
    def _parse_deepseek_response(self, response: str, agent: Dict, site_content: List[Dict]) -> Dict[str, Any]:
        """ParseazÄƒ rÄƒspunsul DeepSeek È™i construieÈ™te strategia"""
        try:
            # â­ CRITIC: VerificÄƒ dacÄƒ rÄƒspunsul este gol sau invalid
            response_text = response if isinstance(response, str) else response.get("content", "") if isinstance(response, dict) else str(response)
            
            if not response_text or len(response_text.strip()) < 50:
                logger.error(f"âŒ CRITIC: RÄƒspuns DeepSeek este gol sau prea scurt ({len(response_text)} caractere)")
                logger.error(f"   RÄƒspuns primit: {response_text[:200]}")
                raise ValueError("DeepSeek nu a returnat un rÄƒspuns valid")
            
            logger.info(f"ğŸ“ Parsing rÄƒspuns DeepSeek ({len(response_text)} caractere)...")
            logger.debug(f"   Primele 500 caractere: {response_text[:500]}")
            
            # CautÄƒ JSON Ã®n rÄƒspuns
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                
                # â­ CRITIC: ÃncearcÄƒ sÄƒ parseze JSON cu mai multe Ã®ncercÄƒri
                strategy = None
                try:
                    strategy = json.loads(json_str)
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸ Eroare JSON parsing (prima Ã®ncercare): {e}")
                    # ÃncearcÄƒ sÄƒ cureÈ›e JSON-ul
                    json_str_clean = json_str
                    # EliminÄƒ markdown code blocks dacÄƒ existÄƒ
                    if "```json" in json_str_clean:
                        json_str_clean = json_str_clean.split("```json")[1].split("```")[0]
                    elif "```" in json_str_clean:
                        json_str_clean = json_str_clean.split("```")[1].split("```")[0]
                    
                    try:
                        strategy = json.loads(json_str_clean)
                        logger.info("âœ… JSON parsat dupÄƒ curÄƒÈ›are")
                    except json.JSONDecodeError as e2:
                        logger.error(f"âŒ Eroare JSON parsing (dupÄƒ curÄƒÈ›are): {e2}")
                        logger.error(f"   JSON string: {json_str_clean[:500]}")
                        
                        # â­ FALLBACK FINAL: CurÄƒÈ›Äƒ JSON agresiv È™i Ã®ncearcÄƒ din nou
                        logger.warning("âš ï¸  Ãncerc curÄƒÈ›are agresivÄƒ JSON...")
                        try:
                            import re
                            # EliminÄƒ comentarii
                            json_cleaned = re.sub(r'//.*?\n', '\n', json_str_clean)
                            # EliminÄƒ virgule trailing
                            json_cleaned = re.sub(r',\s*}', '}', json_cleaned)
                            json_cleaned = re.sub(r',\s*]', ']', json_cleaned)
                            # EliminÄƒ newline Ã®n strings
                            json_cleaned = re.sub(r'(?<=")\n(?=")', ' ', json_cleaned)
                            
                            strategy = json.loads(json_cleaned)
                            logger.info("âœ… JSON parsat dupÄƒ curÄƒÈ›are agresivÄƒ")
                        except Exception as e3:
                            logger.error(f"âŒ Eroare È™i dupÄƒ curÄƒÈ›are agresivÄƒ: {e3}")
                            # FALLBACK ABSOLUT: CreeazÄƒ strategie minimÄƒ
                            logger.warning("âš ï¸ Folosesc fallback total - creez strategie minimÄƒ")
                            strategy = self._create_fallback_strategy(agent, site_content)
                            logger.info("âœ… Strategie fallback creatÄƒ")
                
                if not strategy:
                    raise ValueError("Strategy este None dupÄƒ parsing")
                
                # ValideazÄƒ È™i completeazÄƒ strategia
                if "services" not in strategy:
                    logger.warning("âš ï¸ DeepSeek nu a returnat 'services' - creez lista goalÄƒ")
                    strategy["services"] = []
                
                if "overall_strategy" not in strategy:
                    # â­ CRITIC: DacÄƒ nu existÄƒ strategie generalÄƒ, NU folosi fallback generic
                    # ConstruieÈ™te una detaliatÄƒ bazatÄƒ pe conÈ›inutul REAL al site-ului
                    logger.warning("âš ï¸ DeepSeek nu a returnat overall_strategy - construiesc una detaliatÄƒ bazatÄƒ pe conÈ›inut")
                    
                    domain = agent.get('domain', 'unknown')
                    site_name = agent.get('name', domain)
                    
                    # Extrage servicii reale din conÈ›inut
                    services_found = []
                    for chunk in site_content[:30]:
                        content = chunk.get('content', '').strip()
                        if len(content) > 100:
                            services_found.append(content[:300])
                    
                    # ConstruieÈ™te strategie detaliatÄƒ bazatÄƒ pe conÈ›inutul REAL
                    strategy["overall_strategy"] = {
                        "competitive_analysis_approach": f"Strategie competitivÄƒ COMPLETÄ‚ È™i DETALIATÄ‚ pentru {domain}, bazatÄƒ pe analiza profundÄƒ a conÈ›inutului site-ului {site_name}. Strategia include identificarea TOÈšILOR servicii/produse oferite, analiza concurenÈ›ei pentru fiecare serviciu, È™i recomandÄƒri concrete de cercetare folosind web search È™i resurse online.",
                        "research_priorities": [
                            f"Identificare principalilor concurenÈ›i pentru serviciile oferite de {domain} folosind Google Search cu termeni specifici extraÈ™i din conÈ›inutul site-ului",
                            f"Comparare preÈ›uri È™i pachete de servicii prin analiza site-urilor concurente È™i platforme de review (Google Reviews, Trustpilot, etc.)",
                            f"AnalizÄƒ diferenÈ›iatori cheie (certificÄƒri, experienÈ›Äƒ, portofoliu, tehnologii) din conÈ›inutul online È™i feedback clienÈ›i",
                            f"Evaluare strategii de marketing È™i prezenÈ›Äƒ online (social media, SEO, content marketing) prin monitorizare web search È™i analiza competitorilor"
                        ],
                        "expected_outcomes": f"ListÄƒ COMPLETÄ‚ È™i DETALIATÄ‚ a concurenÈ›ilor pentru {domain}, comparaÈ›ie detaliatÄƒ preÈ›uri È™i caracteristici pentru fiecare serviciu identificat, identificare diferenÈ›iatori cheie È™i avantaje competitive, recomandÄƒri strategice CONCRETE È™i ACÈšIONABILE pentru Ã®mbunÄƒtÄƒÈ›irea poziÈ›iei competitive pe piaÈ›Äƒ"
                    }
                
                # AdaugÄƒ metadata
                strategy["metadata"] = {
                    "agent_id": str(agent.get("_id")),
                    "domain": agent.get("domain"),
                    "analysis_date": datetime.now(timezone.utc).isoformat(),
                    "total_services": len(strategy.get("services", [])),
                    "deepseek_used": True,
                    "response_length": len(response_text)
                }
                
                logger.info(f"âœ… Strategie parsatÄƒ cu succes: {len(strategy.get('services', []))} servicii")
                return strategy
            else:
                # â­ CRITIC: DacÄƒ nu existÄƒ JSON, NU folosi fallback generic - aruncÄƒ eroare
                logger.error(f"âŒ CRITIC: Nu s-a gÄƒsit JSON Ã®n rÄƒspunsul DeepSeek")
                logger.error(f"   RÄƒspuns primit: {response_text[:1000]}")
                raise ValueError(f"DeepSeek nu a returnat JSON valid. RÄƒspuns: {response_text[:500]}")
                
        except ValueError as e:
            # Re-raise ValueError pentru a fi prins de caller
            raise
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Eroare la parsarea JSON: {e}")
            logger.error(f"   RÄƒspuns: {response_text[:1000] if 'response_text' in locals() else 'N/A'}")
            raise ValueError(f"Nu s-a putut parsa JSON din rÄƒspunsul DeepSeek: {e}")
        except Exception as e:
            logger.error(f"âŒ Eroare la parsarea rÄƒspunsului: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise ValueError(f"Eroare la parsarea rÄƒspunsului DeepSeek: {e}")
    
    def _create_fallback_strategy(self, agent: Dict, site_content: List[Dict]) -> Dict[str, Any]:
        """CreeazÄƒ o strategie de bazÄƒ dacÄƒ DeepSeek nu returneazÄƒ JSON valid"""
        return {
            "services": [
                {
                    "service_name": "General Services",
                    "description": f"Servicii generale oferite de {agent.get('domain', 'site')}",
                    "search_keywords": [agent.get('domain', ''), agent.get('business_type', 'services')],
                    "competitive_research_strategy": {
                        "where_to_search": ["Google Search", "Industry directories", "Competitor websites"],
                        "what_to_look_for": ["Similar services", "Pricing", "Features"],
                        "key_questions": ["Who are the main competitors?", "What are their strengths?", "How do they differentiate?"]
                    },
                    "priority": "high"
                }
            ],
            "overall_strategy": {
                "competitive_analysis_approach": "General competitive analysis approach based on site content",
                "research_priorities": ["Market analysis", "Competitor identification", "Feature comparison"],
                "expected_outcomes": "Understanding of competitive landscape"
            },
            "metadata": {
                "agent_id": str(agent.get("_id")),
                "domain": agent.get("domain"),
                "analysis_date": datetime.now(timezone.utc).isoformat(),
                "fallback": True,
                "total_services": 1
            }
        }
    
    async def get_strategy_for_agent(self, agent_id: str) -> Optional[Dict[str, Any]]:
        """ObÈ›ine strategia existentÄƒ pentru un agent"""
        try:
            strategy_doc = self.strategies_collection.find_one({"agent_id": agent_id})
            if strategy_doc:
                return strategy_doc.get("strategy")
            return None
        except Exception as e:
            logger.error(f"âŒ Eroare la obÈ›inerea strategiei: {e}")
            return None

# InstanÈ›Äƒ globalÄƒ
strategy_generator = CompetitiveStrategyGenerator()

