#!/usr/bin/env python3
import os, sys, json, argparse, re, time
from urllib.parse import urlparse, urljoin
from typing import List, Dict, Set, Optional
import openai
import tldextract
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from pymongo import MongoClient
import requests
from bs4 import BeautifulSoup
from collections import deque
import concurrent.futures
from dataclasses import dataclass
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer
from datetime import datetime
import asyncio

@dataclass
class ConstructionService:
    name: str
    category: str
    description: str
    target_market: str
    competition_level: str
    regulations: List[str]
    opportunities: List[str]

class ConstructionAgentCreator:
    def __init__(self):
        # AI Models
        self.gpt4 = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.qwen = openai.OpenAI(base_url="http://localhost:9301/v1", api_key="local-vllm")
        
        # Databases
        self.qdrant = QdrantClient("localhost", port=6333)
        self.mongo = MongoClient("mongodb://localhost:27017/")
        self.db = self.mongo.construction_intelligence
        
        # Collections
        self.sites_collection = self.db.company_sites
        self.services_collection = self.db.construction_services
        self.competitors_collection = self.db.competitors
        self.regulations_collection = self.db.regulations
        self.agents_collection = self.db.site_agents
        
        # Embeddings
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # ConstrucÈ›ii subdominii RomÃ¢nia
        self.construction_domains = {
            "constructii_generale": {
                "keywords": ["constructii case", "constructii vile", "constructii cladiri", "amenajari"],
                "competitors_search": ["constructii bucuresti", "constructii cluj", "constructii timisoara"],
                "regulations": ["autorizatie constructie", "POT", "CUT", "aviz ISU", "certificat energetic"]
            },
            "renovari_amenajari": {
                "keywords": ["renovari apartamente", "amenajari interioare", "design interior", "renovari case"],
                "competitors_search": ["renovari bucuresti", "amenajari cluj", "design interior romania"],
                "regulations": ["aviz condominium", "autorizatie amenajare", "certificate materiale"]
            },
            "instalatii": {
                "keywords": ["instalatii sanitare", "instalatii electrice", "instalatii gaz", "instalatii termice"],
                "competitors_search": ["instalatori bucuresti", "electricieni autorizati", "instalatii sanitare"],
                "regulations": ["ANRE", "certificat instalator", "aviz distributie gaz", "autorizatie electrica"]
            },
            "acoperisuri": {
                "keywords": ["acoperisuri", "tigla", "tabla", "reparatii acoperis", "hidroizolatie"],
                "competitors_search": ["acoperisuri bucuresti", "tigla cluj", "reparatii acoperis"],
                "regulations": ["garantie acoperis", "certificate materiale", "aviz urbanism"]
            },
            "demolari": {
                "keywords": ["demolari", "excavatii", "terasamente", "decopertare"],
                "competitors_search": ["demolari bucuresti", "excavatii cluj", "terasamente"],
                "regulations": ["aviz demolare", "studiu geotehnic", "masuri siguranta", "gestionare deseuri"]
            },
            "izolatie_termica": {
                "keywords": ["izolatie termica", "termoizolatie", "eficienta energetica", "reabilitare termica"],
                "competitors_search": ["izolatie termica bucuresti", "reabilitare bloc", "eficienta energetica"],
                "regulations": ["certificat energetic", "Casa Verde", "ANL", "fonduri europene"]
            }
        }
        
        self.init_databases()
        print("ğŸ—ï¸ Construction Agent Creator iniÈ›ializat")

    def init_databases(self):
        """IniÈ›ializeazÄƒ colecÈ›iile È™i indexurile pentru construcÈ›ii"""
        try:
            # MongoDB indexes
            indexes = [
                (self.sites_collection, [("domain", 1), ("services", 1)]),
                (self.services_collection, [("category", 1), ("competition_level", 1)]),
                (self.competitors_collection, [("service_category", 1), ("location", 1)]),
                (self.regulations_collection, [("domain", 1), ("regulation_type", 1)]),
                (self.agents_collection, [("domain", 1), ("agent_type", 1)])
            ]
            
            for collection, index_spec in indexes:
                try:
                    collection.create_index(index_spec)
                except:
                    pass
            
            # Qdrant collections
            collections = [
                ("construction_sites", 384),
                ("construction_services", 384),
                ("competition_analysis", 384),
                ("regulations_db", 384)
            ]
            
            for name, size in collections:
                try:
                    self.qdrant.get_collection(name)
                except:
                    self.qdrant.create_collection(
                        collection_name=name,
                        vectors_config=VectorParams(size=size, distance=Distance.COSINE)
                    )
            
            print("âœ… Baze de date pentru construcÈ›ii iniÈ›ializate")
        except Exception as e:
            print(f"âš ï¸ Eroare iniÈ›ializare baze de date: {e}")

    def analyze_construction_site(self, site_url: str) -> Dict:
        """AnalizeazÄƒ un site de construcÈ›ii È™i identificÄƒ serviciile"""
        print(f"ğŸ” Analizez site-ul de construcÈ›ii: {site_url}")
        
        # Scraping complet site
        site_data = self.scrape_construction_site(site_url)
        
        # GPT-4 analizeazÄƒ È™i categoriseazÄƒ serviciile
        services_analysis = self.gpt4_analyze_construction_services(site_data, site_url)
        
        # SalveazÄƒ Ã®n baza de date
        self.save_site_analysis(site_url, site_data, services_analysis)
        
        return services_analysis

    def scrape_construction_site(self, site_url: str) -> Dict:
        """Scraping specializat pentru site-uri de construcÈ›ii"""
        print(f"ğŸ•·ï¸ Scraping site construcÈ›ii: {site_url}")
        
        domain = tldextract.extract(site_url).top_domain_under_public_suffix.lower()
        visited = set()
        queue = deque([site_url])
        pages_data = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'ro-RO,ro;q=0.9,en;q=0.8'
        }
        
        # Keywords specifice construcÈ›ii pentru prioritizare
        priority_keywords = [
            'servicii', 'constructii', 'amenajari', 'renovari', 'instalatii', 
            'acoperisuri', 'izolatie', 'demolari', 'proiecte', 'portofoliu',
            'despre', 'contact', 'preturi', 'oferta'
        ]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
            while queue and len(pages_data) < 100:
                batch_urls = []
                for _ in range(min(15, len(queue), 100 - len(pages_data))):
                    if queue:
                        url = queue.popleft()
                        if url not in visited:
                            visited.add(url)
                            batch_urls.append(url)
                
                if not batch_urls:
                    break
                
                futures = {executor.submit(self.scrape_single_page, url, headers): url for url in batch_urls}
                
                for future in concurrent.futures.as_completed(futures):
                    page_data = future.result()
                    if page_data:
                        page_data["domain"] = domain
                        pages_data.append(page_data)
                        
                        # PrioritizeazÄƒ link-urile cu keywords de construcÈ›ii
                        priority_links = []
                        normal_links = []
                        
                        for link in page_data.get("links", []):
                            if (domain in link and link not in visited and 
                                not any(skip in link for skip in ['.jpg', '.pdf', '.png', 'javascript:', 'mailto:'])):
                                
                                if any(keyword in link.lower() for keyword in priority_keywords):
                                    priority_links.append(link)
                                else:
                                    normal_links.append(link)
                        
                        # AdaugÄƒ link-urile prioritare primul
                        for link in priority_links + normal_links[:5]:
                            queue.append(link)
                        
                        print(f"  âœ… Pagina {len(pages_data)}: {page_data['title'][:60]}...")
        
        return {
            "domain": domain,
            "total_pages": len(pages_data),
            "pages": pages_data,
            "scraped_at": datetime.now().isoformat()
        }

    def scrape_single_page(self, url: str, headers: dict) -> Optional[Dict]:
        """Scraping pentru o singurÄƒ paginÄƒ cu focus pe construcÈ›ii"""
        try:
            resp = requests.get(url, headers=headers, timeout=15)
            if resp.status_code != 200:
                return None
                
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # Extract title
            title = soup.find('title')
            title = title.get_text().strip() if title else "FÄƒrÄƒ titlu"
            
            # Extract meta description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', '') if meta_desc else ''
            
            # Extract structured data specific pentru construcÈ›ii
            services_text = self.extract_construction_services(soup)
            contact_info = self.extract_contact_info(soup)
            
            # Clean content
            for elem in soup(["script", "style", "nav", "footer", "header", "aside"]):
                elem.decompose()
            
            content = soup.get_text()
            content = re.sub(r'\s+', ' ', content).strip()
            
            # Extract links
            links = [urljoin(url, link['href']) for link in soup.find_all('a', href=True)]
            
            return {
                "url": url,
                "title": title,
                "description": description,
                "content": content[:5000],
                "services_detected": services_text,
                "contact_info": contact_info,
                "links": links,
                "scraped_at": time.time()
            }
            
        except Exception as e:
            print(f"  âš ï¸ Eroare la {url}: {e}")
            return None

    def extract_construction_services(self, soup: BeautifulSoup) -> List[str]:
        """Extrage servicii de construcÈ›ii din paginÄƒ"""
        services = []
        
        # Keywords pentru detectarea serviciilor
        service_patterns = [
            r'construc[È›|t]i[ei]?\s+(?:case|vil[ei]|cl[Äƒ|a]diri|residen[È›|t]ial[ei]?)',
            r'amenaj[Äƒ|a]ri\s+(?:interior[ei]?|exterior[ei]?)',
            r'renov[Äƒ|a]ri\s+(?:apartament[ei]?|case|vil[ei])',
            r'instalat[,|.]ii?\s+(?:sanitare|electrice|gaz|termice)',
            r'acoperi[s|È™]uri?\s+(?:tigl[Äƒ|a]|tabl[Äƒ|a]|membran[Äƒ|a])',
            r'izola[t|È›]ie?\s+(?:termic[Äƒ|a]|fonic[Äƒ|a]|hidroizola[t|È›]ie)',
            r'demol[Äƒ|a]ri|excava[t|È›]ii|terasamente',
            r'pardoseli\s+(?:epoxidice|industrial[ei]?|decorative)',
            r'zugr[Äƒ|a]veli|vopsitori[ei]?|finisa[j|g]e'
        ]
        
        text = soup.get_text().lower()
        
        for pattern in service_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            services.extend(matches)
        
        return list(set(services))

    def extract_contact_info(self, soup: BeautifulSoup) -> Dict:
        """Extrage informaÈ›ii de contact"""
        contact_info = {}
        
        text = soup.get_text()
        
        # Extract phone numbers
        phone_pattern = r'(?:\+40|0)(?:\s|-)?(?:\d{3})(?:\s|-)?(?:\d{3})(?:\s|-)?(?:\d{3})'
        phones = re.findall(phone_pattern, text)
        if phones:
            contact_info['phones'] = phones
        
        # Extract emails
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, text)
        if emails:
            contact_info['emails'] = emails
        
        # Extract addresses (Romanian specific)
        address_patterns = [
            r'(?:str|strada|bd|bulevardul|calea)\s+[A-Za-z\s]+\s+(?:nr|numÄƒrul)?\s*\d+',
            r'sector\s+\d+',
            r'(?:bucureÈ™ti|cluj|timiÈ™oara|constanÈ›a|iaÈ™i|craiova|braÈ™ov|galaÈ›i|ploieÈ™ti|oradea)',
        ]
        
        addresses = []
        for pattern in address_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            addresses.extend(matches)
        
        if addresses:
            contact_info['addresses'] = addresses
        
        return contact_info

    def gpt4_analyze_construction_services(self, site_data: Dict, site_url: str) -> Dict:
        """GPT-4 analizeazÄƒ serviciile de construcÈ›ii È™i creeazÄƒ strategia"""
        
        # ConstruieÈ™te contextul pentru GPT-4
        context = self.build_gpt4_context(site_data)
        
        prompt = f"""AnalizeazÄƒ acest site de construcÈ›ii din RomÃ¢nia È™i creeazÄƒ o analizÄƒ strategicÄƒ detaliatÄƒ.

CONTEXT SITE:
{context}

URL: {site_url}

CreeazÄƒ o analizÄƒ JSON cu aceastÄƒ structurÄƒ exactÄƒ:

{{
  "company_analysis": {{
    "company_name": "numele companiei",
    "main_location": "oraÈ™ul principal",
    "company_size": "estimare: micro/mic/mediu/mare",
    "years_experience": "estimare ani experienÈ›Äƒ",
    "unique_selling_points": ["puncte forte identificate"]
  }},
  "services_identified": [
    {{
      "service_name": "nume serviciu",
      "category": "categorie din: constructii_generale|renovari_amenajari|instalatii|acoperisuri|demolari|izolatie_termica",
      "description": "descriere serviciu",
      "target_market": "piaÈ›a È›intÄƒ",
      "estimated_demand": "cerere estimatÄƒ: scÄƒzutÄƒ|medie|ridicatÄƒ",
      "competition_level": "competiÈ›ie: scÄƒzutÄƒ|medie|ridicatÄƒ|foarte ridicatÄƒ",
      "price_positioning": "poziÈ›ionare preÈ›: budget|mediu|premium",
      "growth_potential": "potenÈ›ial creÈ™tere: scÄƒzut|mediu|ridicat"
    }}
  ],
  "market_opportunities": [
    {{
      "opportunity": "descriere oportunitate",
      "market_size": "dimensiune piaÈ›Äƒ estimatÄƒ",
      "competition_gap": "goluri Ã®n competiÈ›ie",
      "implementation_difficulty": "dificultate: uÈ™oarÄƒ|medie|ridicatÄƒ"
    }}
  ],
  "digital_presence": {{
    "website_quality": "calitate site: slabÄƒ|medie|bunÄƒ|excelentÄƒ",
    "seo_optimization": "optimizare SEO: slabÄƒ|medie|bunÄƒ",
    "social_media_present": "prezenÈ›Äƒ social media: da/nu",
    "online_reputation": "reputaÈ›ie online estimatÄƒ",
    "improvement_areas": ["zone de Ã®mbunÄƒtÄƒÈ›ire"]
  }},
  "agent_personality": {{
    "role": "Specialist Ã®n [domeniu principal] cu experienÈ›Äƒ Ã®n RomÃ¢nia",
    "expertise_areas": ["zone de expertizÄƒ"],
    "communication_style": "stil comunicare: profesional|prietenos|tehnic|consultativ",
    "key_knowledge": ["cunoÈ™tinÈ›e cheie despre piaÈ›Äƒ/legislaÈ›ie"],
    "response_approach": "abordare rÄƒspunsuri: practicÄƒ|strategicÄƒ|tehnicÄƒ"
  }}
}}

AnalizeazÄƒ Ã®n profunzime piaÈ›a de construcÈ›ii din RomÃ¢nia È™i oferÄƒ insights strategice."""

        try:
            resp = self.gpt4.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=2000
            )
            
            content = resp.choices[0].message.content.strip()
            content = re.sub(r'^```[a-z]*\n?', '', content)
            content = re.sub(r'\n?```$', '', content)
            
            analysis = json.loads(content)
            print("âœ… GPT-4 a analizat serviciile de construcÈ›ii")
            return analysis
            
        except Exception as e:
            print(f"âš ï¸ Eroare GPT-4 analizÄƒ: {e}")
            return self.default_construction_analysis()

    def build_gpt4_context(self, site_data: Dict) -> str:
        """ConstruieÈ™te contextul pentru GPT-4 din datele site-ului"""
        context_parts = []
        
        # AdaugÄƒ informaÈ›ii generale
        context_parts.append(f"DOMENIU: {site_data['domain']}")
        context_parts.append(f"TOTAL PAGINI: {site_data['total_pages']}")
        
        # AdaugÄƒ conÈ›inutul cel mai relevant
        for page in site_data['pages'][:10]:  # Primele 10 pagini
            title = page.get('title', '')
            description = page.get('description', '')
            content = page.get('content', '')[:800]  # Primul 800 caractere
            services = page.get('services_detected', [])
            contact = page.get('contact_info', {})
            
            page_context = f"""
PAGINÄ‚: {title}
DESCRIERE: {description}
SERVICII DETECTATE: {', '.join(services)}
CONTACT: {contact}
CONÈšINUT: {content}
---"""
            context_parts.append(page_context)
        
        return '\n'.join(context_parts)

    def default_construction_analysis(self) -> Dict:
        """AnalizÄƒ de rezervÄƒ pentru construcÈ›ii"""
        return {
            "company_analysis": {
                "company_name": "Companie ConstrucÈ›ii",
                "main_location": "RomÃ¢nia",
                "company_size": "mic",
                "years_experience": "5+",
                "unique_selling_points": ["servicii complete construcÈ›ii"]
            },
            "services_identified": [{
                "service_name": "ConstrucÈ›ii generale",
                "category": "constructii_generale",
                "description": "Servicii de construcÈ›ii",
                "target_market": "clienÈ›i privaÈ›i",
                "estimated_demand": "ridicatÄƒ",
                "competition_level": "ridicatÄƒ",
                "price_positioning": "mediu",
                "growth_potential": "mediu"
            }],
            "market_opportunities": [],
            "digital_presence": {
                "website_quality": "medie",
                "seo_optimization": "medie",
                "social_media_present": "nu",
                "online_reputation": "necunoscutÄƒ",
                "improvement_areas": ["SEO", "social media"]
            },
            "agent_personality": {
                "role": "Specialist construcÈ›ii Ã®n RomÃ¢nia",
                "expertise_areas": ["construcÈ›ii", "renovÄƒri"],
                "communication_style": "profesional",
                "key_knowledge": ["piaÈ›a construcÈ›ii RomÃ¢nia"],
                "response_approach": "practicÄƒ"
            }
        }

    def save_site_analysis(self, site_url: str, site_data: Dict, analysis: Dict):
        """SalveazÄƒ analiza Ã®n bazele de date"""
        domain = tldextract.extract(site_url).top_domain_under_public_suffix.lower()
        
        try:
            # SalveazÄƒ Ã®n MongoDB
            site_record = {
                "domain": domain,
                "url": site_url,
                "analysis": analysis,
                "site_data": site_data,
                "analyzed_at": datetime.now(),
                "status": "analyzed"
            }
            
            self.sites_collection.replace_one(
                {"domain": domain}, 
                site_record, 
                upsert=True
            )
            
            # SalveazÄƒ serviciile separate
            for service in analysis.get('services_identified', []):
                service_record = {
                    "domain": domain,
                    "service_name": service.get('service_name'),
                    "category": service.get('category'),
                    "service_data": service,
                    "created_at": datetime.now()
                }
                
                self.services_collection.insert_one(service_record)
            
            # CreeazÄƒ embeddings pentru Qdrant
            self.create_site_embeddings(domain, site_data, analysis)
            
            print(f"âœ… AnalizÄƒ salvatÄƒ pentru {domain}")
            
        except Exception as e:
            print(f"âš ï¸ Eroare salvare analizÄƒ: {e}")

    def create_site_embeddings(self, domain: str, site_data: Dict, analysis: Dict):
        """CreeazÄƒ embeddings pentru site È™i analizÄƒ"""
        try:
            # Text pentru embedding
            embedding_text = f"""
            {analysis.get('company_analysis', {}).get('company_name', '')}
            {' '.join([s.get('service_name', '') for s in analysis.get('services_identified', [])])}
            {' '.join([s.get('description', '') for s in analysis.get('services_identified', [])])}
            {analysis.get('company_analysis', {}).get('main_location', '')}
            """
            
            # GenereazÄƒ embedding
            embedding = self.embedding_model.encode(embedding_text).tolist()
            
            # SalveazÄƒ Ã®n Qdrant
            point = PointStruct(
                id=hash(domain) % (2**63),
                vector=embedding,
                payload={
                    "domain": domain,
                    "company_name": analysis.get('company_analysis', {}).get('company_name', ''),
                    "services": [s.get('service_name', '') for s in analysis.get('services_identified', [])],
                    "categories": [s.get('category', '') for s in analysis.get('services_identified', [])],
                    "location": analysis.get('company_analysis', {}).get('main_location', ''),
                    "timestamp": time.time()
                }
            )
            
            self.qdrant.upsert(collection_name="construction_sites", points=[point])
            print(f"âœ… Embeddings create pentru {domain}")
            
        except Exception as e:
            print(f"âš ï¸ Eroare creare embeddings: {e}")

    def create_site_agent(self, site_url: str) -> Dict:
        """CreeazÄƒ agentul AI specializat pentru site-ul de construcÈ›ii"""
        print(f"ğŸ¤– Creez agent AI pentru site-ul: {site_url}")
        
        # AnalizeazÄƒ site-ul
        analysis = self.analyze_construction_site(site_url)
        
        # CreeazÄƒ personalitatea agentului
        agent_config = self.create_agent_personality(analysis)
        
        # SalveazÄƒ agentul
        self.save_agent_config(site_url, agent_config)
        
        print(f"âœ… Agent AI creat pentru {site_url}")
        return agent_config

    def create_agent_personality(self, analysis: Dict) -> Dict:
        """CreeazÄƒ personalitatea agentului bazatÄƒ pe analizÄƒ"""
        
        personality = analysis.get('agent_personality', {})
        services = analysis.get('services_identified', [])
        company = analysis.get('company_analysis', {})
        
        agent_config = {
            "agent_id": f"construction_agent_{int(time.time())}",
            "role": personality.get('role', 'Specialist construcÈ›ii'),
            "expertise": personality.get('expertise_areas', ['construcÈ›ii generale']),
            "communication_style": personality.get('communication_style', 'profesional'),
            "knowledge_base": {
                "company_info": company,
                "services_offered": services,
                "market_knowledge": analysis.get('market_opportunities', []),
                "digital_insights": analysis.get('digital_presence', {})
            },
            "response_templates": {
                "greeting": f"BunÄƒ ziua! Sunt {personality.get('role', 'specialistul')} acestei companii. Cu ce vÄƒ pot ajuta Ã®n domeniul construcÈ›iilor?",
                "services_inquiry": "Oferim urmÄƒtoarele servicii principale: {services_list}. Despre care dintre acestea doriÈ›i sÄƒ aflaÈ›i mai multe?",
                "pricing_inquiry": "Pentru o ofertÄƒ personalizatÄƒ, vÄƒ recomand sÄƒ ne contactaÈ›i direct. PreÈ›urile variazÄƒ Ã®n funcÈ›ie de complexitatea proiectului.",
                "recommendation": "Pe baza experienÈ›ei noastre Ã®n piaÈ›a romÃ¢neascÄƒ, vÄƒ recomand urmÄƒtoarea abordare:"
            },
            "specialized_knowledge": {
                "regulations": self.get_relevant_regulations(services),
                "market_trends": self.get_market_trends(services),
                "best_practices": self.get_best_practices(services)
            },
            "created_at": datetime.now().isoformat()
        }
        
        return agent_config

    def get_relevant_regulations(self, services: List[Dict]) -> List[str]:
        """ObÈ›ine reglementÄƒrile relevante pentru serviciile identificate"""
        regulations = []
        
        for service in services:
            category = service.get('category', '')
            if category in self.construction_domains:
                regulations.extend(self.construction_domains[category]['regulations'])
        
        return list(set(regulations))

    def get_market_trends(self, services: List[Dict]) -> List[str]:
        """ObÈ›ine tendinÈ›ele pieÈ›ei pentru serviciile identificate"""
        trends = [
            "CreÈ™terea cererii pentru eficienÈ›Äƒ energeticÄƒ",
            "Adoptarea tehnologiilor smart Ã®n construcÈ›ii",
            "Focus pe materiale sustenabile È™i eco-friendly",
            "Digitalizarea proceselor de proiectare È™i execuÈ›ie",
            "Cererea crescutÄƒ pentru renovÄƒri post-pandemie"
        ]
        
        return trends

    def get_best_practices(self, services: List[Dict]) -> List[str]:
        """ObÈ›ine best practices pentru serviciile identificate"""
        practices = [
            "Respectarea strictÄƒ a normelor de siguranÈ›Äƒ",
            "Utilizarea materialelor certificate",
            "Planificarea detaliatÄƒ a proiectelor",
            "Comunicarea transparentÄƒ cu clienÈ›ii",
            "Respectarea termenelor È™i bugetelor"
        ]
        
        return practices

    def save_agent_config(self, site_url: str, agent_config: Dict):
        """SalveazÄƒ configuraÈ›ia agentului"""
        domain = tldextract.extract(site_url).top_domain_under_public_suffix.lower()
        
        try:
            agent_record = {
                "domain": domain,
                "site_url": site_url,
                "agent_config": agent_config,
                "agent_type": "construction_specialist",
                "status": "active",
                "created_at": datetime.now(),
                "last_updated": datetime.now()
            }
            
            self.agents_collection.replace_one(
                {"domain": domain},
                agent_record,
                upsert=True
            )
            
            print(f"âœ… ConfiguraÈ›ie agent salvatÄƒ pentru {domain}")
            
        except Exception as e:
            print(f"âš ï¸ Eroare salvare agent: {e}")

def main():
    parser = argparse.ArgumentParser(description='Construction Agent Creator')
    parser.add_argument('--url', required=True, help='URL site construcÈ›ii')
    parser.add_argument('--mode', choices=['analyze', 'create_agent'], required=True, help='Modul de funcÈ›ionare')
    
    args = parser.parse_args()
    
    creator = ConstructionAgentCreator()
    
    if args.mode == 'analyze':
        result = creator.analyze_construction_site(args.url)
        print("\nğŸ—ï¸ ANALIZÄ‚ COMPLETÄ‚:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
    elif args.mode == 'create_agent':
        agent_config = creator.create_site_agent(args.url)
        print("\nğŸ¤– AGENT CREAT:")
        print(json.dumps(agent_config, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()
