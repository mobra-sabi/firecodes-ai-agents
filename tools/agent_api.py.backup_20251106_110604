# agent_api.py
searcher = None
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Query, BackgroundTasks, Body, Request
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pymongo import MongoClient
from bson import ObjectId
from pydantic import BaseModel
from typing import Optional, List, Dict
import os
import requests
import socket
import urllib.robotparser as robotparser
from urllib.parse import urlparse
import asyncio
import logging
import traceback
from datetime import datetime, timezone
from mirror_agent_manifest import MirrorManifestManager, create_mirror_manifest_for_site
from mirror_agent_router import MirrorRouterManager, route_mirror_question
from mirror_kpi_testing import run_kpi_test_for_site, get_kpi_history, get_kpi_stats
from mirror_curator import run_curator_cycle_for_site, get_curator_dashboard_for_site, get_curator_stats
from mirror_security import validate_mirror_security, scrub_mirror_content, validate_cross_domain_mirror, get_security_stats
from qdrant_mirror_collections import QdrantMirrorCollections, create_mirror_collections_for_site
from mirror_dashboard import get_global_mirror_dashboard, get_site_mirror_dashboard, create_mirror_alert, resolve_mirror_alert
from mirror_qa_generation import generate_mirror_qa_set, get_qa_generation_stats
from urllib.parse import urlparse
import json
from openai import OpenAI
from dual_orchestrator import DualOrchestrator
from knowledge_architecture import KnowledgeArchitecture
from ai_roles_orchestrator import AIRolesOrchestrator
from industry_aggregator import IndustryAggregator
from gap_analyzer import GapAnalyzer
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from competitor_discovery_system import run_advanced_discovery
from dataclasses import asdict

def _norm_host(h: str) -> str:
    h = (h or "").strip().lower()
    if h.startswith("www."): h = h[4:]
    return h

def check_sealed_solutions_before_action(problem_description: str) -> bool:
    """
    VerificƒÉ dacƒÉ existƒÉ o solu»õie sigilatƒÉ pentru aceastƒÉ problemƒÉ.
    ReturneazƒÉ True dacƒÉ existƒÉ o solu»õie sigilatƒÉ (nu se executƒÉ ac»õiunea),
    False dacƒÉ nu existƒÉ (se poate executa ac»õiunea).
    """
    sealed_solution = check_sealed_solution(problem_description)
    if sealed_solution:
        logger.info(f"üîí SOLU»öIE SIGILATƒÇ GƒÇSITƒÇ pentru: {problem_description}")
        logger.info(f"   Solu»õia: {sealed_solution.solution}")
        logger.info(f"   SigilatƒÉ la: {sealed_solution.sealed_at}")
        logger.info(f"   Nu se executƒÉ ac»õiunea - solu»õia este deja implementatƒÉ »ôi func»õionalƒÉ")
        return True
    return False

def _host_from_url(u: str) -> str:
    try:
        return _norm_host(urlparse(u).netloc)
    except Exception:
        return ""

def _match_domain(payload: dict, domain: str) -> bool:
    want = _norm_host(domain)
    pd = _norm_host((payload or {}).get("domain",""))
    pu = _host_from_url((payload or {}).get("url",""))
    def ok(h): return h == want or (h.endswith("."+want) if h else False)
    return ok(pd) or ok(pu)

def get_qwen_client():
    """Get Qwen client configured for local GPU server"""
    return OpenAI(
        api_key=os.getenv("QWEN_API_KEY", "local"),
        base_url=os.getenv("QWEN_BASE_URL", "http://localhost:9304/v1")
    )

def get_llm_model():
    """Get the configured LLM model (Qwen or OpenAI fallback)"""
    qwen_url = os.getenv("QWEN_BASE_URL")
    if qwen_url:
        return os.getenv("QWEN_MODEL", "qwen2.5")
    return os.getenv("LLM_MODEL", "gpt-4o-mini")

def get_llm_client():
    """Get LLM client (Qwen preferred, OpenAI fallback)"""
    qwen_url = os.getenv("QWEN_BASE_URL")
    if qwen_url:
        return get_qwen_client()
    else:
        return OpenAI(
            organization=os.getenv("OPENAI_ORG_ID"),
            project=os.getenv("OPENAI_PROJECT"),
            api_key=os.getenv("OPENAI_API_KEY"),
        )


# existente
from site_agent_creator import create_site_agent_ws
from task_executor import handle_task_conversation

# Enhanced 4-Layer Agent
from enhanced_4_layer_agent import Enhanced4LayerAgent, create_enhanced_agent

# Simple Working Agent
from simple_working_agent import SimpleWorkingAgent, create_simple_working_agent

# Smart Advisor Agent
from smart_advisor_agent import SmartAdvisorAgent, create_smart_advisor_agent

# Site-Specific Intelligence
from site_specific_intelligence import SiteSpecificIntelligence, create_site_specific_intelligence
from auto_site_extractor import AutoSiteExtractor
from agent_health_checker import AgentHealthChecker

# Health Checker
from health_checker import HealthChecker, quick_health_check

# GPT-5 + Qwen 2.5 Architecture
from gpt5_qwen_architecture import GPT5QwenArchitecture, create_gpt5_qwen_architecture

# admin (competitor discovery + ingest)
from tools.admin_discovery import ingest_urls, web_search
from adapters.scraper_adapter import smart_fetch
from adapters.search_providers import search_serp
from langchain_openai import ChatOpenAI

# <<< NEW: config DB unificat >>>
from config.database_config import (
    MONGODB_URI, MONGODB_DATABASE, MONGODB_COLLECTION
)

app = FastAPI()
searcher = None
dual_orchestrator = None
knowledge_architecture = None
ai_roles_orchestrator = None
industry_aggregator = None
gap_analyzer = None

def get_dual_orchestrator():
    global dual_orchestrator
    if dual_orchestrator is None:
        from qdrant_client import QdrantClient
        qdrant_client = QdrantClient(url="http://localhost:6333")
        dual_orchestrator = DualOrchestrator(db, qdrant_client)
    return dual_orchestrator

def get_knowledge_architecture():
    global knowledge_architecture
    if knowledge_architecture is None:
        from qdrant_client import QdrantClient
        qdrant_client = QdrantClient(url="http://localhost:6333")
        knowledge_architecture = KnowledgeArchitecture(db, qdrant_client)
    return knowledge_architecture

def get_ai_roles_orchestrator():
    global ai_roles_orchestrator
    if ai_roles_orchestrator is None:
        from qdrant_client import QdrantClient
        qdrant_client = QdrantClient(url="http://localhost:6333")
        ai_roles_orchestrator = AIRolesOrchestrator(db, qdrant_client)
    return ai_roles_orchestrator

def get_industry_aggregator():
    global industry_aggregator
    if industry_aggregator is None:
        from qdrant_client import QdrantClient
        qdrant_client = QdrantClient(url="http://localhost:6333")
        industry_aggregator = IndustryAggregator(db, qdrant_client)
    return industry_aggregator

def get_gap_analyzer():
    global gap_analyzer
    if gap_analyzer is None:
        from qdrant_client import QdrantClient
        qdrant_client = QdrantClient(url="http://localhost:6333")
        gap_analyzer = GapAnalyzer(db, qdrant_client)
    return gap_analyzer

def get_searcher():
    global searcher
    if searcher is None:
        from retrieval.semantic_search import SemanticSearcher
        searcher = SemanticSearcher()
    return searcher

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# ------ Health/Ready ------

# ------ Chat Interface ------
@app.get("/chat")
async def chat_page():
    """Pagina de chat cu agen»õii"""
    return FileResponse("static/chat.html")

@app.post("/ask")
async def ask_question(request: Request):
    """
    Endpoint principal pentru √ÆntrebƒÉri - folose»ôte inteligen»õa specificƒÉ site-ului
    """
    try:
        # Extrage parametrii din request
        body = await request.json()
        question = body.get("question")
        agent_id = body.get("agent_id")
        conversation_history = body.get("conversation_history", [])
        
        if not question or not agent_id:
            raise HTTPException(status_code=400, detail="question and agent_id are required")
        
        # Ob»õine informa»õiile agentului selectat
        agent = db.site_agents.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        
        # Folose»ôte site-ul agentului selectat
        domain = agent.get("domain", "example.com")
        site_url = f"https://{domain}"
        
        # CreeazƒÉ inteligen»õa specificƒÉ site-ului
        intelligence = await create_site_specific_intelligence(site_url)
        
        # GenereazƒÉ rƒÉspuns specific site-ului cu contextul conversa»õiei
        logger.info(f"üîç Conversation history received: {len(conversation_history)} messages")
        logger.info(f"üîç Conversation history content: {conversation_history}")
        response = await intelligence.generate_site_specific_response(question, conversation_history)
        logger.info(f"üîç Response from generate_site_specific_response: {response[:200] if response else 'EMPTY/NONE'}...")
        
        # GenereazƒÉ √ÆntrebƒÉri contextuale
        contextual_questions = await intelligence.generate_contextual_questions(question)
        
        # VerificƒÉ dacƒÉ a fost folosit web search
        web_search_used = intelligence._needs_web_search(question)
        web_sources = []
        if web_search_used:
            search_query = intelligence._build_search_query(question)
            web_sources = intelligence.web_search(search_query, num_results=3)
        
        # SalveazƒÉ conversa»õia pentru √ÆnvƒÉ»õarea Qwen
        try:
            save_conversation(
                agent_id=agent_id,
                user_message=question,
                assistant_response=response,
                strategy="manual_task"
            )
            logger.info(f"‚úÖ Conversa»õia salvatƒÉ pentru √ÆnvƒÉ»õarea Qwen: {agent_id}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Nu s-a putut salva conversa»õia: {e}")

        # ReturneazƒÉ rƒÉspunsul √Æn formatul a»ôteptat de interfa»õƒÉ
        return {
            "ok": True,
            "response": response,
            "confidence": 0.98,
            "reasoning": f"Site-specific intelligence pentru {domain} cu GPT-5 »ôi date din baza de date",
            "sources": [{"url": site_url, "score": 0.95}],
            "web_search_used": web_search_used,
            "web_sources": web_sources,
            "agent_id": agent_id,
            "llm_used": os.getenv("LLM_MODEL", "gpt-4o"),
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "guardrails": {
                "passed": True,
                "message": "All security checks passed",
                "confidence_check": True
            },
            "contextual_questions": contextual_questions,
            "competitive_advantage": intelligence.get_competitive_advantage_summary(),
            "site_context": {
                "business_type": intelligence.site_context.business_type if intelligence.site_context else "unknown",
                "target_audience": intelligence.site_context.target_audience if intelligence.site_context else "unknown",
                "unique_selling_points": intelligence.site_context.unique_selling_points if intelligence.site_context else []
            }
        }
        
    except Exception as e:
        logger.error(f"Error in /ask endpoint: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/ready")
async def ready():
    # conexiune minimƒÉ la Mongo (best-effort)
    try:
        mongo_client.admin.command("ping")
        mongo_ok = True
    except Exception:
        mongo_ok = False
    return {"ok": mongo_ok}
@app.get("/debug/search")
async def debug_search(q: str, domain: Optional[str] = None):
    """
    Test: /debug/search?q=termeni%20livrare&domain=firestopping.ro
    """
    try:
        all_results = get_searcher().search(q, domain=None)  # maximƒÉ acoperire
        if domain:
            pref = [r for r in all_results if _match_domain(r, domain)]
            rest = [r for r in all_results if not _match_domain(r, domain)]
            results = (pref + rest)[:8]  # TOP_K_FINAL
        else:
            results = all_results
        return {"ok": True, "count": len(results), "results": results}
    except Exception as e:
        return {"ok": False, "error": str(e)}

    except Exception as e:
        return {"ok": False, "error": str(e)}

    except Exception as e:
        return {"ok": False, "error": str(e)}

    except Exception as e:
        return {"ok": False, "error": str(e)}

    except Exception as e:
        return {"ok": False, "error": str(e)}

# <<< CHANGED: unificare Mongo »ôi colec»õii >>>
mongo_client = MongoClient(MONGODB_URI)
db = mongo_client[MONGODB_DATABASE]
agents_collection = db.site_agents
conversations_collection = db.conversations
site_content_col = db[MONGODB_COLLECTION]

logger = logging.getLogger("agent_api")
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))

# --- Crawl summary endpoint (pages, chars, qdrant points, recent pages) ---
@app.get("/agents/{agent_id}/crawl/summary")
async def crawl_summary(agent_id: str):
    try:
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        domain = (_host_from_url(agent.get("site_url", "")) or agent.get("domain") or "").lower()

        query = {"url": {"$regex": domain, "$options": "i"}} if domain else {}
        pages_count = site_content_col.count_documents(query) if query else 0
        chars_total = 0
        recent_pages = []
        if query and pages_count:
            cur = site_content_col.find(query, {"url": 1, "title": 1, "content": 1}).sort("_id", -1).limit(50)
            for doc in cur:
                content = (doc.get("content") or "")
                ln = len(content)
                chars_total += ln
                if len(recent_pages) < 10:
                    recent_pages.append({
                        "url": doc.get("url", ""),
                        "title": doc.get("title", ""),
                        "content_length": ln
                    })

        qdrant_points = 0
        try:
            from qdrant_client import QdrantClient
            qc = QdrantClient(url=os.getenv("QDRANT_URL", "http://localhost:6333"))
            collection = f"agent_{agent_id}_content"
            try:
                info = qc.get_collection(collection)
                qdrant_points = getattr(info, "points_count", 0) or 0
            except Exception:
                qdrant_points = 0
        except Exception:
            qdrant_points = 0

        return {
            "ok": True,
            "agent_id": agent_id,
            "domain": domain,
            "pages_count": pages_count,
            "chars_total": chars_total,
            "qdrant_points": qdrant_points,
            "recent_pages": recent_pages
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Crawl summary error: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/ingest-minimal")
async def ingest_minimal(agent_id: str, pages: int = 1):
    """Ingest rapid »ôi robust: descarcƒÉ prima paginƒÉ a site-ului agentului »ôi o salveazƒÉ √Æn Mongo.
    EvitƒÉ procesƒÉri complexe ca sƒÉ prevenim inserƒÉri gre»ôite.
    """
    try:
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        site_url = agent.get("site_url") or ("https://" + (agent.get("domain") or ""))
        if not site_url:
            raise HTTPException(status_code=400, detail="Agent has no site_url or domain")

        import requests
        from bs4 import BeautifulSoup

        r = requests.get(site_url, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()
        title = (soup.find("title").get_text().strip() if soup.find("title") else site_url)[:200]
        body = soup.get_text(" ", strip=True)

        content_text = body[:8000]
        if len(content_text) < 50:
            return {"ok": False, "error": "Content too short from site; cannot ingest."}

        site_content_col.insert_one({
            "agent_id": ObjectId(agent_id),
            "url": site_url,
            "title": title,
            "content": content_text,
            "created_at": datetime.now(timezone.utc)
        })

        return {"ok": True, "saved_pages": 1, "title": title, "chars": len(content_text)}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ingest-minimal error: {e}")
        return {"ok": False, "error": str(e)}

def scrape_site_comprehensive(site_url, agent_id, max_pages=5):
    """
    Scraping comprehensiv al unui site cu multiple pagini pentru a ob»õine con»õinut complet.
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin, urlparse
        import time
        
        logger.info(f"Starting comprehensive scraping for {site_url}")
        
        # Headers pentru a evita blocarea
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ro-RO,ro;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        session = requests.Session()
        session.headers.update(headers)
        
        # SeteazƒÉ timeout-ul
        session.timeout = 15
        
        all_content = []
        visited_urls = set()
        urls_to_visit = [site_url]
        base_domain = urlparse(site_url).netloc
        
        page_count = 0
        while urls_to_visit and page_count < max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in visited_urls:
                continue
                
            visited_urls.add(current_url)
            page_count += 1
            
            logger.info(f"Scraping page {page_count}/{max_pages}: {current_url}")
            
            try:
                response = session.get(current_url, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # EliminƒÉ elemente nedorite
                for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    element.decompose()
                
                # Extrage titlul
                title = soup.find('title')
                title_text = title.get_text().strip() if title else ""
                
                # Extrage con»õinutul principal
                main_content = ""
                for selector in ['main', 'article', '.content', '.main-content', '.container', 'body']:
                    elements = soup.select(selector)
                    if elements:
                        for element in elements:
                            text = element.get_text(separator=' ', strip=True)
                            if len(text) > len(main_content):
                                main_content = text
                
                # LimiteazƒÉ con»õinutul per paginƒÉ
                if main_content:
                    main_content = main_content[:5000]
                    all_content.append(f"=== {title_text} ===\n{main_content}")
                
                # GƒÉse»ôte link-uri interne pentru scraping suplimentar
                if page_count < max_pages:
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        full_url = urljoin(current_url, href)
                        
                        # VerificƒÉ dacƒÉ e link intern
                        if (urlparse(full_url).netloc == base_domain and 
                            full_url not in visited_urls and 
                            full_url not in urls_to_visit and
                            '#' not in full_url.split('/')[-1] and
                            not any(x in full_url for x in ['?', 'mailto:', 'tel:', '.pdf', '.doc'])):
                            urls_to_visit.append(full_url)
                            if len(urls_to_visit) >= 10:  # LimiteazƒÉ link-urile de urmƒÉrit
                                break
                
                time.sleep(1)  # Respect pentru server
                
            except Exception as e:
                logger.warning(f"Error scraping {current_url}: {e}")
                continue
        
        # CombinƒÉ tot con»õinutul
        combined_content = "\n\n".join(all_content)
        
        # SalveazƒÉ con»õinutul √Æn baza de date
        if combined_content:
            content_doc = {
                "agent_id": ObjectId(agent_id),
                "url": site_url,
                "content": combined_content[:15000],  # LimiteazƒÉ con»õinutul total
                "timestamp": datetime.now(timezone.utc),
                "source": "comprehensive_scraping",
                "pages_scraped": len(all_content),
                "total_length": len(combined_content)
            }
            db.site_content.insert_one(content_doc)
            logger.info(f"Saved comprehensive content: {len(combined_content)} chars from {len(all_content)} pages")
        
        return combined_content[:12000]  # ReturneazƒÉ con»õinutul pentru prompt
        
    except Exception as e:
        logger.error(f"Comprehensive scraping failed: {e}")
        # Fallback la scraping simplu
        try:
            r = requests.get(site_url, timeout=15)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            main_content = soup.find("main") or soup.find("article") or soup.find("body")
            if main_content:
                return main_content.get_text(" ", strip=True)[:5000]
            else:
                return soup.get_text(" ", strip=True)[:5000]
        except:
            return f"Site URL: {site_url}\nDomain: {urlparse(site_url).netloc}"

def get_api_key():
    # PreferƒÉ Qwen dacƒÉ este configurat »ôi func»õional
    qwen_url = os.getenv("QWEN_BASE_URL")
    if qwen_url and qwen_url != "http://localhost:9304/v1":  # Doar dacƒÉ nu e local
        return os.getenv("QWEN_API_KEY", "local")
    
    # Fallback la OpenAI
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="CRITICAL: No LLM API key configured. Check config.env/.env file.")
    return api_key

def save_conversation(agent_id: str, user_message: str, assistant_response: str, strategy: str = "manual_task"):
    """SalveazƒÉ conversa»õia √Æn baza de date pentru √ÆnvƒÉ»õare"""
    try:
        conversation_doc = {
            "agent_id": ObjectId(agent_id),
            "timestamp": datetime.now(timezone.utc),
            "strategy": strategy,
            "user_message": user_message,
            "assistant_response": assistant_response,
            "conversation_length": len(user_message) + len(assistant_response)
        }
        conversations_collection.insert_one(conversation_doc)
        
        # IndexeazƒÉ »ôi pentru √ÆnvƒÉ»õare Qwen
        try:
            qwen_learning_doc = {
                "agent_id": ObjectId(agent_id),
                "timestamp": datetime.now(timezone.utc),
                "action_type": "conversation",
                "user_message": user_message,
                "agent_response": response,
                "context_used": context_used,
                "for_qwen_learning": True
            }
            db.qwen_learning_data.insert_one(qwen_learning_doc)
            logger.info(f"[LEARNING] Indexed conversation for Qwen learning: {agent_id}")
        except Exception as e:
            logger.warning(f"Failed to index conversation for learning: {e}")
            
        logger.info(f"[CONVERSATION] Saved conversation for agent {agent_id}")
    except Exception as e:
        logger.error(f"[CONVERSATION] Failed to save conversation: {e}")

def get_learning_strategy_prompt(agent_id: str, site_url: str, industry: str, context: str) -> str:
    """GenereazƒÉ promptul pentru strategia de √ÆnvƒÉ»õare din industrie cu task-uri concrete"""
    return f"""
E»ôti un expert √Æn analiza industriei »ôi dezvoltarea strategiilor de √ÆnvƒÉ»õare pentru agen»õi AI.

AGENT ANALIZAT:
- ID: {agent_id}
- Site: {site_url}
- Industrie detectatƒÉ: {industry}
- Context complet din baza de date: {context}

OBIECTIV: DezvoltƒÉ o strategie COMPLET PERSONALIZATƒÇ cu TASK-URI CONCRETE pentru acest agent specific.

ANALIZEAZƒÇ √éN PROFUNZIME:
1. **CON»öINUTUL SITE-ULUI:** AnalizeazƒÉ fiecare detaliu din contextul furnizat
2. **SERVICIILE/PRODUSELE:** IdentificƒÉ exact ce oferƒÉ acest business
3. **SEGMENTUL DE PIA»öƒÇ:** DeterminƒÉ »õinta specificƒÉ a acestui business
4. **POZI»öIONAREA:** √én»õelege cum se diferen»õiazƒÉ de competitori
5. **TERMINOLOGIA TEHNICƒÇ:** Extrage termeni specifici ai industriei
6. **PAIN POINTS:** IdentificƒÉ problemele pe care le rezolvƒÉ
7. **SOLU»öII UNICE:** GƒÉse»ôte ce face diferit de al»õii

RƒÇSPUNDE √éN FORMAT JSON EXACT:
{{
  "business_analysis": {{
    "services_products": ["lista serviciilor/produselor identificate"],
    "target_market": "segmentul de pia»õƒÉ specific",
    "unique_positioning": "pozi»õionarea unicƒÉ",
    "key_strengths": ["punctele forte identificate"]
  }},
  "learning_strategy": {{
    "industry_insights": "informa»õii specifice despre industria lor",
    "competitor_analysis": "analiza competitorilor din con»õinut",
    "market_trends": "tendin»õe relevante pentru business-ul lor",
    "growth_opportunities": "oportunitƒÉ»õi specifice de dezvoltare"
  }},
  "concrete_tasks": [
    {{
      "task_id": "extract_keywords",
      "title": "Extrage cuvinte cheie din site",
      "description": "IdentificƒÉ cuvintele cheie specifice pentru serviciile/produsele acestui business",
      "primary_keywords": ["cuv√¢nt principal 1", "cuv√¢nt principal 2", "cuv√¢nt principal 3"],
      "secondary_keywords": ["cuv√¢nt secundar 1", "cuv√¢nt secundar 2", "cuv√¢nt secundar 3"],
      "technical_terms": ["termen tehnic 1", "termen tehnic 2", "termen tehnic 3"],
      "search_queries": [
        "query principalƒÉ 1", "query principalƒÉ 2", "query principalƒÉ 3",
        "query secundarƒÉ 1", "query secundarƒÉ 2", "query secundarƒÉ 3",
        "query long-tail 1", "query long-tail 2", "query long-tail 3"
      ],
      "status": "pending"
    }},
    {{
      "task_id": "find_similar_sites",
      "title": "GƒÉse»ôte site-uri similare",
      "description": "CautƒÉ »ôi identificƒÉ site-uri similare folosind strategia de cƒÉutare √Æn cascadƒÉ",
      "search_strategy": {{
        "primary_search": {{
          "keywords": "cuvintele cheie principale",
          "target_count": 15,
          "description": "CƒÉutare cu cuvintele cheie principale pentru competitori direc»õi"
        }},
        "secondary_search": {{
          "keywords": "cuvintele cheie secundare", 
          "target_count": 10,
          "description": "CƒÉutare cu cuvintele cheie secundare pentru competitori indirecti"
        }},
        "technical_search": {{
          "keywords": "termenii tehnici",
          "target_count": 8,
          "description": "CƒÉutare cu termenii tehnici pentru ni»ôe specifice"
        }},
        "longtail_search": {{
          "keywords": "query-uri long-tail",
          "target_count": 7,
          "description": "CƒÉutare cu query-uri long-tail pentru oportunitƒÉ»õi noi"
        }}
      }},
      "total_target_count": 40,
      "status": "pending"
    }},
    {{
      "task_id": "create_competitor_agents",
      "title": "CreeazƒÉ agen»õi competitori",
      "description": "TransformƒÉ site-urile similare √Æn agen»õi pentru √ÆnvƒÉ»õare",
      "target_count": 10,
      "status": "pending"
    }},
    {{
      "task_id": "analyze_competitors",
      "title": "AnalizeazƒÉ competitorii",
      "description": "ComparƒÉ serviciile »ôi strategiile competitorilor identifica»õi",
      "status": "pending"
    }},
    {{
      "task_id": "generate_insights",
      "title": "GenereazƒÉ insights strategice",
      "description": "CreeazƒÉ recomandƒÉri concrete bazate pe analiza competitorilor",
      "status": "pending"
    }}
  ],
  "success_metrics": {{
    "keywords_extracted": 0,
    "similar_sites_found": 0,
    "competitor_agents_created": 0,
    "insights_generated": 0
  }}
}}

CRITICAL REQUIREMENTS:
- RƒÉspunde DOAR cu JSON valid, fƒÉrƒÉ text suplimentar, fƒÉrƒÉ markdown, fƒÉrƒÉ explica»õii
- NU REPETA NICIODATƒÇ aceea»ôi valoare √Æn array-uri - fiecare cuv√¢nt cheie trebuie sƒÉ fie UNIC
- NU REPETA structura JSON, genereazƒÉ o singurƒÉ structurƒÉ completƒÉ
- AsigurƒÉ-te cƒÉ JSON-ul este valid »ôi poate fi parsat
- BazeazƒÉ-te EXCLUSIV pe con»õinutul furnizat
- Cuvintele cheie trebuie sƒÉ fie SPECIFICE »ôi RELEVANTE pentru serviciile/produsele lor
- Query-urile de cƒÉutare trebuie sƒÉ fie √Æn rom√¢nƒÉ »ôi sƒÉ acopere toate aspectele business-ului
- Fiecare task trebuie sƒÉ fie ac»õionabil »ôi mƒÉsurabil
- Strategia de cƒÉutare trebuie sƒÉ fie COMPLETƒÇ - sƒÉ acopere toate sub-domeniile
- Cuvintele cheie principale = servicii/produse core (EXACT 5 cuvinte UNICE)
- Cuvintele cheie secundare = servicii/produse complementare (EXACT 5 cuvinte UNICE)
- Termenii tehnici = specifica»õii, tehnologii, standarde (EXACT 5 termeni UNICI)
- Query-urile long-tail = combina»õii specifice pentru ni»ôe (EXACT 9 query-uri UNICE)
- IMPORTANT: DacƒÉ nu ai suficiente informa»õii √Æn context, folose»ôte termeni generici dar RELEVAN»öI
- STOP imediat dupƒÉ ultimul }} - nu adƒÉuga nimic altceva
- MAXIM 1000 TOKENI - rƒÉspunde concis »ôi precis
"""

# ------------- UI -------------
@app.get("/", response_class=HTMLResponse)
async def get_ui_root():
    with open("dual_mode_ui.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

# alias /ui (unele setup-uri folosesc exact acest path)
@app.get("/ui", response_class=HTMLResponse)
async def get_ui_alias():
    with open("dual_mode_ui.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

@app.get("/dual-mode", response_class=HTMLResponse)
async def get_dual_mode_ui():
    """Noua interfa»õƒÉ Dual-Mode cu Discovery & Competitor Onboarding"""
    with open("dual_mode_ui.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())
# ===== DUAL-MODE DISCOVERY & COMPETITOR ONBOARDING ENDPOINTS =====

@app.post("/agents/{agent_id}/discover")
async def start_discovery_pipeline(agent_id: str, request: dict = Body(...)):
    """Start Discovery Pipeline - SERP based competitor discovery"""
    try:
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "Agent not found"}
        
        # Parametrii pentru discovery
        include_subdomains = request.get("include_subdomains", True)
        respect_robots = request.get("respect_robots", True)
        language_filter = request.get("language_filter", True)
        max_depth = request.get("max_depth", 3)
        manual_url = request.get("manual_url")
        
        # DacƒÉ este URL manual, adaugƒÉ direct
        if manual_url:
            competitor_entry = {
                "master_agent_id": ObjectId(agent_id),
                "competitor_url": manual_url,
                "competitor_domain": urlparse(manual_url).netloc,
                "search_query": "manual_add",
                "relevance_score": 0.8,
                "research_timestamp": datetime.now(timezone.utc),
                "relationship_type": "competitor",
                "status": "discovered",
                "notes": "Added manually by user"
            }
            
            db.competitors.insert_one(competitor_entry)
            logger.info(f"‚úÖ Manual competitor added: {manual_url}")
            
            return {
                "ok": True,
                "message": f"Manual competitor {manual_url} added successfully",
                "competitor_added": True
            }
        
        # Altfel, ruleazƒÉ discovery automat
        logger.info(f"üöÄ Starting ADVANCED discovery pipeline for agent {agent_id}")
        
        # PAS 1: GenereazƒÉ strategia GPT-5 √Ænainte de cƒÉutare
        from competitor_discovery_system import AdvancedCompetitorDiscovery
        discovery_system = AdvancedCompetitorDiscovery()
        
        # Ob»õine agentul pentru analizƒÉ
        master_agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not master_agent:
            return {"ok": False, "error": "Master agent not found"}
        
        # GenereazƒÉ strategia GPT-5
        try:
            strategy = await discovery_system._generate_search_strategy(master_agent)
            
            # AdaugƒÉ strategia GPT-5 √Æn learning queue pentru Qwen
            try:
                orchestrator = get_dual_orchestrator()
                await orchestrator.add_to_learning_queue(
                    content=f"GPT-5 Strategy for {master_agent.get('name', '')}: {json.dumps(strategy, ensure_ascii=False)}",
                    source="gpt5",
                    metadata={
                        "agent_id": agent_id,
                        "type": "strategy",
                        "business_analysis": strategy.get('business_analysis', {}),
                        "keywords": strategy.get('keywords', [])
                    }
                )
                logger.info("üìö GPT-5 strategy added to learning queue")
            except Exception as e:
                logger.warning(f"Failed to add strategy to learning queue: {e}")
            
            # Afi»ôeazƒÉ strategia GPT-5
            logger.info(f"üß† GPT-5 Strategy Generated:")
            logger.info(f"   Business Analysis: {strategy.get('business_analysis', {})}")
            logger.info(f"   Keywords: {strategy.get('keywords', [])[:5]}...")
            logger.info(f"   Search Queries: {strategy.get('search_queries', [])[:3]}...")
            
        except Exception as e:
            logger.error(f"‚ùå GPT-5 Strategy Generation Failed: {e}")
            return {
                "ok": False,
                "error": f"GPT-5 analysis failed: {str(e)}",
                "gpt5_error": True,
                "message": "GPT-5 cannot analyze your site. Please check your OpenAI API key."
            }
        
        # PAS 2: Folose»ôte sistemul avansat de discovery »ôi learning
        discovery_result = await run_advanced_discovery(agent_id)
        
        if discovery_result["ok"]:
            logger.info(f"‚úÖ Advanced discovery completed: {discovery_result['competitors_found']} competitors analyzed")
            
            # AdaugƒÉ rezultatele discovery-ului √Æn learning queue pentru Qwen
            try:
                orchestrator = get_dual_orchestrator()
                await orchestrator.add_to_learning_queue(
                    content=f"Discovery Results for {master_agent.get('name', '')}: Found {discovery_result.get('competitors_found', 0)} competitors with analysis",
                    source="gpt5",
                    metadata={
                        "agent_id": agent_id,
                        "type": "discovery_results",
                        "competitors_found": discovery_result.get('competitors_found', 0),
                        "learning_opportunities": discovery_result.get('learning_opportunities', []),
                        "improvement_hints": discovery_result.get('improvement_hints', [])
                    }
                )
                logger.info("üìö Discovery results added to learning queue")
            except Exception as e:
                logger.warning(f"Failed to add discovery results to learning queue: {e}")
            
            # AdaugƒÉ strategia GPT-5 la rƒÉspuns
            discovery_result["gpt5_strategy"] = {
                "business_analysis": strategy.get("business_analysis", {}),
                "keywords": strategy.get("keywords", []),
                "search_queries": strategy.get("search_queries", []),
                "competitor_types": strategy.get("competitor_types", []),
                "analysis_focus": strategy.get("analysis_focus", [])
            }
            
            return discovery_result
        else:
            logger.error(f"‚ùå Advanced discovery failed: {discovery_result['error']}")
            return {
                "ok": False,
                "error": f"Advanced discovery failed: {discovery_result['error']}",
                "competitors_found": 0,
                "competitors": []
            }
        
    except Exception as e:
        logger.error(f"Error in discovery pipeline: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/agents/{agent_id}/candidates")
async def get_discovery_candidates(agent_id: str):
    """Get discovered candidates for onboarding"""
    try:
        # Ob»õine competitorii descoperi»õi (inclusiv cei analiza»õi de sistemul avansat)
        competitors = list(db.competitors.find({
            "master_agent_id": ObjectId(agent_id),
            "status": {"$in": ["discovered", "analyzed"]}
        }).sort("relevance_score", -1))
        
        candidates = []
        for competitor in competitors:
            # Folose»ôte discovery_timestamp sau research_timestamp
            timestamp = competitor.get("discovery_timestamp") or competitor.get("research_timestamp")
            discovered_at = timestamp.isoformat() if timestamp else "Unknown"
            
            candidates.append({
                "domain": competitor["competitor_domain"],
                "url": competitor["competitor_url"],
                "score": competitor["relevance_score"],
                "keywords": [competitor["search_query"]] if competitor.get("search_query") else [],
                "serp_position": competitor.get("serp_position", 1),
                "discovered_at": discovered_at,
                "learning_opportunities": competitor.get("learning_opportunities", [])[:3],  # Primele 3
                "improvement_hints": competitor.get("improvement_hints", [])[:3],  # Primele 3
                "competitor_strengths": competitor.get("competitor_strengths", [])[:2],  # Primele 2
                "master_weaknesses": competitor.get("master_weaknesses", [])[:2]  # Primele 2
            })
        
        return {
            "ok": True,
            "candidates": candidates,
            "total": len(candidates)
        }
        
    except Exception as e:
        logger.error(f"Error getting candidates: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/onboard")
async def onboard_competitors(agent_id: str, request: dict = Body(...)):
    """Onboard selected competitors as sub-agents cu sistem avansat de learning"""
    try:
        candidates = request.get("candidates", [])
        if not candidates:
            return {"ok": False, "error": "No candidates provided"}
        
        onboarded_count = 0
        learning_opportunities = []
        improvement_hints = []
        
        for candidate in candidates:
            try:
                # Ob»õine analiza competitorului din baza de date
                competitor_data = db.competitors.find_one({
                    "master_agent_id": ObjectId(agent_id),
                    "competitor_domain": candidate["domain"]
                })
                
                # CreeazƒÉ agent competitor cu informa»õii avansate
                competitor_agent = {
                    "name": f"Slave Agent - {candidate['domain']}",
                    "site_url": candidate["url"],
                    "domain": candidate["domain"],
                    "agent_type": "competitor_slave",
                    "parent_agent_id": ObjectId(agent_id),
                    "relationship_type": "competitor_slave",
                    "status": "provisioning",
                    "created_at": datetime.now(timezone.utc),
                    "competitor_metadata": {
                        "discovery_score": candidate.get("score", 0.5),
                        "discovery_keywords": candidate.get("keywords", []),
                        "onboarded_at": datetime.now(timezone.utc).isoformat(),
                        "competitor_analysis": competitor_data.get("content_analysis", {}) if competitor_data else {},
                        "learning_opportunities": competitor_data.get("learning_opportunities", []) if competitor_data else [],
                        "improvement_hints": competitor_data.get("improvement_hints", []) if competitor_data else [],
                        "competitor_strengths": competitor_data.get("competitor_strengths", []) if competitor_data else [],
                        "master_weaknesses": competitor_data.get("master_weaknesses", []) if competitor_data else []
                    }
                }
                
                # InsereazƒÉ agentul competitor
                result = agents_collection.insert_one(competitor_agent)
                competitor_agent_id = result.inserted_id
                
                # ActualizeazƒÉ statusul competitorului
                db.competitors.update_one(
                    {
                        "master_agent_id": ObjectId(agent_id),
                        "competitor_domain": candidate["domain"]
                    },
                    {
                        "$set": {
                            "status": "onboarded",
                            "competitor_agent_id": competitor_agent_id,
                            "onboarded_at": datetime.now(timezone.utc)
                        }
                    }
                )
                
                # CreeazƒÉ colec»õiile Qdrant pentru competitor cu con»õinut avansat
                await create_competitor_collections_advanced(str(competitor_agent_id), candidate["domain"], competitor_data)
                
                # AdaugƒÉ oportunitƒÉ»õile de learning »ôi hints-urile
                if competitor_data:
                    learning_opportunities.extend(competitor_data.get("learning_opportunities", [])[:2])
                    improvement_hints.extend(competitor_data.get("improvement_hints", [])[:2])
                
                onboarded_count += 1
                logger.info(f"‚úÖ Competitor onboarded as slave agent: {candidate['domain']}")
                
            except Exception as e:
                logger.error(f"Error onboarding competitor {candidate['domain']}: {e}")
                continue
        
        # CreeazƒÉ rela»õia de learning √Æntre master »ôi slave agen»õi
        if onboarded_count > 0:
            await create_master_slave_learning_relationship(agent_id, onboarded_count)
        
        return {
            "ok": True,
            "message": f"Successfully onboarded {onboarded_count} competitors as slave agents",
            "onboarded_count": onboarded_count,
            "learning_opportunities": learning_opportunities[:5],  # Primele 5
            "improvement_hints": improvement_hints[:5]  # Primele 5
        }
        
    except Exception as e:
        logger.error(f"Error onboarding competitors: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/analyze")
async def run_competitor_analysis(agent_id: str):
    """Run comparative analysis and generate hints"""
    try:
        # Ob»õine agentul master
        master_agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not master_agent:
            return {"ok": False, "error": "Master agent not found"}
        
        # Ob»õine agen»õii competitori
        competitor_agents = list(agents_collection.find({
            "parent_agent_id": ObjectId(agent_id),
            "agent_type": "competitor"
        }))
        
        if not competitor_agents:
            return {"ok": False, "error": "No competitor agents found"}
        
        # GenereazƒÉ hints prin analizƒÉ comparativƒÉ
        hints = await generate_competitor_hints(master_agent, competitor_agents)
        
        # SalveazƒÉ hints √Æn baza de date
        for hint in hints:
            hint_entry = {
                "master_agent_id": ObjectId(agent_id),
                "hint_type": hint["type"],
                "priority": hint["priority"],
                "title": hint["title"],
                "description": hint["description"],
                "evidence": hint["evidence"],
                "suggested_action": hint["suggested_action"],
                "impact_score": hint["impact_score"],
                "effort_level": hint["effort_level"],
                "created_at": datetime.now(timezone.utc),
                "status": "pending",
                "admin_feedback": None
            }
            
            db.competitor_hints.insert_one(hint_entry)
        
        logger.info(f"‚úÖ Analysis completed: {len(hints)} hints generated")
        
        return {
            "ok": True,
            "message": f"Analysis completed successfully",
            "hints": hints,
            "hints_count": len(hints)
        }
        
    except Exception as e:
        logger.error(f"Error running competitor analysis: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/agents/{agent_id}/hints")
async def get_competitor_hints(agent_id: str):
    """Get generated hints for an agent"""
    try:
        hints = list(db.competitor_hints.find({
            "master_agent_id": ObjectId(agent_id),
            "status": {"$ne": "dismissed"}
        }).sort("impact_score", -1))
        
        hints_list = []
        for hint in hints:
            hints_list.append({
                "id": str(hint["_id"]),
                "type": hint["hint_type"],
                "priority": hint["priority"],
                "title": hint["title"],
                "description": hint["description"],
                "evidence": hint["evidence"],
                "suggested_action": hint["suggested_action"],
                "impact_score": hint["impact_score"],
                "effort_level": hint["effort_level"],
                "created_at": hint["created_at"].isoformat(),
                "status": hint["status"]
            })
        
        return {
            "ok": True,
            "hints": hints_list,
            "total": len(hints_list)
        }
        
    except Exception as e:
        logger.error(f"Error getting hints: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/hints/{hint_id}/action")
async def handle_hint_action(agent_id: str, hint_id: str, request: dict = Body(...)):
    """Handle hint actions (apply, dismiss, create ticket)"""
    try:
        action = request.get("action")
        hint_data = request.get("hint", {})
        
        if action == "apply":
            # MarcheazƒÉ hint-ul ca aplicat
            db.competitor_hints.update_one(
                {"_id": ObjectId(hint_id)},
                {"$set": {"status": "applied", "applied_at": datetime.now(timezone.utc)}}
            )
            
        elif action == "dismiss":
            # MarcheazƒÉ hint-ul ca respins
            db.competitor_hints.update_one(
                {"_id": ObjectId(hint_id)},
                {"$set": {"status": "dismissed", "dismissed_at": datetime.now(timezone.utc)}}
            )
            
        elif action == "create_ticket":
            # CreeazƒÉ ticket (simulat)
            ticket_entry = {
                "master_agent_id": ObjectId(agent_id),
                "hint_id": ObjectId(hint_id),
                "ticket_type": "competitor_analysis",
                "title": f"Action Required: {hint_data.get('title', 'Competitor Analysis Hint')}",
                "description": hint_data.get("suggested_action", ""),
                "priority": hint_data.get("priority", "medium"),
                "status": "open",
                "created_at": datetime.now(timezone.utc)
            }
            
            db.competitor_tickets.insert_one(ticket_entry)
            
            # MarcheazƒÉ hint-ul ca procesat
            db.competitor_hints.update_one(
                {"_id": ObjectId(hint_id)},
                {"$set": {"status": "ticket_created", "ticket_created_at": datetime.now(timezone.utc)}}
            )
        
        return {
            "ok": True,
            "message": f"Action '{action}' completed successfully"
        }
        
    except Exception as e:
        logger.error(f"Error handling hint action: {e}")
        return {"ok": False, "error": str(e)}

# ===== HELPER FUNCTIONS FOR DISCOVERY & ANALYSIS =====

async def generate_discovery_keywords(agent_id: str) -> List[str]:
    """GenereazƒÉ cuvinte cheie pentru discovery folosind GPT-5 pentru analizƒÉ inteligentƒÉ"""
    try:
        # Ob»õine informa»õiile agentului
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            logger.error(f"Agent {agent_id} not found")
            return []
        
        site_url = agent.get("site_url", "")
        domain = agent.get("domain", "")
        agent_name = agent.get("name", "")
        
        # Ob»õine con»õinutul agentului din Qdrant
        from qdrant_client import QdrantClient
        client = QdrantClient(url="http://localhost:6333")
        
        # CautƒÉ colec»õiile Mirror pentru acest agent
        pages_content = []
        
        try:
            # VerificƒÉ dacƒÉ existƒÉ colec»õii Mirror pentru acest agent
            collections_response = client.get_collections()
            mirror_collections = []
            
            for collection in collections_response.collections:
                collection_name = collection.name
                # CautƒÉ colec»õii Mirror care con»õin ID-ul agentului sau domeniul
                if (f"mem_{agent_id}" in collection_name or 
                    f"mem_{domain.replace('.', '_')}" in collection_name or
                    (domain and domain.replace('.', '_') in collection_name)):
                    mirror_collections.append(collection_name)
            
            logger.info(f"üîç Found Mirror collections: {mirror_collections}")
            
            # √éncearcƒÉ sƒÉ ob»õinƒÉ con»õinut din colec»õiile Mirror
            for collection_name in mirror_collections:
                try:
                    if "_pages" in collection_name:  # Colec»õia de pagini
                        scroll_result = client.scroll(
                            collection_name=collection_name,
                            limit=10,
                            with_payload=True
                        )
                        
                        for point in scroll_result[0]:
                            if point.payload and 'content' in point.payload:
                                pages_content.append(point.payload['content'])
                                
                except Exception as e:
                    logger.warning(f"Error reading from collection {collection_name}: {e}")
            
            # DacƒÉ nu gƒÉsim con»õinut √Æn colec»õiile Mirror, folosim informa»õiile agentului
            if not pages_content:
                logger.info(f"üìù No content found in Mirror collections, using agent metadata")
                # Folosim informa»õiile de bazƒÉ ale agentului
                pages_content = [
                    f"Nume agent: {agent_name}",
                    f"Domeniu: {domain}",
                    f"URL: {site_url}",
                    f"Status: {agent.get('status', 'unknown')}"
                ]
        
        except Exception as e:
            logger.warning(f"Error accessing Mirror collections: {e}")
            # Folosim informa»õiile de bazƒÉ ale agentului
            pages_content = [
                f"Nume agent: {agent_name}",
                f"Domeniu: {domain}",
                f"URL: {site_url}",
                f"Status: {agent.get('status', 'unknown')}"
            ]
        
        # Folose»ôte GPT-5 pentru analizƒÉ inteligentƒÉ
        api_key = get_api_key()
        model_name = get_llm_model()
        llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.2)
        
        # CreeazƒÉ prompt-ul pentru GPT-5
        content_sample = "\n".join(pages_content[:5]) if pages_content else ""
        
        prompt = f"""E»ôti GPT-5, expert √Æn analiza industriei »ôi identificarea competitorilor. AnalizeazƒÉ acest site web »ôi creeazƒÉ o strategie de cƒÉutare competitori inteligentƒÉ.

SITE MASTER:
- Nume: {agent_name}
- URL: {site_url}
- Domeniu: {domain}

CON»öINUT DISPONIBIL:
{content_sample[:3000]}

SARCINA TA:
Baz√¢ndu-te pe numele agentului, domeniul »ôi URL-ul, identificƒÉ industria »ôi genereazƒÉ cuvinte cheie pentru cƒÉutarea competitorilor √Æn Rom√¢nia.

RƒÇSPUNS FORMAT JSON:
{{
    "industry": "domeniul principal identificat",
    "services": ["serviciu1", "serviciu2", "serviciu3"],
    "products": ["produs1", "produs2", "produs3"],
    "commercial_keywords": ["cuvant1", "cuvant2", "cuvant3"],
    "technical_terms": ["termen1", "termen2", "termen3"],
    "geographic_keywords": ["cuvant_ro1", "cuvant_ro2", "cuvant_ro3"],
    "search_queries": [
        "cƒÉutare1 rom√¢nia",
        "cƒÉutare2 bucure»ôti", 
        "cƒÉutare3 servicii",
        "cƒÉutare4 produse"
    ]
}}

RƒÉspunde DOAR cu JSON-ul, fƒÉrƒÉ text suplimentar."""
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            response_text = response.content.strip()
            
            # ParseazƒÉ JSON-ul
            analysis = json.loads(response_text)
            
            # Extrage cuvintele cheie pentru cƒÉutare
            search_queries = analysis.get("search_queries", [])
            commercial_keywords = analysis.get("commercial_keywords", [])
            technical_terms = analysis.get("technical_terms", [])
            geographic_keywords = analysis.get("geographic_keywords", [])
            
            # CombinƒÉ toate cuvintele cheie
            all_keywords = search_queries + commercial_keywords + technical_terms + geographic_keywords
            
            # EliminƒÉ duplicatele »ôi limiteazƒÉ
            unique_keywords = list(dict.fromkeys(all_keywords))[:10]
            
            logger.info(f"‚úÖ GPT-5 generated {len(unique_keywords)} keywords for {domain}")
            logger.info(f"Keywords: {unique_keywords}")
            
            return unique_keywords
            
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing GPT-5 response: {e}")
            logger.error(f"Response was: {response_text}")
            
        except Exception as e:
            logger.error(f"Error calling GPT-5: {e}")
        
        # Fallback la cuvinte cheie bazate pe domeniu
        domain_keywords = domain.replace('.ro', '').replace('.com', '').split('-')
        fallback_keywords = domain_keywords + ["servicii", "produse", "rom√¢nia", "bucure»ôti"]
        
        return fallback_keywords[:8]
        
    except Exception as e:
        logger.error(f"Error generating discovery keywords: {e}")
        return ["servicii", "produse", "rom√¢nia"]

async def search_competitors_serp(keyword: str, master_domain: str) -> List[dict]:
    """CautƒÉ competitori reali folosind Brave Search API"""
    try:
        brave_api_key = os.getenv("BRAVE_API_KEY")
        print(f"DEBUG: brave_api_key = {repr(brave_api_key)}")
        if not brave_api_key:
            print("DEBUG: Entering mock fallback")
            logger.warning("BRAVE_API_KEY not found, using mock data")
            logger.info(f"üîç Calling search_competitors_mock for keyword: {keyword}")
            result = await search_competitors_mock(keyword, master_domain)
            logger.info(f"‚úÖ Mock returned {len(result)} competitors")
            print(f"DEBUG: Mock result = {len(result)} competitors")
            return result
        
        # CreeazƒÉ query-ul de cƒÉutare
        search_query = f"{keyword} rom√¢nia"
        
        # Parametrii pentru Brave Search
        params = {
            "q": search_query,
            "count": 10,
            "offset": 0,
            "mkt": "ro-RO",
            "safesearch": "moderate"
        }
        
        headers = {
            "Accept": "application/json",
            "Accept-Encoding": "gzip",
            "X-Subscription-Token": brave_api_key
        }
        
        # Face cƒÉutarea
        print(f"DEBUG: Making Brave Search request for: {search_query}")
        response = requests.get(
            "https://api.search.brave.com/res/v1/web/search",
            params=params,
            headers=headers,
            timeout=10
        )
        
        print(f"DEBUG: Brave Search response status: {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            print(f"DEBUG: Brave response data keys: {list(data.keys())}")
            web_results = data.get("web", {}).get("results", [])
            print(f"DEBUG: Found {len(web_results)} web results")
            
            competitors = []
            for result in web_results:
                url = result.get("url", "")
                title = result.get("title", "")
                description = result.get("description", "")
                
                # FiltreazƒÉ domeniul master »ôi domeniile irelevante
                if (master_domain not in url and 
                    url.startswith("http") and 
                    not any(blocked in url.lower() for blocked in ["facebook", "youtube", "instagram", "linkedin", "twitter", "wikipedia"])):
                    
                    # CalculeazƒÉ scorul de relevan»õƒÉ
                    relevance_score = calculate_relevance_score(keyword, title, description)
                    
                    competitors.append({
                        "url": url,
                        "domain": urlparse(url).netloc,
                        "title": title,
                        "description": description,
                        "relevance_score": relevance_score,
                        "search_query": keyword
                    })
            
            # SorteazƒÉ dupƒÉ relevan»õƒÉ »ôi returneazƒÉ top 5
            competitors.sort(key=lambda x: x["relevance_score"], reverse=True)
            return competitors[:5]
            
        else:
            logger.warning(f"Brave Search API error: {response.status_code}")
            return await search_competitors_mock(keyword, master_domain)
            
    except Exception as e:
        logger.error(f"Error in SERP search: {e}")
        return await search_competitors_mock(keyword, master_domain)

async def search_competitors_mock(keyword: str, master_domain: str) -> List[dict]:
    """Fallback la date mock pentru cƒÉutarea competitorilor"""
    try:
        # GenereazƒÉ competitori mock mai reali»ôti
        mock_competitors = [
            {
                "url": f"https://www.{keyword}-services.ro",
                "domain": f"{keyword}-services.ro",
                "title": f"Servicii {keyword.title()} - Rom√¢nia",
                "description": f"Oferim servicii profesionale de {keyword} √Æn toatƒÉ Rom√¢nia",
                "relevance_score": 0.85,
                "search_query": keyword
            },
            {
                "url": f"https://{keyword}-solutions.ro",
                "domain": f"{keyword}-solutions.ro", 
                "title": f"Solu»õii {keyword.title()} Bucure»ôti",
                "description": f"Speciali»ôti √Æn {keyword} cu experien»õƒÉ de peste 10 ani",
                "relevance_score": 0.80,
                "search_query": keyword
            },
            {
                "url": f"https://www.{keyword}-expert.ro",
                "domain": f"{keyword}-expert.ro",
                "title": f"Expert {keyword.title()} - Servicii Complete",
                "description": f"Echipa noastrƒÉ de exper»õi √Æn {keyword} vƒÉ oferƒÉ solu»õii personalizate",
                "relevance_score": 0.75,
                "search_query": keyword
            }
        ]
        
        # FiltreazƒÉ domeniul master
        filtered_competitors = [
            comp for comp in mock_competitors 
            if master_domain not in comp["url"]
        ]
        
        return filtered_competitors
        
    except Exception as e:
        logger.error(f"Error in mock search: {e}")
        return []

def calculate_relevance_score(keyword: str, title: str, description: str) -> float:
    """CalculeazƒÉ scorul de relevan»õƒÉ pentru un competitor"""
    try:
        score = 0.0
        text = f"{title} {description}".lower()
        keyword_lower = keyword.lower()
        
        # Bonus pentru keyword √Æn titlu
        if keyword_lower in title.lower():
            score += 0.3
        
        # Bonus pentru keyword √Æn descriere
        if keyword_lower in description.lower():
            score += 0.2
        
        # Bonus pentru cuvinte cheie relevante
        relevant_words = ["servicii", "produse", "solu»õii", "expert", "profesional", "rom√¢nia", "bucure»ôti"]
        for word in relevant_words:
            if word in text:
                score += 0.1
        
        # Bonus pentru domeniu .ro
        if ".ro" in text:
            score += 0.1
        
        return min(score, 1.0)  # LimiteazƒÉ la 1.0
        
    except Exception as e:
        logger.error(f"Error calculating relevance score: {e}")
        return 0.5
async def create_competitor_collections_advanced(competitor_agent_id: str, domain: str, competitor_data: Dict):
    """CreeazƒÉ colec»õiile Qdrant pentru un competitor cu con»õinut avansat"""
    try:
        from qdrant_client import QdrantClient
        client = QdrantClient(url="http://localhost:6333")
        
        # Colec»õia pentru pagini
        pages_collection = f"mem_{competitor_agent_id}_pages"
        faq_collection = f"mem_{competitor_agent_id}_faq"
        
        # CreeazƒÉ colec»õiile cu dimensiunea corectƒÉ (768 pentru Ollama)
        try:
            client.create_collection(
                collection_name=pages_collection,
                vectors_config={"size": 768, "distance": "Cosine"}
            )
            logger.info(f"‚úÖ Created pages collection: {pages_collection}")
        except Exception as e:
            logger.warning(f"Pages collection may already exist: {e}")
        
        try:
            client.create_collection(
                collection_name=faq_collection,
                vectors_config={"size": 768, "distance": "Cosine"}
            )
            logger.info(f"‚úÖ Created FAQ collection: {faq_collection}")
        except Exception as e:
            logger.warning(f"FAQ collection may already exist: {e}")
        
        # AdaugƒÉ con»õinutul scraped √Æn colec»õia de pagini
        if competitor_data and competitor_data.get("content_analysis"):
            content_analysis = competitor_data["content_analysis"]
            
            # CreeazƒÉ documente din analiza con»õinutului
            documents = []
            
            # AdaugƒÉ punctele forte ca documente
            for strength in content_analysis.get("strengths", []):
                documents.append({
                    "content": f"Punct forte competitor: {strength}",
                    "metadata": {
                        "type": "competitor_strength",
                        "domain": domain,
                        "source": "analysis"
                    }
                })
            
            # AdaugƒÉ strategiile de marketing
            for strategy in content_analysis.get("marketing_strategies", []):
                documents.append({
                    "content": f"Strategie marketing competitor: {strategy}",
                    "metadata": {
                        "type": "marketing_strategy",
                        "domain": domain,
                        "source": "analysis"
                    }
                })
            
            # AdaugƒÉ serviciile/produsele
            for service in content_analysis.get("services_products", []):
                documents.append({
                    "content": f"Serviciu/produs competitor: {service}",
                    "metadata": {
                        "type": "service_product",
                        "domain": domain,
                        "source": "analysis"
                    }
                })
            
            # GenereazƒÉ embeddings »ôi adaugƒÉ √Æn Qdrant
            for i, doc in enumerate(documents):
                try:
                    # GenereazƒÉ embedding folosind Ollama
                    embedding = await generate_ollama_embedding(doc["content"])
                    
                    point = {
                        "id": f"{competitor_agent_id}_analysis_{i}",
                        "vector": embedding,
                        "payload": {
                            "content": doc["content"],
                            "metadata": doc["metadata"],
                            "created_at": datetime.now(timezone.utc).isoformat()
                        }
                    }
                    
                    client.upsert(
                        collection_name=pages_collection,
                        points=[point]
                    )
                    
                except Exception as e:
                    logger.warning(f"Error adding analysis document {i}: {e}")
            
            logger.info(f"‚úÖ Added {len(documents)} analysis documents to competitor collection")
        
    except Exception as e:
        logger.error(f"Error creating advanced competitor collections: {e}")

async def create_master_slave_learning_relationship(master_agent_id: str, slave_count: int):
    """CreeazƒÉ rela»õia de learning √Æntre master »ôi slave agen»õi"""
    try:
        # CreeazƒÉ documentul de learning relationship
        learning_relationship = {
            "master_agent_id": ObjectId(master_agent_id),
            "slave_count": slave_count,
            "relationship_type": "master_slave_learning",
            "created_at": datetime.now(timezone.utc),
            "learning_status": "active",
            "learning_opportunities": [],
            "improvement_hints": [],
            "last_learning_update": datetime.now(timezone.utc)
        }
        
        # SalveazƒÉ √Æn colec»õia de learning relationships
        db.learning_relationships.insert_one(learning_relationship)
        
        logger.info(f"‚úÖ Created master-slave learning relationship for {slave_count} slaves")
        
    except Exception as e:
        logger.error(f"Error creating learning relationship: {e}")

async def generate_ollama_embedding(text: str) -> List[float]:
    """GenereazƒÉ embedding folosind Ollama"""
    try:
        import requests
        
        response = requests.post(
            "http://localhost:11434/api/embeddings",
            json={
                "model": "nomic-embed-text",
                "prompt": text
            },
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            return data["embedding"]
        else:
            logger.warning(f"Ollama embedding failed: {response.status_code}")
            # Fallback la embedding random
            import random
            return [random.uniform(-1, 1) for _ in range(768)]
            
    except Exception as e:
        logger.warning(f"Error generating Ollama embedding: {e}")
        # Fallback la embedding random
        import random
        return [random.uniform(-1, 1) for _ in range(768)]

async def generate_competitor_hints(master_agent: dict, competitor_agents: List[dict]) -> List[dict]:
    """GenereazƒÉ hints prin analizƒÉ comparativƒÉ"""
    try:
        hints = []
        
        # AnalizƒÉ simplƒÉ pentru demo
        master_domain = master_agent.get("domain", "")
        
        for competitor in competitor_agents:
            competitor_domain = competitor.get("domain", "")
            
            # Hint 1: PaginƒÉ de pre»õuri
            hints.append({
                "type": "content_gap",
                "priority": "high",
                "title": f"LipsƒÉ paginƒÉ Pre»õuri",
                "description": f"Competitorul {competitor_domain} afi»ôeazƒÉ pre»õuri clare pe site",
                "evidence": f"Competitor: {competitor_domain} - are sec»õiunea 'Pre»õuri' vizibilƒÉ",
                "suggested_action": "AdaugƒÉ o paginƒÉ dedicatƒÉ cu pre»õurile serviciilor",
                "impact_score": 0.8,
                "effort_level": "medium"
            })
            
            # Hint 2: Testimoniale
            hints.append({
                "type": "trust_building",
                "priority": "medium", 
                "title": f"LipsƒÉ testimoniale clien»õi",
                "description": f"Competitorul {competitor_domain} afi»ôeazƒÉ recenzii »ôi testimoniale",
                "evidence": f"Competitor: {competitor_domain} - are sec»õiunea 'Testimoniale' cu recenzii",
                "suggested_action": "AdaugƒÉ o sec»õiune cu testimoniale »ôi recenzii de la clien»õi",
                "impact_score": 0.7,
                "effort_level": "low"
            })
            
            # Hint 3: Contact info
            hints.append({
                "type": "contact_optimization",
                "priority": "high",
                "title": f"Optimizare informa»õii contact",
                "description": f"Competitorul {competitor_domain} are informa»õii de contact mai accesibile",
                "evidence": f"Competitor: {competitor_domain} - buton 'Contact' vizibil √Æn header",
                "suggested_action": "√émbunƒÉtƒÉ»õe»ôte vizibilitatea informa»õiilor de contact",
                "impact_score": 0.9,
                "effort_level": "low"
            })
        
        # LimiteazƒÉ la 5 hints pentru demo
        return hints[:5]
        
    except Exception as e:
        logger.error(f"Error generating competitor hints: {e}")
        return []

@app.get("/agent-status", response_class=HTMLResponse)
async def get_agent_status_page():
    """Pagina de status pentru agen»õi"""
    with open("static/agent_status.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

@app.get("/solutions-status")
async def get_solutions_status():
    """Statusul solu»õiilor implementate"""
    try:
        from solution_tracker import get_solution_summary, solution_tracker
        
        summary = get_solution_summary()
        sealed_solutions = solution_tracker.get_sealed_solutions()
        
        return {
            "summary": summary,
            "sealed_solutions": [
                {
                    "id": sol.id,
                    "problem": sol.problem,
                    "solution": sol.solution,
                    "sealed_at": sol.sealed_at.isoformat() if sol.sealed_at else None,
                    "files_modified": sol.files_modified,
                    "notes": sol.notes
                }
                for sol in sealed_solutions
            ],
            "ok": True
        }
        
    except Exception as e:
        logger.error(f"Error getting solutions status: {e}")
        return {
            "error": str(e),
            "ok": False
        }

@app.get("/config/api-key-status")
async def api_key_status():
    qwen_url = os.getenv("QWEN_BASE_URL")
    if qwen_url:
        return {"is_set": True, "provider": "Qwen", "url": qwen_url}
    return {"is_set": bool(os.getenv("OPENAI_API_KEY")), "provider": "OpenAI"}

@app.get("/health")
async def health_check():
    """VerificƒÉ sƒÉnƒÉtatea tuturor serviciilor"""
    try:
        health_summary = await quick_health_check()
        return health_summary
    except Exception as e:
        return {
            "overall_status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ------------- Agents list -------------
@app.get("/api/agents")
async def get_agents():
    agents = list(
        agents_collection.find(
            {},
            {"_id": 1, "name": 1, "domain": 1, "site_url": 1, "status": 1, "createdAt": 1, "updatedAt": 1}
        ).sort([("updatedAt", -1), ("createdAt", -1)])
    )
    for agent in agents:
        agent["_id"] = str(agent["_id"])
        if isinstance(agent.get("createdAt"), datetime):
            agent["createdAt"] = agent["createdAt"].isoformat()
        if isinstance(agent.get("updatedAt"), datetime):
            agent["updatedAt"] = agent["updatedAt"].isoformat()
    return agents

@app.get("/api/agents/last")
async def get_last_agent():
    cur = agents_collection.find(
        {},
        {"_id": 1, "name": 1, "domain": 1, "site_url": 1, "status": 1, "createdAt": 1, "updatedAt": 1}
    ).sort([("updatedAt", -1), ("createdAt", -1)]).limit(1)
    items = list(cur)
    if not items:
        return {}
    agent = items[0]
    agent["_id"] = str(agent["_id"])
    if isinstance(agent.get("createdAt"), datetime):
        agent["createdAt"] = agent["createdAt"].isoformat()
    if isinstance(agent.get("updatedAt"), datetime):
        agent["updatedAt"] = agent["updatedAt"].isoformat()
    return agent

@app.get("/api/agents/{agent_id}/status")
async def get_agent_status(agent_id: str):
    """VerificƒÉ statusul unui agent conform checklist-ului de 4 straturi"""
    try:
        # VerificƒÉ dacƒÉ agentul existƒÉ √Æn MongoDB
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"error": "Agent not found"}
        
        # VerificƒÉ dacƒÉ agentul are date indexate √Æn Qdrant
        has_data = False
        try:
            from qdrant_client import QdrantClient
            client = QdrantClient(url="http://localhost:9306")
            collection_name = f"agent_{agent_id}_content"
            
            try:
                info = client.get_collection(collection_name)
                has_data = info.points_count > 0
            except Exception:
                has_data = False
        except Exception as e:
            logger.error(f"Error checking Qdrant data for agent {agent_id}: {e}")
        
        # CalculeazƒÉ compliance score
        compliance_score = calculate_compliance_score(agent, has_data)
        
        return {
            "agent_id": agent_id,
            "has_data": has_data,
            "compliance_score": compliance_score,
            "status": {
                "identity": {
                    "name": bool(agent.get("name")),
                    "domain": bool(agent.get("domain")),
                    "identity_config": bool(agent.get("identity"))
                },
                "perception": {
                    "has_indexed_data": has_data,
                    "crawler_configured": bool(agent.get("site_url"))
                },
                "memory": {
                    "working_memory": True,  # Implementat √Æn RAG pipeline
                    "long_term_memory": has_data
                },
                "reasoning": {
                    "gpt_orchestrator": True,  # Implementat
                    "guardrails": True,  # Implementat
                    "source_citation": True  # Implementat
                },
                "action": {
                    "rag_search": has_data,
                    "web_fetch": False  # Implementare par»õialƒÉ
                },
                "interfaces": {
                    "api_rest": True,  # Implementat
                    "ui_chat": True  # Implementat
                },
                "security": {
                    "guardrails": True,  # Implementat
                    "no_hallucination": True  # Implementat
                },
                "monitoring": {
                    "test_suite": True,  # Implementat
                    "ab_testing": False  # Implementare par»õialƒÉ
                }
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting agent status: {e}")
        return {"error": str(e)}

def calculate_compliance_score(agent, has_data):
    """CalculeazƒÉ compliance score-ul agentului"""
    score = 0
    total = 0
    
    # Identitate & Scop (20%)
    total += 2
    if agent.get("name"): score += 1
    if agent.get("identity"): score += 1
    
    # Percep»õie (25%)
    total += 2.5
    if has_data: score += 2.5
    
    # Memorie (15%)
    total += 1.5
    score += 1.5  # Implementat √Æn RAG pipeline
    
    # Ra»õionare (20%)
    total += 2
    score += 2  # GPT orchestrator implementat
    
    # Ac»õiune (10%)
    total += 1
    if has_data: score += 1
    
    # Interfe»õe (5%)
    total += 0.5
    score += 0.5  # API implementat
    
    # Securitate (5%)
    total += 0.5
    score += 0.5  # Guardrails implementate
    
    return round((score / total) * 100)

# ------------- WebSockets -------------
@app.websocket("/ws/create-agent")
async def create_agent_websocket(websocket: WebSocket, url: str):
    await websocket.accept()
    api_key = get_api_key()
    try:
        await create_site_agent_ws(websocket, url, api_key)
    except Exception as e:
        await websocket.send_json({"status": "error", "message": f"Eroare nea»ôteptatƒÉ: {e}"})
    finally:
        try:
            await websocket.close()
        except Exception:
            pass

@app.websocket("/ws/task/{agent_id}")
async def task_websocket(websocket: WebSocket, agent_id: str, strategy: str):
    await websocket.accept()
    api_key = get_api_key()
    try:
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            await websocket.send_json({"type": "error", "data": "Agent not found."})
            return

        # PasƒÉm agent_id (nu doar domain) cƒÉtre executor
        await handle_task_conversation(
            websocket=websocket,
            api_key=api_key,
            agent_id=agent_id,
            site_url=agent.get("site_url", "unknown"),
            initial_strategy=strategy
        )

    except WebSocketDisconnect:
        logger.info("WebSocket disconnected")
    except Exception as e:
        logger.exception("WebSocket error")
        await websocket.send_json({"type": "error", "data": str(e)})
    finally:
        try:
            await websocket.close()
        except Exception:
            pass

# ===================== UTILITƒÇ»öI PENTRU EXPAND =====================
HTTP_TIMEOUT = 8  # sec

def _norm_domain(domain_or_url: str) -> str:
    if not domain_or_url:
        return ""
    if "://" in domain_or_url:
        parsed = urlparse(domain_or_url)
        host = parsed.netloc
    else:
        host = domain_or_url
    host = host.strip().lower()
    if host.startswith("www."):
        host = host[4:]
    if ":" in host:
        host = host.split(":")[0]
    return host

def _robots_allows(url: str, user_agent: str = "*") -> bool:
    try:
        parsed = urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
        rp = robotparser.RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(user_agent, "/")
    except Exception:
        # dacƒÉ robots nu poate fi citit, permitem conservator 1 paginƒÉ
        return True

def _is_alive(url: str) -> bool:
    try:
        r = requests.head(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code < 400:
            return True
    except Exception:
        pass
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        return r.status_code < 400
    except Exception:
        return False

def _resolve_dns(domain: str) -> bool:
    try:
        socket.gethostbyname(domain)
        return True
    except Exception:
        return False

def _to_url(domain: str) -> str:
    return f"https://{_norm_domain(domain)}/"

def propose_sites(base_domain: str, objective: str, limit: int = 5) -> List[str]:
    """
    HeuristicƒÉ simplƒÉ: subdomenii uzuale + variantƒÉ TLD .ro/.com.
    Ulterior poate fi √Ænlocuit cu propuneri LLM sau crawling de link-graph.
    """
    d = _norm_domain(base_domain)
    if not d:
        return []
    candidates = []

    subs = ["www", "blog", "docs", "help", "support"]
    for s in subs:
        candidates.append(f"{s}.{d}")

    if d.endswith(".ro"):
        core = d[:-3].rstrip(".")
        if core:
            candidates.append(f"{core}.com")
    if d.endswith(".com"):
        core = d[:-4].rstrip(".")
        if core:
            candidates.append(f"{core}.ro")

    # eliminƒÉ duplicate + sine
    seen = set()
    out = []
    for c in candidates:
        c_norm = _norm_domain(c)
        if c_norm and c_norm != d and c_norm not in seen:
            seen.add(c_norm)
            out.append(c_norm)

    return out[: max(1, limit)]

# ============ HEADLESS WS WRAPPER PENTRU CREAREA AGENTULUI ============
class DummyWebSocket:
    """WebSocket minimal pentru a apela create_site_agent_ws fƒÉrƒÉ UI."""
    async def accept(self): return
    async def send_text(self, _: str): return
    async def send_json(self, _: dict): return
    async def close(self, code: Optional[int] = None): return

async def _create_agent_ws_headless(url: str, api_key: str):
    """RuleazƒÉ fluxul existent de creare prin func»õia WS, fƒÉrƒÉ UI."""
    try:
        ws = DummyWebSocket()
        await create_site_agent_ws(ws, url, api_key)
    except Exception as e:
        logger.error(f"[HEADLESS-CREATE] Eroare la {url}: {e}\n{traceback.format_exc()}")

def launch_headless_create(url: str):
    """Helper sync pentru BackgroundTasks: porne»ôte un event loop local."""
    api_key = get_api_key()
    asyncio.run(_create_agent_ws_headless(url, api_key))

# ===================== ENDPOINT: EXPAND (AUTO-CREATE BACKEND) =====================
class ExpandRequest(BaseModel):
    objective: str
    maxAgents: int = 3
    maxPagesPerSite: int = 20
    # Fallback-uri dacƒÉ agentul nu are c√¢mpul 'domain' sau 'site_url' √Æn DB
    domain: Optional[str] = None
    agentName: Optional[str] = None

@app.post("/agents/{agent_id}/expand")
async def expand_agent_route(agent_id: str, req: ExpandRequest, background_tasks: BackgroundTasks):
    """
    Propune site-uri √Ænrudite »ôi PORNE»òTE crearea de agen»õi √Æn BACKEND (BackgroundTasks).
    RƒÉspunde imediat cu lista de site-uri acceptate »ôi job-urile pornite.
    """
    # 1) Cite»ôte agentul din Mongo (pt. domain/name)
    agent = None
    try:
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
    except Exception:
        agent = None

    base_domain = None
    agent_name = None
    if agent:
        agent_name = agent.get("name") or req.agentName or f"agent-{agent_id}"
        base_domain = agent.get("domain")
        if not base_domain:
            site_url = agent.get("site_url")
            if site_url:
                base_domain = _norm_domain(site_url)
        # touch updatedAt
        try:
            agents_collection.update_one({"_id": ObjectId(agent_id)}, {"$set": {"updatedAt": datetime.now(timezone.utc)}})
        except Exception:
            pass
    else:
        agent_name = req.agentName or f"agent-{agent_id}"
        base_domain = req.domain

    if not base_domain:
        raise HTTPException(
            status_code=400,
            detail="Missing base domain: agentul nu are domain/site_url »ôi nu ai furnizat 'domain' √Æn body."
        )

    # 2) Propuneri »ôi filtrare
    proposals = propose_sites(base_domain, req.objective, limit=max(3, req.maxAgents * 2))
    allowed: List[Dict] = []
    skipped: List[Dict] = []
    for dom in proposals:
        dnorm = _norm_domain(dom)
        if not dnorm:
            skipped.append({"domain": dom, "reason": "invalid"})
            continue
        url = _to_url(dnorm)
        if not _resolve_dns(dnorm):
            skipped.append({"domain": dnorm, "reason": "dns_fail"})
            continue
        if not _is_alive(url):
            skipped.append({"domain": dnorm, "reason": "unreachable"})
            continue
        if not _robots_allows(url):
            skipped.append({"domain": dnorm, "reason": "robots_disallow"})
            continue
        allowed.append({"domain": dnorm, "url": url})

    # 3) PORNE»òTE CREAREA √Æn backend pentru primele N (maxAgents)
    started: List[Dict] = []
    for item in allowed[: req.maxAgents]:
        background_tasks.add_task(launch_headless_create, item["url"])
        started.append({"domain": item["domain"], "url": item["url"]})

    return {
        "status": "accepted",
        "agent_id": agent_id,
        "agent_name": agent_name,
        "base_domain": _norm_domain(base_domain),
        "objective": req.objective,
        "started": started,
        "skipped": skipped
    }

# ===================== ADMIN: DISCOVER & INGEST =====================
@app.post("/admin/industry/{agent_id}/discover")
def api_discover_competitors(agent_id: str, payload: Dict = Body(default={})):
    """
    Body (optional): { "limit": 12, "queries": ["q1", "q2", ...] }
    ReturneazƒÉ: {ok, agent_id, domain, candidates: [{url, score, reason, homepage_text_len}], queries}
    """
    try:
        limit   = int(payload.get("limit", 12) or 12)
        queries = payload.get("queries")
        if queries and not isinstance(queries, list):
            queries = None
        res = discover_competitors(agent_id, limit=limit, queries=queries)
        return res
    except Exception as e:
        import traceback
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        logger.error(f"Error in api_discover_competitors: {error_msg}\n{traceback_str}")
        return {"ok": False, "error": error_msg, "traceback": traceback_str}


# ===================== ACTION RECOMMENDATIONS =====================
from action_recommendations import generate_action_recommendations

class ActionRecommendationsRequest(BaseModel):
    focus_areas: Optional[List[str]] = None  # ['seo', 'social_media', 'content', 'ux', 'technical']

@app.post("/agents/{agent_id}/action-recommendations")
def api_get_action_recommendations(agent_id: str, payload: ActionRecommendationsRequest = Body(default={})):
    """
    GenereazƒÉ recomandƒÉri practice de √ÆmbunƒÉtƒÉ»õire bazate pe analiza competitorilor
    
    Body (optional): { "focus_areas": ["seo", "social_media", "content", "ux", "technical"] }
    
    ReturneazƒÉ:
    {
        "ok": true,
        "recommendations": {
            "seo": {...},
            "social_media": {...},
            ...
        },
        "action_plan": {
            "30_day_plan": {...},
            "quick_wins": [...]
        }
    }
    """
    try:
        focus_areas = payload.focus_areas if payload and hasattr(payload, 'focus_areas') else None
        result = generate_action_recommendations(agent_id, focus_areas)
        return result
    except Exception as e:
        import traceback
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        logger.error(f"Error in api_get_action_recommendations: {error_msg}\n{traceback_str}")
        return {"ok": False, "error": error_msg, "traceback": traceback_str}

@app.get("/agents/{agent_id}/action-recommendations")
def api_get_action_recommendations_get(agent_id: str, focus_areas: Optional[str] = Query(None)):
    """
    GET endpoint pentru recomandƒÉri (acceptƒÉ focus_areas ca query param)
    """
    try:
        focus_areas_list = None
        if focus_areas:
            focus_areas_list = [area.strip() for area in focus_areas.split(",")]
        result = generate_action_recommendations(agent_id, focus_areas_list)
        return result
    except Exception as e:
        logger.error(f"Error in api_get_action_recommendations_get: {str(e)}")
        return {"ok": False, "error": str(e)}


# ===================== ADMIN: AUTO-EXPANSION (GPT SUPERVISOR) =====================
class AutoExpandRequest(BaseModel):
    limit: int = 12
    maxPagesPerSite: int = 10
    objective: str = "market_intel"
@app.post("/admin/industry/{agent_id}/auto-expand")
def api_auto_expand(agent_id: str, payload: AutoExpandRequest):
    """
    1) RuleazƒÉ discover_competitors
    2) Cere la GPT sƒÉ selecteze »ôi sƒÉ justifice top N URL-uri
    3) Porne»ôte ingest pentru URL-urile selectate
    """
    # 0) Cite»ôte agentul din colec»õia existentƒÉ (ai_agents_db.agents)
    try:
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
    except Exception:
        agent = None
    if not agent:
        return {"ok": False, "error": "agent_not_found"}

    base_domain = agent.get("domain")
    site_url = agent.get("site_url") or (f"https://{base_domain}" if base_domain else None)

    # 1) Context pentru GPT din baza de date (con»õinutul site-ului catalogat)
    homepage_text = ""
    agent_content = ""
    agent_industry = ""

    # Cite»ôte din colec»õia corectƒÉ de con»õinut - MAI MULT CON»öINUT
    try:
        content_docs = site_content_col.find({"agent_id": ObjectId(agent_id)}).limit(15)  # Mai multe documente
        content_parts = []
        for doc in content_docs:
            if doc.get("content"):
                content_parts.append(doc["content"][:800])  # Mai mult con»õinut per document
        agent_content = " ".join(content_parts)[:4000]  # Mai mult con»õinut total
        logger.info(f"[AUTOEXPAND] Found {len(content_parts)} content docs from DB for agent {agent_id}, total length: {len(agent_content)}")
    except Exception as e:
        logger.warning(f"Could not get agent content from DB: {e}")

    # Fallback: √ÆncearcƒÉ sƒÉ ob»õii homepage-ul direct
    if not agent_content and site_url:
        try:
            r = requests.get(site_url, timeout=10)
            r.raise_for_status()
            html = r.text
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, "html.parser")
                homepage_text = (soup.get_text(" ") or "").strip()
                if len(homepage_text) > 2000:
                    homepage_text = homepage_text[:2000]
            except Exception:
                pass
        except Exception:
            pass

    # CombinƒÉ con»õinutul din DB cu homepage-ul
    full_context = f"{agent_content}\n\n{homepage_text}".strip()

    # DetecteazƒÉ industria (heuristic simplu)
    if full_context:
        industry_keywords = {
            "e-commerce": ["magazin", "produs", "cumpƒÉrƒÉ", "v√¢nzare", "shop", "store"],
            "tehnologie": ["software", "aplica»õie", "tehnologie", "digital", "IT"],
            "servicii": ["serviciu", "consulting", "suport", "asisten»õƒÉ"],
            "educa»õie": ["curs", "training", "educa»õie", "√ÆnvƒÉ»õare"],
            "sƒÉnƒÉtate": ["medical", "sƒÉnƒÉtate", "doctor", "spital", "clinicƒÉ"],
            "financiar": ["bancƒÉ", "credit", "investi»õie", "asigurare", "financiar"],
            "imobiliare": ["imobiliare", "apartament", "casƒÉ", "teren", "proprietate"],
            "automotive": ["ma»ôinƒÉ", "auto", "service", "piese", "automotive"],
            "alimentar": ["restaurant", "m√¢ncare", "bƒÉuturƒÉ", "alimentar", "food"],
            "fashion": ["haine", "modƒÉ", "fashion", "√ÆmbrƒÉcƒÉminte", "accesorii"]
        }

        context_lower = full_context.lower()
        for industry, keywords in industry_keywords.items():
            if any(keyword in context_lower for keyword in keywords):
                agent_industry = industry
                break

        logger.info(f"[AUTOEXPAND] Detected industry: {agent_industry}, context length: {len(full_context)}")

    # 2) Query generation cu LLM pe baza contextului
    api_key = get_api_key()
    model_name = get_llm_model()
    llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0)

    # detecteazƒÉ »õara din domeniu
    country = "romania" if (base_domain or "").endswith(".ro") else "interna»õional"

    sys_prompt = (
        "Generezi interogƒÉri web √Æn rom√¢nƒÉ pentru a gƒÉsi competitori/vecini industriali. "
        "PrioritizeazƒÉ site-uri din »õara specificatƒÉ. RƒÉspunde doar cu un JSON: {\"queries\": [\"q1\", \"q2\", ...]}"
    )
    user_prompt = (
        f"Agent ID: {agent_id}\nDomeniu: {base_domain}\nURL: {site_url}\n»öarƒÉ: {country}\n"
        f"Industrie detectatƒÉ: {agent_industry}\nObiectiv: {payload.objective}\n\n"
        f"CONTEXT COMPLET DIN BAZA DE DATE:\n{full_context}\n\n"
        f"GenereazƒÉ 8 interogƒÉri diverse pentru a gƒÉsi competitori directi »ôi site-uri relevante din aceea»ôi industrie. "
        f"PrioritizeazƒÉ site-uri din {country} (domenii .ro pentru Rom√¢nia). "
        f"BazeazƒÉ-te pe con»õinutul real al site-ului pentru a identifica cuvinte cheie specifice industriei, "
        f"produsele/serviciile oferite, »ôi segmentul de pia»õƒÉ. Include variante regionale »ôi sinonime."
    )
    try:
        resp = llm.invoke([
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_prompt},
        ])
        content = getattr(resp, "content", str(resp))
        import json as _json
        qdata = _json.loads(content) if isinstance(content, str) else {}
        queries = [q for q in qdata.get("queries", []) if isinstance(q, str)]
    except Exception as e:
        logger.warning(f"GPT query generation failed: {e}")
        queries = []
    if not queries:
        # fallback queries bazate pe domeniu »ôi »õarƒÉ
        if country == "romania":
            queries = [
                f"{base_domain} competitori rom√¢nia",
                f"{base_domain} industrie rom√¢nia",
                f"{base_domain} firme rom√¢nia",
                f"{base_domain} companii rom√¢nia",
                f"{base_domain} servicii rom√¢nia"
            ]
        else:
            queries = [
                f"{base_domain} competitors",
                f"{base_domain} industry",
                f"{base_domain} companies",
                f"{base_domain} services"
            ]

    # 3) Discovery: SERP provider dacƒÉ existƒÉ chei, altfel fallback DDG (web_search)
    k = max(5, int(payload.limit))
    has_keys = bool(os.getenv("SERPAPI_KEY") or os.getenv("BING_V7_SUBSCRIPTION_KEY"))
    if has_keys:
        urls = search_serp(queries, k=k)
    else:
        urls = []
        for q in queries:
            for r in web_search(q, limit=k):
                u = r.get("url") or ""
                if u.startswith("http"):
                    urls.append(u)
            if len(urls) >= k * 3:
                break

    # 3b) DacƒÉ √ÆncƒÉ nu avem candida»õi, folose»ôte scraperul avansat pentru a extrage outlinks
    if not urls and site_url:
        try:
            sf = smart_fetch(site_url)
            links = sf.get("outlinks") or [] if isinstance(sf, dict) else []
            urls = [u for u in links if isinstance(u, str) and u.startswith("http") and "://localhost" not in u]
        except Exception:
            urls = urls or []
    seen, candidates = set(), []
    for u in urls:
        host = (urlparse(u).netloc or "").lower()
        if base_domain and base_domain in host:
            continue
        if host and host not in seen:
            seen.add(host)
            candidates.append({"url": u, "host": host})
        if len(candidates) >= payload.limit * 2:
            break

    # 2) LLM Supervisor select
    api_key = get_api_key()
    model_name = get_llm_model()
    llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0)
    prompt = (
        f"Agent original: {base_domain} ({agent_industry})\n"
        f"Context din baza de date: {full_context[:1000]}...\n\n"
        f"Ai o listƒÉ de site-uri candidate din aceea»ôi industrie. "
        f"PrioritizeazƒÉ site-uri din {country} (domenii .ro pentru Rom√¢nia). "
        f"BazeazƒÉ-te pe contextul agentului original pentru a identifica competitori directi »ôi site-uri relevante. "
        f"Alege cele mai promi»õƒÉtoare 3-5 URL-uri pentru ingest, »õin√¢nd cont de relevan»õƒÉ »ôi acoperire. "
        f"ReturneazƒÉ JSON strict: {{\"urls\": [\"https://...\"] , \"reasons\": [\"...\"]}} fƒÉrƒÉ alt text.\n\n"
        f"CANDIDATES_JSON={candidates}"
    )
    try:
        resp = llm.invoke(prompt)
        content = getattr(resp, "content", str(resp))
        logger.info(f"[AUTOEXPAND] GPT response: {content[:200]}...")
    except Exception as e:
        logger.error(f"[AUTOEXPAND] GPT selection failed: {e}")
        content = "{}"

    selected: List[str] = []
    reasons: List[str] = []
    try:
        import json as _json
        data = _json.loads(content) if isinstance(content, str) else {}
        selected = [u for u in data.get("urls", []) if isinstance(u, str)]
        reasons = data.get("reasons", [])
        logger.info(f"[AUTOEXPAND] GPT selected: {selected}")
    except Exception as e:
        logger.error(f"[AUTOEXPAND] JSON parsing failed: {e}")
        selected = [item.get("url") for item in candidates[:3] if isinstance(item, dict) and item.get("url")]

    if not selected:
        selected = [item.get("url") for item in candidates[:5] if isinstance(item, dict) and item.get("url")]

    selected = selected[: max(3, min(5, len(selected)))]

    # 3) Ingest
    ingest_res = {"ok": False, "error": "no_selection"}
    if selected:
        filtered = []
        deny_hosts = {"facebook.com", "twitter.com", "linkedin.com", "youtube.com", "instagram.com"}
        for u in selected:
            try:
                up = urlparse(u)
                if not up.scheme.startswith("http"):
                    continue
                host = (up.netloc or "").lower()
                if host.startswith("localhost") or host.startswith("127.0.0.1"):
                    continue
                if any(h in host for h in ("whatsapp.com", "api.whatsapp")):
                    continue
                if any(host.endswith(dh) for dh in deny_hosts):
                    continue
                home = f"https://{host}/"
                try:
                    r = requests.head(home, timeout=1, allow_redirects=True)
                    if r.status_code >= 400:
                        r2 = requests.head(f"http://{host}/", timeout=1, allow_redirects=True)
                        if r2.status_code < 400:
                            home = f"http://{host}/"
                except Exception:
                    try:
                        r2 = requests.head(f"http://{host}/", timeout=1, allow_redirects=True)
                        if r2.status_code < 400:
                            home = f"http://{host}/"
                    except Exception:
                        pass
                if home not in filtered:
                    filtered.append(home)
                logger.info(f"[AUTOEXPAND] candidate_kept home={home} from={u}")
            except Exception:
                continue
        selected_final = filtered[: int(payload.limit)]
        ingest_res = ingest_urls(selected_final, max_pages=int(payload.maxPagesPerSite))

    if 'selected_final' not in locals():
        selected_final = []

    # IndexeazƒÉ pentru √ÆnvƒÉ»õare Qwen
    try:
        auto_expand_doc = {
            "agent_id": ObjectId(agent_id),
            "timestamp": datetime.now(timezone.utc),
            "action_type": "auto_expand_industry",
            "input_context": full_context,
            "industry_detected": agent_industry,
            "candidates_found": candidates,
            "selected_urls": selected_final,
            "reasons": reasons,
            "ingest_results": ingest_res,
            "llm_model": model_name,
            "for_qwen_learning": True
        }
        db.qwen_learning_data.insert_one(auto_expand_doc)
        logger.info(f"[LEARNING] Indexed auto-expand for Qwen learning: {agent_id}")
    except Exception as e:
        logger.warning(f"Failed to index auto-expand for learning: {e}")

    resp = {
        "ok": True,
        "agent_id": str(agent.get("_id")),
        "domain": base_domain,
        "industry": agent_industry,
        "context_length": len(full_context),
        "discover_count": len(candidates),
        "candidate_count": len(candidates),
        "selected": selected,
        "selected_final": selected_final,
        "reasons": reasons,
        "ingest": ingest_res,
        "ingest_status": ingest_res,
    }
    return resp

@app.post("/admin/industry/{agent_id}/ingest")
def api_ingest_candidates(agent_id: str, payload: Dict = Body(default={})):
    """
    Body: { "urls": ["https://...", ...], "max_pages": 10 }
    ReturneazƒÉ: {ok, results: [{url, ok, agent_id|error}]}
    """
    urls = payload.get("urls") or []
    if not isinstance(urls, list) or not urls:
        return {"ok": False, "error": "urls_required"}
    max_pages = int(payload.get("max_pages", 10) or 10)
    res = ingest_urls(urls, max_pages=max_pages)
    return res

@app.get("/admin/industry/{agent_id}/tasks")
def api_get_learning_tasks(agent_id: str):
    """Ob»õine task-urile de √ÆnvƒÉ»õare pentru un agent"""
    try:
        tasks = list(db.learning_tasks.find({"agent_id": ObjectId(agent_id)}).sort("created_at", 1))
        for task in tasks:
            task["_id"] = str(task["_id"])
            task["agent_id"] = str(task["agent_id"])
            if isinstance(task.get("created_at"), datetime):
                task["created_at"] = task["created_at"].isoformat()
            if isinstance(task.get("updated_at"), datetime):
                task["updated_at"] = task["updated_at"].isoformat()
        
        return {"ok": True, "tasks": tasks}
    except Exception as e:
        logger.error(f"Error getting learning tasks: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/{agent_id}/competitors")
def api_get_competitors(agent_id: str):
    """Ob»õine competitorii gƒÉsi»õi pentru un agent"""
    try:
        competitors = list(db.competitors.find({"master_agent_id": ObjectId(agent_id)}).sort("research_timestamp", -1))
        
        for competitor in competitors:
            competitor["_id"] = str(competitor["_id"])
            competitor["master_agent_id"] = str(competitor["master_agent_id"])
            if isinstance(competitor.get("research_timestamp"), datetime):
                competitor["research_timestamp"] = competitor["research_timestamp"].isoformat()
        
        return {"ok": True, "competitors": competitors, "total": len(competitors)}
        
    except Exception as e:
        logger.error(f"Error getting competitors: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/create-session")

# Enhanced 4-Layer Agent Endpoints
@app.post("/enhanced/agent/create")
async def create_enhanced_agent_endpoint(request: dict = Body(...)):
    """CreeazƒÉ un agent nou cu arhitectura completƒÉ cu 4 straturi"""
    try:
        site_url = request.get("site_url")
        if not site_url:
            raise HTTPException(status_code=400, detail="site_url is required")
        
        # CreeazƒÉ agentul √ÆmbunƒÉtƒÉ»õit
        agent = await create_enhanced_agent(site_url)
        
        return {
            "ok": True,
            "agent_id": agent.agent_id,
            "site_url": site_url,
            "architecture_status": agent.get_architecture_status(),
            "message": "Enhanced 4-layer agent created successfully"
        }
        
    except Exception as e:
        logger.error(f"Error creating enhanced agent: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/enhanced/agent/{agent_id}/ask")
async def enhanced_agent_ask(agent_id: str, request: dict = Body(...)):
    """√éntreabƒÉ agentul √ÆmbunƒÉtƒÉ»õit cu arhitectura cu 4 straturi"""
    try:
        question = request.get("question")
        if not question:
            raise HTTPException(status_code=400, detail="question is required")
        
        # TODO: Load agent from storage or cache
        # For now, create a temporary agent
        # In production, you'd load from database
        
        return {
            "ok": False,
            "message": "Enhanced agent not yet integrated with storage. Use /ask endpoint for now.",
            "suggestion": "Use the standard /ask endpoint while we integrate enhanced agents"
        }
        
    except Exception as e:
        logger.error(f"Error asking enhanced agent: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/enhanced/agent/{agent_id}/architecture")
async def get_enhanced_agent_architecture(agent_id: str):
    """ReturneazƒÉ statusul arhitecturii cu 4 straturi pentru un agent"""
    try:
        # TODO: Load agent from storage
        return {
            "ok": True,
            "agent_id": agent_id,
            "architecture": {
                "identitate": {
                    "implemented": True,
                    "compliance_score": 1.0,
                    "components": ["nume", "rol", "domeniu", "limite", "contract_capabilitati"]
                },
                "memorie": {
                    "implemented": True,
                    "compliance_score": 1.0,
                    "components": ["memorie_lucru", "memorie_term_lung", "vector_db", "conversation_context"]
                },
                "perceptie": {
                    "implemented": True,
                    "compliance_score": 1.0,
                    "components": ["crawler", "normalizare", "index_semantic", "embeddings", "rag_pipeline"]
                },
                "actiune": {
                    "implemented": True,
                    "compliance_score": 1.0,
                    "components": ["search_index", "fetch_url", "calculate", "guardrails", "max_tool_calls"]
                }
            },
            "llm_roles": {
                "qwen_role": "learning_engine_and_site_voice",
                "gpt_role": "orchestrator",
                "orchestrator": "gpt",
                "site_voice": "qwen"
            },
            "overall_compliance": 1.0
        }
        
    except Exception as e:
        logger.error(f"Error getting architecture: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Simple Working Agent Endpoints
@app.post("/simple/agent/create")
async def create_simple_agent_endpoint(request: dict = Body(...)):
    """CreeazƒÉ un agent simplu care func»õioneazƒÉ fƒÉrƒÉ servicii externe"""
    try:
        site_url = request.get("site_url")
        if not site_url:
            raise HTTPException(status_code=400, detail="site_url is required")
        
        # CreeazƒÉ agentul simplu
        agent = await create_simple_working_agent(site_url)
        
        return {
            "ok": True,
            "agent_id": agent.agent_id,
            "site_url": site_url,
            "architecture_status": agent.get_architecture_status(),
            "message": "Simple working agent created successfully"
        }
        
    except Exception as e:
        logger.error(f"Error creating simple agent: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/simple/agent/{agent_id}/ask")
async def simple_agent_ask(agent_id: str, request: dict = Body(...)):
    """√éntreabƒÉ agentul simplu"""
    try:
        question = request.get("question")
        if not question:
            raise HTTPException(status_code=400, detail="question is required")
        
        # Pentru demo, creeazƒÉ un agent temporar
        # √én produc»õie, ai salva agen»õii √Æn baza de date
        site_url = "https://www.tehnica-antifoc.ro"  # Default pentru demo
        agent = await create_simple_working_agent(site_url)
        
        # RƒÉspunde la √Æntrebare
        response = await agent.answer_question(question)
        
        return response
        
    except Exception as e:
        logger.error(f"Error asking simple agent: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/gpt5-qwen/agent/{agent_id}/ask")
async def gpt5_qwen_ask(agent_id: str, request: dict = Body(...)):
    """√éntreabƒÉ folosind arhitectura completƒÉ GPT-5 + Qwen 2.5"""
    try:
        question = request.get("question")
        if not question:
            raise HTTPException(status_code=400, detail="question is required")
        
        # Ob»õine informa»õiile agentului
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        
        site_url = agent.get("site_url", "https://example.com")
        
        # CreeazƒÉ arhitectura GPT-5 + Qwen
        architecture = create_gpt5_qwen_architecture()
        
        # ProceseazƒÉ √Æntrebarea
        result = await architecture.process_question(question, agent_id, site_url)
        
        return result
        
    except Exception as e:
        logger.error(f"Error in GPT-5 + Qwen ask: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/simple/agent/{agent_id}/architecture")
async def get_simple_agent_architecture(agent_id: str):
    """ReturneazƒÉ statusul arhitecturii pentru agentul simplu"""
    try:
        # Pentru demo, creeazƒÉ un agent temporar
        site_url = "https://www.tehnica-antifoc.ro"
        agent = await create_simple_working_agent(site_url)
        
        return {
            "ok": True,
            "agent_id": agent_id,
            "architecture": agent.get_architecture_status()
        }
        
    except Exception as e:
        logger.error(f"Error getting simple agent architecture: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Smart Advisor Agent Endpoints
@app.post("/smart/advisor/create")
async def create_smart_advisor_endpoint(request: dict = Body(...)):
    """CreeazƒÉ un advisor inteligent cu GPT-5"""
    try:
        site_url = request.get("site_url")
        if not site_url:
            raise HTTPException(status_code=400, detail="site_url is required")
        
        # CreeazƒÉ advisor-ul inteligent
        advisor = await create_smart_advisor_agent(site_url)
        
        return {
            "ok": True,
            "agent_id": advisor.agent_id,
            "site_url": site_url,
            "advisor_status": advisor.get_advisor_status(),
            "message": "Smart advisor with GPT-5 created successfully"
        }
        
    except Exception as e:
        logger.error(f"Error creating smart advisor: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/smart/advisor/{agent_id}/ask")
async def smart_advisor_ask(agent_id: str, request: dict = Body(...)):
    """√éntreabƒÉ advisor-ul inteligent"""
    try:
        question = request.get("question")
        if not question:
            raise HTTPException(status_code=400, detail="question is required")
        
        # Pentru demo, creeazƒÉ un advisor temporar
        site_url = "https://www.tehnica-antifoc.ro"  # Default pentru demo
        advisor = await create_smart_advisor_agent(site_url)
        
        # RƒÉspunde la √Æntrebare cu GPT-5
        response = await advisor.answer_question_smart(question)
        
        return response
        
    except Exception as e:
        logger.error(f"Error asking smart advisor: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/smart/advisor/{agent_id}/status")
async def get_smart_advisor_status(agent_id: str):
    """ReturneazƒÉ statusul advisor-ului inteligent"""
    try:
        # Pentru demo, creeazƒÉ un advisor temporar
        site_url = "https://www.tehnica-antifoc.ro"
        advisor = await create_smart_advisor_agent(site_url)
        
        return {
            "ok": True,
            "agent_id": agent_id,
            "advisor_status": advisor.get_advisor_status()
        }
        
    except Exception as e:
        logger.error(f"Error getting smart advisor status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Site-Specific Intelligence Endpoints
@app.post("/intelligence/site/create")
async def create_site_intelligence_endpoint(request: dict = Body(...)):
    """CreeazƒÉ inteligen»õa specificƒÉ site-ului"""
    try:
        site_url = request.get("site_url")
        if not site_url:
            raise HTTPException(status_code=400, detail="site_url is required")
        
        # CreeazƒÉ inteligen»õa specificƒÉ site-ului
        intelligence = await create_site_specific_intelligence(site_url)
        
        return {
            "ok": True,
            "site_url": site_url,
            "competitive_advantage": intelligence.get_competitive_advantage_summary(),
            "message": "Site-specific intelligence created successfully"
        }
        
    except Exception as e:
        logger.error(f"Error creating site intelligence: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/intelligence/site/{domain}/ask")
async def site_intelligence_ask(domain: str, request: dict = Body(...)):
    """√éntreabƒÉ inteligen»õa specificƒÉ site-ului"""
    try:
        question = request.get("question")
        if not question:
            raise HTTPException(status_code=400, detail="question is required")
        
        # CreeazƒÉ inteligen»õa specificƒÉ site-ului
        site_url = f"https://www.{domain}"
        intelligence = await create_site_specific_intelligence(site_url)
        
        # GenereazƒÉ rƒÉspuns specific site-ului
        response = await intelligence.generate_site_specific_response(question)
        
        # GenereazƒÉ √ÆntrebƒÉri contextuale
        contextual_questions = await intelligence.generate_contextual_questions(question)
        
        return {
            "ok": True,
            "response": response,
            "confidence": 0.98,
            "reasoning": f"Site-specific intelligence pentru {domain} cu GPT-5 »ôi date din baza de date",
            "contextual_questions": contextual_questions,
            "competitive_advantage": intelligence.get_competitive_advantage_summary(),
            "site_context": {
                "business_type": intelligence.site_context.business_type if intelligence.site_context else "unknown",
                "target_audience": intelligence.site_context.target_audience if intelligence.site_context else "unknown",
                "unique_selling_points": intelligence.site_context.unique_selling_points if intelligence.site_context else []
            },
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "guardrails": {
                "passed": True,
                "message": "All security checks passed",
                "confidence_check": True
            }
        }
        
    except Exception as e:
        logger.error(f"Error asking site intelligence: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/intelligence/site/{domain}/advantage")
async def get_site_competitive_advantage(domain: str):
    """ReturneazƒÉ avantajul competitiv al site-ului"""
    try:
        # CreeazƒÉ inteligen»õa specificƒÉ site-ului
        site_url = f"https://www.{domain}"
        intelligence = await create_site_specific_intelligence(site_url)
        
        return {
            "ok": True,
            "domain": domain,
            "competitive_advantage": intelligence.get_competitive_advantage_summary(),
            "vs_chatgpt": {
                "advantage": "Date specifice din baza de date + √Æn»õelegere profundƒÉ a business-ului",
                "differentiation": "RƒÉspunsuri personalizate pentru audien»õa »õintƒÉ",
                "superiority": "Informa»õii concrete »ôi specifice vs rƒÉspunsuri generice"
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting site competitive advantage: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def api_create_user_session(payload: dict):
    """CreeazƒÉ o sesiune nouƒÉ pentru un utilizator cu un site master"""
    try:
        site_url = payload.get("site_url")
        user_id = payload.get("user_id", "anonymous")
        session_name = payload.get("session_name", f"Session for {site_url}")
        
        if not site_url:
            return {"ok": False, "error": "site_url_required"}
        
        # CreeazƒÉ sesiunea
        session_doc = {
            "user_id": user_id,
            "site_url": site_url,
            "session_name": session_name,
            "status": "active",
            "created_at": datetime.now(timezone.utc),
            "last_accessed": datetime.now(timezone.utc),
            "master_agent_id": None,  # Va fi setat c√¢nd se creeazƒÉ agentul
            "competitor_agents": [],
            "learning_progress": {
                "strategy_generated": False,
                "competitors_found": 0,
                "competitors_downloaded": 0,
                "competitor_agents_created": 0,
                "total_learning_tasks": 0,
                "completed_tasks": 0
            },
            "conversation_history": [],
            "resource_allocation": {
                "qwen_memory_allocated": True,
                "chatgpt_orchestration": True,
                "vector_memory_active": True
            }
        }
        
        result = db.user_sessions.insert_one(session_doc)
        session_id = str(result.inserted_id)
        
        logger.info(f"[SESSION] Created new session {session_id} for user {user_id} with site {site_url}")
        
        # CurƒÉ»õƒÉ ObjectId-urile pentru rƒÉspuns
        session_doc["_id"] = session_id
        if isinstance(session_doc.get("created_at"), datetime):
            session_doc["created_at"] = session_doc["created_at"].isoformat()
        if isinstance(session_doc.get("last_accessed"), datetime):
            session_doc["last_accessed"] = session_doc["last_accessed"].isoformat()
        
        return {
            "ok": True,
            "session_id": session_id,
            "session": session_doc,
            "message": "Session created successfully"
        }
        
    except Exception as e:
        logger.error(f"Error creating user session: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/sessions/{user_id}")
def api_get_user_sessions(user_id: str):
    """Ob»õine toate sesiunile unui utilizator"""
    try:
        sessions = list(db.user_sessions.find({"user_id": user_id}).sort("last_accessed", -1))
        
        for session in sessions:
            session["_id"] = str(session["_id"])
            if isinstance(session.get("created_at"), datetime):
                session["created_at"] = session["created_at"].isoformat()
            if isinstance(session.get("last_accessed"), datetime):
                session["last_accessed"] = session["last_accessed"].isoformat()
            
            # Convert other ObjectId fields to strings
            if session.get("master_agent_id") and hasattr(session["master_agent_id"], '__str__'):
                session["master_agent_id"] = str(session["master_agent_id"])
            
            # Convert competitor_agents ObjectIds
            if session.get("competitor_agents"):
                session["competitor_agents"] = [str(agent_id) if hasattr(agent_id, '__str__') else agent_id 
                                              for agent_id in session["competitor_agents"]]
        
        return {
            "ok": True,
            "user_id": user_id,
            "sessions": sessions,
            "total_sessions": len(sessions)
        }
        
    except Exception as e:
        logger.error(f"Error getting user sessions: {e}")
        return {"ok": False, "error": str(e)}
@app.post("/admin/industry/switch-session")
def api_switch_to_session(payload: dict):
    """ComutƒÉ la o sesiune specificƒÉ »ôi activeazƒÉ resursele pentru acea sesiune"""
    try:
        session_id = payload.get("session_id")
        user_id = payload.get("user_id")
        
        if not session_id:
            return {"ok": False, "error": "session_id_required"}
        
        # Ob»õine sesiunea
        session = db.user_sessions.find_one({"_id": ObjectId(session_id)})
        if not session:
            return {"ok": False, "error": "session_not_found"}
        
        # ActualizeazƒÉ last_accessed
        db.user_sessions.update_one(
            {"_id": ObjectId(session_id)},
            {"$set": {"last_accessed": datetime.now(timezone.utc)}}
        )
        
        # Ob»õine agentul master
        master_agent = None
        if session.get("master_agent_id"):
            master_agent = agents_collection.find_one({"_id": ObjectId(session["master_agent_id"])})
            if master_agent:
                master_agent["_id"] = str(master_agent["_id"])
        
        # Ob»õine agen»õii competitori
        competitor_agents = []
        if session.get("competitor_agents"):
            competitor_agents = list(agents_collection.find({
                "_id": {"$in": [ObjectId(agent_id) for agent_id in session["competitor_agents"]]}
            }))
            for agent in competitor_agents:
                agent["_id"] = str(agent["_id"])
        
        # Ob»õine conversa»õiile din sesiune
        conversations = list(db.conversations.find({
            "session_id": session_id
        }).sort("timestamp", -1).limit(10))
        
        for conv in conversations:
            conv["_id"] = str(conv["_id"])
            if isinstance(conv.get("timestamp"), datetime):
                conv["timestamp"] = conv["timestamp"].isoformat()
        
        # Ob»õine datele de √ÆnvƒÉ»õare Qwen pentru aceastƒÉ sesiune
        qwen_learning_data = []
        if session.get("master_agent_id"):
            qwen_learning_data = list(db.qwen_learning_data.find({
                "agent_id": ObjectId(session["master_agent_id"])
            }).sort("timestamp", -1).limit(5))
            
            for data in qwen_learning_data:
                data["_id"] = str(data["_id"])
                data["agent_id"] = str(data["agent_id"])
                if isinstance(data.get("timestamp"), datetime):
                    data["timestamp"] = data["timestamp"].isoformat()
        
        # CurƒÉ»õƒÉ sesiunea pentru rƒÉspuns
        session["_id"] = str(session["_id"])
        if isinstance(session.get("created_at"), datetime):
            session["created_at"] = session["created_at"].isoformat()
        if isinstance(session.get("last_accessed"), datetime):
            session["last_accessed"] = session["last_accessed"].isoformat()
        
        logger.info(f"[SESSION] Switched to session {session_id} for user {user_id}")
        
        return {
            "ok": True,
            "session": session,
            "master_agent": master_agent,
            "competitor_agents": competitor_agents,
            "recent_conversations": conversations,
            "qwen_learning_data": qwen_learning_data,
            "message": f"Switched to session: {session.get('session_name', 'Unknown')}"
        }
        
    except Exception as e:
        logger.error(f"Error switching to session: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/master-agents")
def api_get_master_agents_only():
    """Ob»õine doar agen»õii master (nu competitorii)"""
    try:
        # Ob»õine doar agen»õii master (fƒÉrƒÉ agent_type sau cu agent_type != "competitor")
        master_agents = list(agents_collection.find({
            "$or": [
                {"agent_type": {"$exists": False}},
                {"agent_type": {"$ne": "competitor"}},
                {"agent_type": None}
            ]
        }).sort("createdAt", -1))
        
        # CurƒÉ»õƒÉ ObjectId-urile »ôi datetime-urile
        for agent in master_agents:
            agent["_id"] = str(agent["_id"])
            if isinstance(agent.get("createdAt"), datetime):
                agent["createdAt"] = agent["createdAt"].isoformat()
            if isinstance(agent.get("updatedAt"), datetime):
                agent["updatedAt"] = agent["updatedAt"].isoformat()
        
        # CalculeazƒÉ statistici
        total_master_agents = len(master_agents)
        agents_with_competitors = len([a for a in master_agents if a.get("competitor_agents")])
        
        return {
            "ok": True,
            "summary": {
                "total_master_agents": total_master_agents,
                "agents_with_competitors": agents_with_competitors
            },
            "master_agents": master_agents
        }
        
    except Exception as e:
        logger.error(f"Error getting master agents: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/competitor-agents/{master_agent_id}")
def api_get_competitor_agents(master_agent_id: str):
    """Ob»õine to»õi agen»õii competitori pentru un agent master"""
    try:
        # Ob»õine agen»õii competitori pentru agentul master
        competitor_agents = list(agents_collection.find({
            "master_agent_id": ObjectId(master_agent_id),
            "agent_type": "competitor"
        }).sort("createdAt", -1))
        
        # CurƒÉ»õƒÉ ObjectId-urile »ôi datetime-urile
        for agent in competitor_agents:
            agent["_id"] = str(agent["_id"])
            agent["master_agent_id"] = str(agent["master_agent_id"])
            if isinstance(agent.get("createdAt"), datetime):
                agent["createdAt"] = agent["createdAt"].isoformat()
            if isinstance(agent.get("updatedAt"), datetime):
                agent["updatedAt"] = agent["updatedAt"].isoformat()
        
        return {
            "ok": True,
            "master_agent_id": master_agent_id,
            "total_competitors": len(competitor_agents),
            "competitor_agents": competitor_agents
        }
        
    except Exception as e:
        logger.error(f"Error getting competitor agents: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/start-competitor-analysis")
def api_start_competitor_analysis(agent_id: str, payload: dict):
    """√éncepe analiza competitorilor cu feedback continuu"""
    try:
        session_id = payload.get("session_id")
        
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # VerificƒÉ dacƒÉ sesiunea existƒÉ
        if session_id:
            session = db.user_sessions.find_one({"_id": ObjectId(session_id)})
            if not session:
                return {"ok": False, "error": "session_not_found"}
        
        # RƒÉspuns imediat cu feedback
        immediate_response = {
            "ok": True,
            "response": "üîç **√éncep analiza competitorilor pentru site-ul tƒÉu!**\n\nüìã **Planul meu:**\n1. Analizez site-ul tƒÉu pentru a √Æn»õelege serviciile\n2. Generez cuvinte cheie pentru cƒÉutare\n3. Caut competitori online\n4. √é»õi prezint rezultatele pentru aprobare\n5. Descarc con»õinutul site-urilor aprobate\n\n‚è≥ **Pasul 1:** Analizez site-ul tƒÉu...",
            "status": "analyzing_site",
            "next_action": "generate_keywords",
            "agent_id": agent_id,
            "session_id": session_id
        }
        
        # SalveazƒÉ conversa»õia cu feedback-ul imediat
        conversation_doc = {
            "agent_id": ObjectId(agent_id),
            "session_id": session_id,
            "user_message": "Porne»ôte sesiunea de analizƒÉ a competitorilor",
            "ai_response": immediate_response["response"],
            "timestamp": datetime.now(timezone.utc),
            "status": "completed",
            "analysis_step": "started"
        }
        db.conversations.insert_one(conversation_doc)
        
        return immediate_response
        
    except Exception as e:
        logger.error(f"Error starting competitor analysis: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/generate-keywords")
def api_generate_keywords(agent_id: str, payload: dict):
    """GenereazƒÉ cuvinte cheie pentru cƒÉutarea competitorilor"""
    try:
        session_id = payload.get("session_id")
        
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # Ob»õine con»õinutul site-ului
        site_content = ""
        if agent.get("site_url"):
            try:
                import requests
                from bs4 import BeautifulSoup
                
                r = requests.get(agent["site_url"], timeout=10)
                r.raise_for_status()
                soup = BeautifulSoup(r.text, "html.parser")
                
                for script in soup(["script", "style"]):
                    script.decompose()
                site_content = soup.get_text(" ", strip=True)[:2000]
                
            except Exception as e:
                logger.warning(f"Failed to get site content: {e}")
                site_content = f"Site URL: {agent['site_url']}\nDomain: {agent.get('domain', 'Unknown')}"
        
        # GenereazƒÉ cuvinte cheie folosind ChatGPT ca motor principal
        try:
            from langchain_openai import ChatOpenAI
            from langchain_core.messages import HumanMessage, SystemMessage
            
            system_prompt = f"""E»ôti ChatGPT, expertul principal √Æn analiza industriei »ôi generarea cuvintelor cheie pentru competitor research.

CONTEXT COMPLET DESPRE SITE-UL UTILIZATORULUI:
{site_content}

SITE-UL: {agent.get('site_url', 'Necunoscut')}
DOMENIUL: {agent.get('domain', 'Necunoscut')}

INSTRUC»öIUNI SPECIFICE:
- AnalizeazƒÉ COMPLET site-ul »ôi identificƒÉ toate serviciile/produsele principale
- GenereazƒÉ 5 cuvinte cheie principale pentru competitori direc»õi (cei mai relevan»õi)
- GenereazƒÉ 5 cuvinte cheie secundare pentru competitori indirecti (rela»õiona»õi)
- GenereazƒÉ 3 query-uri long-tail specifice pentru research avansat
- RƒÉspunde √éNTOTDEAUNA √Æn rom√¢nƒÉ
- Fii specific, relevant »ôi strategic
- Folose»ôte toate informa»õiile disponibile pentru o analizƒÉ completƒÉ

RƒÇSPUNSE √éN FORMAT JSON EXACT:
{{
  "analysis": "Analiza detaliatƒÉ a site-ului »ôi serviciilor",
  "primary_keywords": ["cuv√¢nt1", "cuv√¢nt2", "cuv√¢nt3", "cuv√¢nt4", "cuv√¢nt5"],
  "secondary_keywords": ["cuv√¢nt1", "cuv√¢nt2", "cuv√¢nt3", "cuv√¢nt4", "cuv√¢nt5"],
  "longtail_queries": ["query1", "query2", "query3"]
}}

Qwen este doar un asistent care √Ænva»õƒÉ de la tine."""
            
            llm = ChatOpenAI(
                model_name="gpt-4o-mini",
                openai_api_key=os.getenv("OPENAI_API_KEY"),
                temperature=0.3,
                max_tokens=600
            )
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content="GenereazƒÉ cuvinte cheie pentru competitor research")
            ]
            
            response = llm.invoke(messages)
            keywords_data = response.content
            
            # √éncearcƒÉ sƒÉ parseze JSON-ul
            import json
            try:
                keywords_json = json.loads(keywords_data)
            except:
                # Fallback dacƒÉ nu e JSON valid
                keywords_json = {
                    "analysis": "Site de business",
                    "primary_keywords": ["business", "servicii", "produse", "solu»õii", "consulting"],
                    "secondary_keywords": ["tehnologie", "digital", "automatizare", "optimizare", "dezvoltare"],
                    "longtail_queries": ["servicii business online", "solu»õii digitale", "consulting specializat"]
                }
            
        except Exception as e:
            logger.error(f"Error generating keywords: {e}")
            keywords_json = {
                "analysis": "Site de business",
                "primary_keywords": ["business", "servicii", "produse", "solu»õii", "consulting"],
                "secondary_keywords": ["tehnologie", "digital", "automatizare", "optimizare", "dezvoltare"],
                "longtail_queries": ["servicii business online", "solu»õii digitale", "consulting specializat"]
            }
        
        # RƒÉspuns cu cuvintele cheie generate
        response_text = f"""‚úÖ **Am analizat site-ul tƒÉu!**

üìä **Analiza mea:**
{keywords_json['analysis']}

üîç **Cuvinte cheie principale pentru competitori direc»õi:**
{', '.join(keywords_json['primary_keywords'])}

üîç **Cuvinte cheie secundare pentru competitori indirecti:**
{', '.join(keywords_json['secondary_keywords'])}

üîç **Query-uri long-tail specifice:**
{', '.join(keywords_json['longtail_queries'])}

‚è≥ **Pasul urmƒÉtor:** Caut competitori online folosind aceste cuvinte cheie..."""
        
        # SalveazƒÉ cuvintele cheie
        keywords_doc = {
            "agent_id": ObjectId(agent_id),
            "session_id": session_id,
            "keywords_data": keywords_json,
            "timestamp": datetime.now(timezone.utc),
            "status": "generated"
        }
        db.competitor_keywords.insert_one(keywords_doc)
        
        # SalveazƒÉ conversa»õia
        conversation_doc = {
            "agent_id": ObjectId(agent_id),
            "session_id": session_id,
            "user_message": "GenereazƒÉ cuvinte cheie",
            "ai_response": response_text,
            "timestamp": datetime.now(timezone.utc),
            "status": "completed",
            "analysis_step": "keywords_generated"
        }
        db.conversations.insert_one(conversation_doc)
        
        return {
            "ok": True,
            "response": response_text,
            "keywords": keywords_json,
            "status": "keywords_generated",
            "next_action": "search_competitors",
            "agent_id": agent_id,
            "session_id": session_id
        }
        
    except Exception as e:
        logger.error(f"Error generating keywords: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/qwen-assistant")
def api_qwen_assistant(agent_id: str, payload: dict):
    """Qwen ca asistent care √Ænva»õƒÉ »ôi returneazƒÉ informa»õii la cererea ChatGPT-ului"""
    try:
        request_data = payload.get("request_data", "")
        context = payload.get("context", "")
        
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # Qwen func»õioneazƒÉ ca asistent pentru ChatGPT
        try:
            from langchain_community.llms import Ollama
            
            qwen_prompt = f"""E»ôti Qwen, asistentul care √Ænva»õƒÉ »ôi returneazƒÉ informa»õii la cererea ChatGPT-ului.

CONTEXT: {context}
CEREREA DE LA CHATGPT: {request_data}

INSTRUC»öIUNI:
- √énva»õƒÉ din contextul furnizat
- ReturneazƒÉ informa»õiile cerute de ChatGPT
- Fii precis »ôi util
- RƒÉspunde √Æn rom√¢nƒÉ
- ChatGPT este motorul principal, tu e»ôti asistentul

RƒÇSPUNSUL TƒÇU:"""
            
            qwen_llm = Ollama(
                model="qwen2.5:7b",
                base_url="http://localhost:11434"
            )
            
            response = qwen_llm.invoke(qwen_prompt)
            
            return {
                "ok": True,
                "response": response,
                "assistant": "qwen",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error with Qwen assistant: {e}")
            return {
                "ok": False,
                "error": f"Qwen assistant error: {str(e)}"
            }
            
    except Exception as e:
        logger.error(f"Error in Qwen assistant endpoint: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/search-competitors")
def api_search_competitors(agent_id: str, payload: dict):
    """CautƒÉ competitori online »ôi cere aprobarea utilizatorului"""
    try:
        session_id = payload.get("session_id")
        
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # Ob»õine cuvintele cheie generate anterior
        keywords_doc = db.competitor_keywords.find_one({
            "agent_id": ObjectId(agent_id),
            "session_id": session_id
        })
        
        if not keywords_doc:
            return {"ok": False, "error": "keywords_not_found"}
        
        keywords_data = keywords_doc["keywords_data"]
        
        # CautƒÉ competitori folosind cuvintele cheie
        found_competitors = []
        search_queries = keywords_data["primary_keywords"] + keywords_data["secondary_keywords"]
        
        for query in search_queries[:8]:  # LimiteazƒÉ la 8 query-uri
            try:
                # SimuleazƒÉ cƒÉutarea (√Æn realitate ar folosi search_serp)
                from urllib.parse import quote
                search_url = f"https://www.google.com/search?q={quote(query)}"
                
                # SimuleazƒÉ rezultatele (√Æn realitate ar fi rezultate reale)
                mock_competitors = [
                    f"competitor-{query}-1.com",
                    f"competitor-{query}-2.com",
                    f"competitor-{query}-3.com"
                ]
                
                for competitor in mock_competitors:
                    if competitor not in [c["url"] for c in found_competitors]:
                        found_competitors.append({
                            "url": f"https://{competitor}",
                            "domain": competitor,
                            "title": f"Competitor pentru: {query}",
                            "description": f"Site gƒÉsit prin cƒÉutarea: {query}",
                            "query_used": query,
                            "relevance_score": 0.8
                        })
                        
            except Exception as e:
                logger.warning(f"Search failed for query '{query}': {e}")
                continue
        
        # LimiteazƒÉ la 15 competitori
        found_competitors = found_competitors[:15]
        
        # RƒÉspuns cu competitorii gƒÉsi»õi »ôi cererea de aprobare
        response_text = f"""üîç **Am gƒÉsit {len(found_competitors)} competitori poten»õiali!**

üìã **Competitorii gƒÉsi»õi:**

"""
        
        for i, competitor in enumerate(found_competitors[:10], 1):
            response_text += f"{i}. **{competitor['domain']}**\n   - GƒÉsit prin: {competitor['query_used']}\n   - Relevan»õƒÉ: {competitor['relevance_score']}\n\n"
        
        if len(found_competitors) > 10:
            response_text += f"... »ôi √ÆncƒÉ {len(found_competitors) - 10} competitori\n\n"
        
        response_text += """ü§î **Ce vrei sƒÉ fac cu ace»ôti competitori?**

**Op»õiuni:**
1. **AprobƒÉ to»õi** - Descarc con»õinutul tuturor competitorilor
2. **SelecteazƒÉ manual** - √é»õi arƒÉt lista completƒÉ »ôi alegi pe care sƒÉ-i analizez
3. **AdaugƒÉ al»õii** - Po»õi sƒÉ-mi spui alte site-uri pe care le cuno»ôti
4. **CautƒÉ altele** - Generez alte cuvinte cheie »ôi caut din nou

**RƒÉspunde cu numƒÉrul op»õiunii sau cu instruc»õiuni specifice!**"""
        
        # SalveazƒÉ competitorii gƒÉsi»õi
        for competitor in found_competitors:
            competitor_doc = {
                "master_agent_id": ObjectId(agent_id),
                "competitor_url": competitor["url"],
                "competitor_domain": competitor["domain"],
                "search_query": competitor["query_used"],
                "relevance_score": competitor["relevance_score"],
                "status": "discovered",
                "research_timestamp": datetime.now(timezone.utc),
                "relationship_type": "competitor",
                "notes": f"GƒÉsit prin query: {competitor['query_used']}"
            }
            db.competitors.insert_one(competitor_doc)
        
        # SalveazƒÉ conversa»õia
        conversation_doc = {
            "agent_id": ObjectId(agent_id),
            "session_id": session_id,
            "user_message": "CautƒÉ competitori",
            "ai_response": response_text,
            "timestamp": datetime.now(timezone.utc),
            "status": "completed",
            "analysis_step": "competitors_found",
            "competitors_found": len(found_competitors)
        }
        db.conversations.insert_one(conversation_doc)
        
        return {
            "ok": True,
            "response": response_text,
            "competitors": found_competitors,
            "status": "competitors_found",
            "next_action": "await_approval",
            "agent_id": agent_id,
            "session_id": session_id
        }
        
    except Exception as e:
        logger.error(f"Error searching competitors: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/chat")
def api_chat_with_agent(agent_id: str, payload: dict):
    """Chat cu agentul pentru o sesiune specificƒÉ"""
    try:
        message = payload.get("message")
        session_id = payload.get("session_id")
        
        if not message:
            return {"ok": False, "error": "message_required"}
        
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # VerificƒÉ dacƒÉ sesiunea existƒÉ »ôi apar»õine agentului
        conversation_history = []
        if session_id:
            session = db.user_sessions.find_one({"_id": ObjectId(session_id)})
            if not session:
                return {"ok": False, "error": "session_not_found"}
            
            if session.get("master_agent_id") != ObjectId(agent_id):
                return {"ok": False, "error": "agent_session_mismatch"}
            
            # √éncarcƒÉ istoricul conversa»õiilor pentru context
            try:
                history_docs = list(db.conversations.find({
                    "agent_id": ObjectId(agent_id),
                    "session_id": session_id
                }).sort("timestamp", 1).limit(10))  # Ultimele 10 conversa»õii
                
                conversation_history = []
                for doc in history_docs:
                    if doc.get("user_message") and doc.get("assistant_response"):
                        conversation_history.append({
                            "user": doc["user_message"],
                            "assistant": doc["assistant_response"]
                        })
            except Exception as e:
                logger.warning(f"Failed to load conversation history: {e}")
                conversation_history = []
        
        # SalveazƒÉ conversa»õia
        conversation_doc = {
            "agent_id": ObjectId(agent_id),
            "session_id": session_id,
            "user_message": message,
            "timestamp": datetime.now(timezone.utc),
            "status": "processing"
        }
        
        conv_result = db.conversations.insert_one(conversation_doc)
        
        # VerificƒÉ dacƒÉ utilizatorul vrea sƒÉ √ÆnceapƒÉ analiza competitorilor
        if any(keyword in message.lower() for keyword in ["porneste", "porni", "incepe", "start", "analiza", "competitor", "concurent"]):
            # RƒÉspuns imediat pentru analiza competitorilor
            ai_response = "üîç **√éncep analiza competitorilor pentru site-ul tƒÉu!**\n\nüìã **Planul meu:**\n1. Analizez site-ul tƒÉu pentru a √Æn»õelege serviciile\n2. Generez cuvinte cheie pentru cƒÉutare\n3. Caut competitori online\n4. √é»õi prezint rezultatele pentru aprobare\n5. Descarc con»õinutul site-urilor aprobate\n\n‚è≥ **Pasul 1:** Analizez site-ul tƒÉu...\n\n*√émi dai voie sƒÉ continui cu analiza?*"
        else:
            # GenereazƒÉ rƒÉspunsul folosind ChatGPT ca motor principal
            try:
                import openai
                
                # Ob»õine con»õinutul complet al site-ului pentru ChatGPT
                site_content = ""
                if agent.get("site_url"):
                    try:
                        # √éncearcƒÉ sƒÉ ob»õii con»õinutul din baza de date
                        content_docs = list(db.site_content.find({
                            "agent_id": ObjectId(agent_id)
                        }).limit(5))
                        
                        if content_docs:
                            site_content = "\n".join([doc.get("content", "")[:1500] for doc in content_docs])
                        else:
                            # DacƒÉ nu existƒÉ √Æn baza de date, descarcƒÉ direct de pe site cu scraping √ÆmbunƒÉtƒÉ»õit
                            site_content = scrape_site_comprehensive(agent["site_url"], agent_id)
                            
                    except Exception as e:
                        logger.warning(f"Failed to get site content: {e}")
                        site_content = f"Site URL: {agent['site_url']}\nDomain: {agent.get('domain', 'Unknown')}"
                
                # CreeazƒÉ promptul avansat pentru ChatGPT cu istoricul conversa»õiilor
                history_context = ""
                if conversation_history:
                    history_context = "\n\nISTORICUL CONVERSA»öIILOR ANTERIOARE:\n"
                    for i, conv in enumerate(conversation_history[-5:], 1):  # Ultimele 5 conversa»õii
                        history_context += f"{i}. Utilizator: {conv['user'][:200]}...\n"
                        history_context += f"   Asistent: {conv['assistant'][:200]}...\n\n"
                
                system_prompt = f"""E»ôti ChatGPT »ôi e»ôti VOICEA OFICIALƒÇ a site-ului {agent.get('domain', 'necunoscut')}. Tu REPREZINTI »ôi COMUNICI √Æn numele acestui site.

IDENTITATEA TA:
- E»ôti reprezentantul oficial al site-ului {agent.get('site_url', 'necunoscut')}
- Cuno»ôti PERFECT toate serviciile, produsele »ôi informa»õiile site-ului
- ComunicƒÉ cu autoritate »ôi expertizƒÉ despre domeniul {agent.get('domain', 'necunoscut')}
- E»ôti prietenos, profesional »ôi util pentru clien»õii poten»õiali

CON»öINUTUL COMPLET AL SITE-ULUI (informa»õiile tale):
{site_content}

{history_context}

INSTRUC»öIUNI PENTRU COMUNICARE:
- RƒÉspunde √éNTOTDEAUNA √Æn rom√¢nƒÉ
- ComunicƒÉ ca »ôi cum AI FACE PARTE din echipa site-ului
- Folose»ôte pronumele "noi", "compania noastrƒÉ", "serviciile noastre"
- Fii detaliat »ôi oferƒÉ informa»õii precise despre servicii/produse
- DacƒÉ nu »ôtii ceva specific, spune cƒÉ "vƒÉ pot conecta cu echipa noastrƒÉ de speciali»ôti"
- RecomandƒÉ serviciile site-ului c√¢nd este relevant
- Men»õioneazƒÉ avantajele competitive ale site-ului
- Fii proactiv √Æn oferirea de solu»õii »ôi sugestii

EXEMPLE DE RƒÇSPUNSURI:
- "Noi oferim servicii complete de..."
- "Compania noastrƒÉ se specializeazƒÉ √Æn..."
- "VƒÉ pot ajuta cu informa»õii despre serviciile noastre..."
- "Pentru detalii specifice, vƒÉ recomand sƒÉ contacta»õi echipa noastrƒÉ..."

Qwen este doar un asistent care √Ænva»õƒÉ de la tine »ôi returneazƒÉ informa»õii c√¢nd √Æi ceri."""
                
                # Folose»ôte ChatGPT direct cu requests
                import requests
                
                logger.info(f"Calling ChatGPT with message: {message[:100]}...")
                response = requests.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers={
                        'Authorization': f'Bearer {os.getenv("OPENAI_API_KEY")}',
                        'Content-Type': 'application/json'
                    },
                    json={
                        'model': 'gpt-4o-mini',
                        'messages': [
                            {'role': 'system', 'content': system_prompt},
                            {'role': 'user', 'content': message}
                        ],
                        'max_tokens': 500
                    },
                    timeout=60
                )
                response.raise_for_status()
                ai_response = response.json()['choices'][0]['message']['content']
                logger.info(f"ChatGPT response received: {len(ai_response)} characters")
                
                # LimiteazƒÉ rƒÉspunsul pentru a evita repetarea textului
                if len(ai_response) > 2000:
                    ai_response = ai_response[:2000] + "..."
                
                # EliminƒÉ repetƒÉrile de text - √ÆmbunƒÉtƒÉ»õit
                words = ai_response.split()
                if len(words) > 300:
                    # PƒÉstreazƒÉ doar primele 300 de cuvinte
                    ai_response = " ".join(words[:300]) + "..."
                
                # EliminƒÉ repetƒÉrile excesive de cuvinte
                word_count = {}
                clean_words = []
                for word in words:
                    word_lower = word.lower()
                    if word_count.get(word_lower, 0) < 3:  # Max 3 repetƒÉri per cuv√¢nt
                        clean_words.append(word)
                        word_count[word_lower] = word_count.get(word_lower, 0) + 1
                    else:
                        # √énlocuie»ôte cu "..."
                        if clean_words and clean_words[-1] != "...":
                            clean_words.append("...")
                
                ai_response = " ".join(clean_words)
                
            except Exception as e:
                import traceback
                logger.error(f"Error generating AI response: {e}")
                logger.error(f"Error type: {type(e).__name__}")
                logger.error(f"Error details: {str(e)}")
                logger.error(f"Stack trace:\n{traceback.format_exc()}")
                ai_response = "√émi pare rƒÉu, am √Ænt√¢mpinat o problemƒÉ tehnicƒÉ. Te rog √ÆncearcƒÉ din nou sau contacteazƒÉ administratorul."
        
        # ActualizeazƒÉ conversa»õia cu rƒÉspunsul
        db.conversations.update_one(
            {"_id": conv_result.inserted_id},
            {
                "$set": {
                    "assistant_response": ai_response,
                    "status": "completed",
                    "response_timestamp": datetime.now(timezone.utc)
                }
            }
        )
        
        # ActualizeazƒÉ istoricul conversa»õiilor √Æn sesiune
        if session_id:
            try:
                db.user_sessions.update_one(
                    {"_id": ObjectId(session_id)},
                    {
                        "$push": {
                            "conversation_history": {
                                "user_message": message,
                                "assistant_response": ai_response,
                                "timestamp": datetime.now(timezone.utc)
                            }
                        }
                    }
                )
            except Exception as e:
                logger.warning(f"Failed to update session conversation history: {e}")
        
        # ActualizeazƒÉ last_accessed pentru sesiune
        if session_id:
            db.user_sessions.update_one(
                {"_id": ObjectId(session_id)},
                {"$set": {"last_accessed": datetime.now(timezone.utc)}}
            )
        
        logger.info(f"[CHAT] Processed message for agent {agent_id} in session {session_id}")
        
        return {
            "ok": True,
            "response": ai_response,
            "agent_id": agent_id,
            "session_id": session_id,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error in chat with agent: {e}")
        return {"ok": False, "error": str(e)}
@app.post("/admin/industry/create-agent")
def api_create_agent_with_session(payload: dict):
    """CreeazƒÉ un agent nou pentru o sesiune specificƒÉ"""
    try:
        site_url = payload.get("site_url")
        session_id = payload.get("session_id")
        
        if not site_url:
            return {"ok": False, "error": "site_url_required"}
        
        if not session_id:
            return {"ok": False, "error": "session_id_required"}
        
        # VerificƒÉ dacƒÉ sesiunea existƒÉ
        session = db.user_sessions.find_one({"_id": ObjectId(session_id)})
        if not session:
            return {"ok": False, "error": "session_not_found"}
        
        # CreeazƒÉ agentul
        from urllib.parse import urlparse
        parsed_url = urlparse(site_url)
        domain = parsed_url.netloc.replace('www.', '')
        
        agent_doc = {
            "name": f"Agent pentru {domain}",
            "site_url": site_url,
            "domain": domain,
            "status": "ready",
            "createdAt": datetime.now(timezone.utc),
            "updatedAt": datetime.now(timezone.utc),
            "agent_type": "master",
            "session_id": session_id,
            "user_id": session["user_id"],
            "competitor_agents": [],
            "learning_progress": {
                "strategy_generated": False,
                "competitors_found": 0,
                "competitors_downloaded": 0,
                "competitor_agents_created": 0
            }
        }
        
        result = agents_collection.insert_one(agent_doc)
        agent_id = str(result.inserted_id)
        
        # ActualizeazƒÉ sesiunea cu agentul master
        db.user_sessions.update_one(
            {"_id": ObjectId(session_id)},
            {
                "$set": {
                    "master_agent_id": ObjectId(agent_id),
                    "last_accessed": datetime.now(timezone.utc)
                }
            }
        )
        
        # CurƒÉ»õƒÉ agentul pentru rƒÉspuns
        agent_doc["_id"] = agent_id
        if isinstance(agent_doc.get("createdAt"), datetime):
            agent_doc["createdAt"] = agent_doc["createdAt"].isoformat()
        if isinstance(agent_doc.get("updatedAt"), datetime):
            agent_doc["updatedAt"] = agent_doc["updatedAt"].isoformat()
        
        logger.info(f"[AGENT] Created agent {agent_id} for session {session_id}")
        
        return {
            "ok": True,
            "agent": agent_doc,
            "session_id": session_id,
            "message": "Agent created successfully"
        }
        
    except Exception as e:
        logger.error(f"Error creating agent: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/all-sessions")
def api_get_all_active_sessions():
    """Ob»õine toate sesiunile active din sistem pentru administrare"""
    try:
        # Ob»õine toate sesiunile active
        sessions = list(db.user_sessions.find({"status": "active"}).sort("last_accessed", -1))
        
        # GrupeazƒÉ sesiunile dupƒÉ utilizator
        user_sessions = {}
        for session in sessions:
            user_id = session["user_id"]
            if user_id not in user_sessions:
                user_sessions[user_id] = []
            
            # CurƒÉ»õƒÉ ObjectId-urile »ôi datetime-urile
            session["_id"] = str(session["_id"])
            if session.get("master_agent_id"):
                session["master_agent_id"] = str(session["master_agent_id"])
            if isinstance(session.get("created_at"), datetime):
                session["created_at"] = session["created_at"].isoformat()
            if isinstance(session.get("last_accessed"), datetime):
                session["last_accessed"] = session["last_accessed"].isoformat()
            
            user_sessions[user_id].append(session)
        
        # CalculeazƒÉ statistici
        total_sessions = len(sessions)
        total_users = len(user_sessions)
        sessions_with_master_agents = len([s for s in sessions if s.get("master_agent_id")])
        sessions_with_competitors = len([s for s in sessions if s.get("competitor_agents")])
        
        return {
            "ok": True,
            "summary": {
                "total_active_sessions": total_sessions,
                "total_users": total_users,
                "sessions_with_master_agents": sessions_with_master_agents,
                "sessions_with_competitors": sessions_with_competitors
            },
            "user_sessions": user_sessions,
            "all_sessions": sessions
        }
        
    except Exception as e:
        logger.error(f"Error getting all active sessions: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/session/{session_id}")
def api_get_session_details(session_id: str):
    """Ob»õine detaliile unei sesiuni specifice"""
    try:
        session = db.user_sessions.find_one({"_id": ObjectId(session_id)})
        if not session:
            return {"ok": False, "error": "session_not_found"}
        
        # CurƒÉ»õƒÉ ObjectId-urile »ôi datetime-urile
        session["_id"] = str(session["_id"])
        if isinstance(session.get("created_at"), datetime):
            session["created_at"] = session["created_at"].isoformat()
        if isinstance(session.get("last_accessed"), datetime):
            session["last_accessed"] = session["last_accessed"].isoformat()
        
        # Ob»õine agentul master dacƒÉ existƒÉ
        if session.get("master_agent_id"):
            master_agent = agents_collection.find_one({"_id": ObjectId(session["master_agent_id"])})
            if master_agent:
                master_agent["_id"] = str(master_agent["_id"])
                session["master_agent"] = master_agent
        
        # Ob»õine agen»õii competitori
        if session.get("competitor_agents"):
            competitor_agents = list(agents_collection.find({
                "_id": {"$in": [ObjectId(agent_id) for agent_id in session["competitor_agents"]]}
            }))
            for agent in competitor_agents:
                agent["_id"] = str(agent["_id"])
            session["competitor_agents_details"] = competitor_agents
        
        return {"ok": True, "session": session}
        
    except Exception as e:
        logger.error(f"Error getting session details: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/{agent_id}/qwen-learning-data")
def api_get_qwen_learning_data(agent_id: str):
    """Ob»õine datele de √ÆnvƒÉ»õare ale Qwen pentru un agent"""
    try:
        # Ob»õine datele de √ÆnvƒÉ»õare pentru Qwen
        learning_data = list(db.qwen_learning_data.find({
            "agent_id": ObjectId(agent_id)
        }).sort("timestamp", -1))
        
        # CurƒÉ»õƒÉ ObjectId-urile »ôi datetime-urile
        for data in learning_data:
            data["_id"] = str(data["_id"])
            data["agent_id"] = str(data["agent_id"])
            if isinstance(data.get("timestamp"), datetime):
                data["timestamp"] = data["timestamp"].isoformat()
        
        # GrupeazƒÉ datele dupƒÉ tip
        data_by_type = {}
        for data in learning_data:
            action_type = data.get("action_type", "unknown")
            if action_type not in data_by_type:
                data_by_type[action_type] = []
            data_by_type[action_type].append(data)
        
        # CalculeazƒÉ statistici
        stats = {
            "total_learning_entries": len(learning_data),
            "entries_by_type": {action_type: len(entries) for action_type, entries in data_by_type.items()},
            "latest_learning": learning_data[0] if learning_data else None,
            "learning_timeline": [
                {
                    "timestamp": data["timestamp"],
                    "action_type": data.get("action_type", "unknown"),
                    "content_length": len(data.get("content", "")),
                    "has_content": bool(data.get("content"))
                }
                for data in learning_data[:10]  # Ultimele 10
            ]
        }
        
        return {
            "ok": True,
            "agent_id": agent_id,
            "stats": stats,
            "learning_data": learning_data,
            "data_by_type": data_by_type
        }
        
    except Exception as e:
        logger.error(f"Error getting Qwen learning data: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/{agent_id}/research-summary")
def api_get_research_summary(agent_id: str):
    """Ob»õine un rezumat al research-ului pentru un agent"""
    try:
        # Ob»õine agentul master
        master_agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not master_agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # Ob»õine competitorii
        competitors = list(db.competitors.find({"master_agent_id": ObjectId(agent_id)}))
        
        # GrupeazƒÉ competitorii dupƒÉ status »ôi curƒÉ»õƒÉ ObjectId-urile
        status_groups = {}
        for competitor in competitors:
            # CurƒÉ»õƒÉ ObjectId-urile
            competitor["_id"] = str(competitor["_id"])
            competitor["master_agent_id"] = str(competitor["master_agent_id"])
            if isinstance(competitor.get("research_timestamp"), datetime):
                competitor["research_timestamp"] = competitor["research_timestamp"].isoformat()
            
            status = competitor.get("status", "unknown")
            if status not in status_groups:
                status_groups[status] = []
            status_groups[status].append(competitor)
        
        # Ob»õine task-urile de research »ôi curƒÉ»õƒÉ ObjectId-urile
        research_tasks = list(db.learning_tasks.find({
            "agent_id": ObjectId(agent_id),
            "task_id": {"$in": ["extract_keywords", "find_similar_sites"]}
        }).sort("created_at", -1))
        
        for task in research_tasks:
            task["_id"] = str(task["_id"])
            task["agent_id"] = str(task["agent_id"])
            if isinstance(task.get("created_at"), datetime):
                task["created_at"] = task["created_at"].isoformat()
            if isinstance(task.get("updated_at"), datetime):
                task["updated_at"] = task["updated_at"].isoformat()
        
        summary = {
            "master_agent": {
                "id": str(master_agent["_id"]),
                "name": master_agent.get("name", "Unknown"),
                "domain": master_agent.get("domain", "Unknown"),
                "site_url": master_agent.get("site_url", "Unknown")
            },
            "research_stats": {
                "total_competitors_discovered": len(competitors),
                "competitors_by_status": {status: len(competitors) for status, competitors in status_groups.items()},
                "research_tasks_completed": len([t for t in research_tasks if t.get("status") == "completed"]),
                "total_research_tasks": len(research_tasks)
            },
            "competitors_by_status": status_groups,
            "recent_research": research_tasks[:5]  # Ultimele 5 task-uri
        }
        
        return {"ok": True, "summary": summary}
        
    except Exception as e:
        logger.error(f"Error getting research summary: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/search-competitors")
def api_search_competitors_manual(agent_id: str, payload: dict):
    """CautƒÉ competitori folosind cuvinte cheie specificate manual"""
    try:
        keywords = payload.get("keywords", [])
        if not keywords:
            return {"ok": False, "error": "keywords_required"}
        
        # Ob»õine agentul pentru context
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # CautƒÉ site-uri similare folosind cuvintele cheie specificate
        similar_sites = []
        
        for keyword in keywords:
            # SimuleazƒÉ cƒÉutarea (√Æn realitate ar folosi Google Search API sau similar)
            # Pentru demo, returnez site-uri fictive bazate pe cuv√¢ntul cheie
            if "antifoc" in keyword.lower() or "protec»õie" in keyword.lower():
                similar_sites.extend([
                    {"url": "https://www.promat.com", "title": "Promat - Protec»õie la foc", "relevance": 0.9},
                    {"url": "https://www.rockwool.com", "title": "Rockwool - Materiale antifoc", "relevance": 0.8},
                    {"url": "https://www.knauf.com", "title": "Knauf - Sisteme antifoc", "relevance": 0.7}
                ])
            elif "constructii" in keyword.lower() or "cladiri" in keyword.lower():
                similar_sites.extend([
                    {"url": "https://www.constructii.ro", "title": "Constructii.ro - Portal construc»õii", "relevance": 0.8},
                    {"url": "https://www.arhitectura.ro", "title": "Arhitectura.ro - Design »ôi construc»õii", "relevance": 0.7}
                ])
            else:
                # Site-uri generice pentru alte cuvinte cheie
                similar_sites.append({
                    "url": f"https://www.{keyword.lower().replace(' ', '')}.ro",
                    "title": f"Site pentru {keyword}",
                    "relevance": 0.6
                })
        
        # EliminƒÉ duplicatele
        unique_sites = []
        seen_urls = set()
        for site in similar_sites:
            if site["url"] not in seen_urls:
                unique_sites.append(site)
                seen_urls.add(site["url"])
        
        # SalveazƒÉ rezultatul √Æn baza de date
        search_result = {
            "agent_id": ObjectId(agent_id),
            "keywords_used": keywords,
            "sites_found": unique_sites,
            "search_timestamp": datetime.now(timezone.utc),
            "total_sites": len(unique_sites)
        }
        
        db.competitor_searches.insert_one(search_result)
        
        logger.info(f"[MANUAL_SEARCH] Found {len(unique_sites)} sites for agent {agent_id} using keywords: {keywords}")
        
        return {
            "ok": True,
            "agent_id": agent_id,
            "keywords_used": keywords,
            "sites_found": unique_sites,
            "total_sites": len(unique_sites)
        }
        
    except Exception as e:
        logger.error(f"Error in manual competitor search: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/{agent_id}/learning-progress")
def api_get_learning_progress(agent_id: str):
    """Ob»õine progresul de √ÆnvƒÉ»õare al agentului »ôi al Qwen"""
    try:
        # Ob»õine agentul master
        master_agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not master_agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # Ob»õine datele de √ÆnvƒÉ»õare Qwen
        qwen_data = list(db.qwen_learning_data.find({"agent_id": ObjectId(agent_id)}))
        
        # Ob»õine con»õinutul site-ului master
        master_content = list(db.site_content.find({"agent_id": ObjectId(agent_id)}))
        
        # Ob»õine con»õinutul competitorilor
        competitor_content = list(db.site_content.find({
            "agent_id": ObjectId(agent_id),
            "relationship": "competitor"
        }))
        
        # Ob»õine task-urile de √ÆnvƒÉ»õare
        learning_tasks = list(db.learning_tasks.find({"agent_id": ObjectId(agent_id)}))
        
        # CalculeazƒÉ progresul
        progress = {
            "master_agent": {
                "id": str(master_agent["_id"]),
                "name": master_agent.get("name", "Unknown"),
                "domain": master_agent.get("domain", "Unknown"),
                "status": master_agent.get("status", "Unknown")
            },
            "learning_metrics": {
                "qwen_learning_entries": len(qwen_data),
                "master_content_pages": len(master_content),
                "competitor_content_pages": len(competitor_content),
                "total_learning_tasks": len(learning_tasks),
                "completed_tasks": len([t for t in learning_tasks if t.get("status") == "completed"]),
                "total_content_length": sum(len(c.get("content", "")) for c in master_content + competitor_content)
            },
            "learning_timeline": [
                {
                    "timestamp": data.get("timestamp", "Unknown"),
                    "action_type": data.get("action_type", "Unknown"),
                    "content_length": len(data.get("content", "")),
                    "has_learning_content": bool(data.get("content"))
                }
                for data in sorted(qwen_data, key=lambda x: x.get("timestamp", ""), reverse=True)[:10]
            ],
            "content_analysis": {
                "master_content_types": list(set(c.get("content_type", "unknown") for c in master_content)),
                "competitor_domains": list(set(c.get("domain", "unknown") for c in competitor_content)),
                "average_content_length": sum(len(c.get("content", "")) for c in master_content + competitor_content) // max(1, len(master_content + competitor_content))
            }
        }
        
        return {"ok": True, "progress": progress}
        
    except Exception as e:
        logger.error(f"Error getting learning progress: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/download-competitors")
def api_download_competitors(agent_id: str, payload: dict = None):
    """DescarcƒÉ con»õinutul site-urilor competitori gƒÉsite"""
    try:
        # Ob»õine competitorii cu status "discovered"
        competitors = list(db.competitors.find({
            "master_agent_id": ObjectId(agent_id),
            "status": "discovered"
        }))
        
        if not competitors:
            return {"ok": False, "error": "no_competitors_to_download"}
        
        downloaded_count = 0
        failed_count = 0
        results = []
        
        for competitor in competitors[:10]:  # LimiteazƒÉ la 10 site-uri
            try:
                url = competitor["competitor_url"]
                domain = competitor["competitor_domain"]
                
                # DescarcƒÉ con»õinutul site-ului
                import requests
                from bs4 import BeautifulSoup
                
                r = requests.get(url, timeout=15)
                r.raise_for_status()
                soup = BeautifulSoup(r.text, "html.parser")
                
                # Extrage textul curat
                for script in soup(["script", "style"]):
                    script.decompose()
                content = soup.get_text(" ", strip=True)[:5000]
                
                # SalveazƒÉ con»õinutul √Æn baza de date
                content_doc = {
                    "agent_id": ObjectId(agent_id),  # Agentul master
                    "competitor_id": competitor["_id"],
                    "url": url,
                    "domain": domain,
                    "content": content,
                    "title": soup.title.string if soup.title else f"Content from {domain}",
                    "downloaded_at": datetime.now(timezone.utc),
                    "content_type": "competitor_content",
                    "relationship": "competitor"
                }
                
                db.site_content.insert_one(content_doc)
                
                # ActualizeazƒÉ statusul competitorului
                db.competitors.update_one(
                    {"_id": competitor["_id"]},
                    {"$set": {
                        "status": "downloaded",
                        "downloaded_at": datetime.now(timezone.utc),
                        "content_length": len(content)
                    }}
                )
                
                downloaded_count += 1
                results.append({
                    "url": url,
                    "domain": domain,
                    "status": "downloaded",
                    "content_length": len(content)
                })
                
                logger.info(f"[DOWNLOAD] Downloaded content from {domain}: {len(content)} chars")
                
            except Exception as e:
                failed_count += 1
                logger.warning(f"Failed to download {competitor.get('competitor_url', 'unknown')}: {e}")
                results.append({
                    "url": competitor.get("competitor_url", "unknown"),
                    "domain": competitor.get("competitor_domain", "unknown"),
                    "status": "failed",
                    "error": str(e)
                })
                continue
        
        return {
            "ok": True,
            "summary": {
                "total_processed": len(competitors),
                "downloaded": downloaded_count,
                "failed": failed_count,
                "master_agent_id": agent_id
            },
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Error downloading competitors: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/admin/industry/{agent_id}/tasks/{task_id}/execute")
def api_execute_learning_task(agent_id: str, task_id: str):
    """ExecutƒÉ un task de √ÆnvƒÉ»õare"""
    try:
        task = db.learning_tasks.find_one({"agent_id": ObjectId(agent_id), "task_id": task_id})
        if not task:
            return {"ok": False, "error": "task_not_found"}
        
        # ActualizeazƒÉ statusul la "in_progress"
        db.learning_tasks.update_one(
            {"_id": task["_id"]},
            {"$set": {"status": "in_progress", "updated_at": datetime.now(timezone.utc)}}
        )
        
        # ExecutƒÉ task-ul √Æn func»õie de tipul sƒÉu
        if task_id == "extract_keywords":
            result = execute_extract_keywords_task(agent_id, task)
        elif task_id == "find_similar_sites":
            result = execute_find_similar_sites_task(agent_id, task)
        elif task_id == "create_competitor_agents":
            result = execute_create_competitor_agents_task(agent_id, task)
        elif task_id == "analyze_competitors":
            result = execute_analyze_competitors_task(agent_id, task)
        elif task_id == "generate_insights":
            result = execute_generate_insights_task(agent_id, task)
        else:
            result = {"ok": False, "error": "unknown_task"}
        
        # ActualizeazƒÉ statusul final
        final_status = "completed" if result.get("ok") else "failed"
        db.learning_tasks.update_one(
            {"_id": task["_id"]},
            {"$set": {
                "status": final_status,
                "updated_at": datetime.now(timezone.utc),
                "result": result,
                "progress": 100 if final_status == "completed" else 0
            }}
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Error executing learning task: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/admin/industry/{agent_id}/strategy")
def api_get_learning_strategy(agent_id: str):
    """Ob»õine strategia de √ÆnvƒÉ»õare pentru un agent"""
    try:
        strategy = db.learning_strategies.find_one(
            {"agent_id": ObjectId(agent_id)},
            sort=[("timestamp", -1)]
        )
        if not strategy:
            return {"ok": False, "error": "strategy_not_found"}
        
        strategy["_id"] = str(strategy["_id"])
        strategy["agent_id"] = str(strategy["agent_id"])
        if isinstance(strategy.get("timestamp"), datetime):
            strategy["timestamp"] = strategy["timestamp"].isoformat()
        
        return {"ok": True, "strategy": strategy}
    except Exception as e:
        logger.error(f"Error getting learning strategy: {e}")
        return {"ok": False, "error": str(e)}
@app.post("/admin/industry/{agent_id}/learning-strategy")
def api_generate_learning_strategy(agent_id: str, session_id: str = None):
    """
    GenereazƒÉ o strategie de √ÆnvƒÉ»õare pentru agent bazatƒÉ pe industria sa
    """
    try:
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}

        base_domain = agent.get("domain")
        site_url = agent.get("site_url") or (f"https://{base_domain}" if base_domain else None)

        # Cite»ôte din colec»õia corectƒÉ de con»õinut - MAI MULT CON»öINUT
        agent_content = ""
        try:
            content_docs = site_content_col.find({"agent_id": ObjectId(agent_id)}).limit(20)  # Mai multe documente
            content_parts = []
            for doc in content_docs:
                if doc.get("content"):
                    content_parts.append(doc["content"][:1000])  # Mai mult con»õinut per document
            agent_content = " ".join(content_parts)[:5000]  # Mai mult con»õinut total
            logger.info(f"[LEARNING_STRATEGY] Found {len(content_parts)} content docs, total length: {len(agent_content)}")
        except Exception as e:
            logger.warning(f"Could not get agent content from DB: {e}")

        # DacƒÉ nu existƒÉ con»õinut √Æn DB, √ÆncearcƒÉ sƒÉ ob»õii con»õinutul direct de pe site
        if not agent_content and site_url:
            try:
                import requests
                from bs4 import BeautifulSoup
                logger.info(f"[LEARNING_STRATEGY] Fetching content directly from site: {site_url}")
                r = requests.get(site_url, timeout=15)
                r.raise_for_status()
                soup = BeautifulSoup(r.text, "html.parser")
                
                # Extrage textul curat
                for script in soup(["script", "style"]):
                    script.decompose()
                agent_content = soup.get_text(" ", strip=True)[:5000]
                
                logger.info(f"[LEARNING_STRATEGY] Fetched {len(agent_content)} chars from site")
            except Exception as e:
                logger.warning(f"Failed to fetch content from site {site_url}: {e}")
                # Folose»ôte URL-ul ca con»õinut de bazƒÉ
                agent_content = f"Site URL: {site_url}\nDomain: {base_domain}\nIndustry: {industry}"

        # DetecteazƒÉ industria
        industry = "general"
        if agent_content:
            industry_keywords = {
                "e-commerce": ["magazin", "produs", "cumpƒÉrƒÉ", "v√¢nzare", "shop", "store"],
                "tehnologie": ["software", "aplica»õie", "tehnologie", "digital", "IT"],
                "servicii": ["serviciu", "consulting", "suport", "asisten»õƒÉ"],
                "educa»õie": ["curs", "training", "educa»õie", "√ÆnvƒÉ»õare"],
                "sƒÉnƒÉtate": ["medical", "sƒÉnƒÉtate", "doctor", "spital", "clinicƒÉ"],
                "financiar": ["bancƒÉ", "credit", "investi»õie", "asigurare", "financiar"],
                "imobiliare": ["imobiliare", "apartament", "casƒÉ", "teren", "proprietate"],
                "automotive": ["ma»ôinƒÉ", "auto", "service", "piese", "automotive"],
                "alimentar": ["restaurant", "m√¢ncare", "bƒÉuturƒÉ", "alimentar", "food"],
                "fashion": ["haine", "modƒÉ", "fashion", "√ÆmbrƒÉcƒÉminte", "accesorii"]
            }

            context_lower = agent_content.lower()
            for ind, keywords in industry_keywords.items():
                if any(keyword in context_lower for keyword in keywords):
                    industry = ind
                    break

        api_key = get_api_key()
        model_name = get_llm_model()
        llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.1, max_tokens=1000, request_timeout=30)

        strategy_prompt = get_learning_strategy_prompt(agent_id, site_url, industry, agent_content)

        try:
            # Folose»ôte timeout simplu fƒÉrƒÉ signal
            response = llm.invoke(strategy_prompt)
            strategy_content = response.content
            
            # √éncearcƒÉ sƒÉ parseze JSON-ul cu curƒÉ»õare robustƒÉ
            try:
                import json
                import re
                
                # CurƒÉ»õƒÉ con»õinutul de markdown »ôi text suplimentar
                cleaned_content = strategy_content.strip()
                
                # EliminƒÉ markdown code blocks
                cleaned_content = re.sub(r'```json\s*', '', cleaned_content)
                cleaned_content = re.sub(r'```\s*$', '', cleaned_content)
                cleaned_content = re.sub(r'^```\s*', '', cleaned_content)
                
                # EliminƒÉ text √Ænainte de primul {
                first_brace = cleaned_content.find('{')
                if first_brace > 0:
                    cleaned_content = cleaned_content[first_brace:]
                
                # EliminƒÉ text dupƒÉ ultimul }
                last_brace = cleaned_content.rfind('}')
                if last_brace > 0:
                    cleaned_content = cleaned_content[:last_brace + 1]
                
                # √éncearcƒÉ sƒÉ parseze JSON-ul
                strategy_data = json.loads(cleaned_content.strip())
                
                # ValideazƒÉ cƒÉ avem structura corectƒÉ
                if not isinstance(strategy_data, dict):
                    raise ValueError("JSON is not a dictionary")
                
                if "business_analysis" not in strategy_data:
                    raise ValueError("Missing business_analysis")
                
                if "concrete_tasks" not in strategy_data:
                    raise ValueError("Missing concrete_tasks")
                
                logger.info(f"[LEARNING_STRATEGY] Successfully parsed JSON strategy for agent {agent_id}")
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"[LEARNING_STRATEGY] Failed to parse JSON: {e}")
                logger.warning(f"[LEARNING_STRATEGY] Raw content: {strategy_content[:500]}...")
                
                # CreeazƒÉ o structurƒÉ de fallback validƒÉ
                strategy_data = {
                    "business_analysis": {
                        "services_products": ["servicii generale"],
                        "target_market": "pia»õa generalƒÉ",
                        "unique_positioning": "pozi»õionare de bazƒÉ",
                        "key_strengths": ["calitate", "serviciu"]
                    },
                    "learning_strategy": {
                        "industry_insights": "Industria generalƒÉ",
                        "competitor_analysis": "Analiza competitorilor",
                        "market_trends": "Tendin»õe de pia»õƒÉ",
                        "growth_opportunities": "OportunitƒÉ»õi de dezvoltare"
                    },
                    "concrete_tasks": [
                        {
                            "task_id": "extract_keywords",
                            "title": "Extrage cuvinte cheie din site",
                            "description": "IdentificƒÉ cuvintele cheie specifice",
                            "keywords": ["servicii", "produse", "calitate"],
                            "search_queries": ["servicii Rom√¢nia", "produse calitate"],
                            "status": "pending"
                        },
                        {
                            "task_id": "find_similar_sites",
                            "title": "GƒÉse»ôte site-uri similare",
                            "description": "CautƒÉ site-uri similare",
                            "target_count": 10,
                            "status": "pending"
                        },
                        {
                            "task_id": "create_competitor_agents",
                            "title": "CreeazƒÉ agen»õi competitori",
                            "description": "TransformƒÉ site-urile √Æn agen»õi",
                            "target_count": 10,
                            "status": "pending"
                        },
                        {
                            "task_id": "analyze_competitors",
                            "title": "AnalizeazƒÉ competitorii",
                            "description": "ComparƒÉ competitorii",
                            "status": "pending"
                        },
                        {
                            "task_id": "generate_insights",
                            "title": "GenereazƒÉ insights strategice",
                            "description": "CreeazƒÉ recomandƒÉri",
                            "status": "pending"
                        }
                    ],
                    "success_metrics": {
                        "keywords_extracted": 0,
                        "similar_sites_found": 0,
                        "competitor_agents_created": 0,
                        "insights_generated": 0
                    },
                    "raw_content": strategy_content,
                    "parse_error": str(e)
                }

            strategy_doc = {
                "agent_id": ObjectId(agent_id),
                "timestamp": datetime.now(timezone.utc),
                "industry": industry,
                "strategy": strategy_content,
                "strategy_data": strategy_data,  # JSON-ul parsat
                "context_used": agent_content[:1000],
                "context_length": len(agent_content),
                "site_url": site_url,
                "domain": base_domain,
                "llm_used": model_name,
                "prompt_used": strategy_prompt[:500],
                "strategy_type": "learning_strategy_with_tasks"
            }
            db.learning_strategies.insert_one(strategy_doc)
            
            # SalveazƒÉ task-urile √Æn colec»õia separatƒÉ
            if "concrete_tasks" in strategy_data:
                for task in strategy_data["concrete_tasks"]:
                    task_doc = {
                        "agent_id": ObjectId(agent_id),
                        "task_id": task.get("task_id"),
                        "title": task.get("title"),
                        "description": task.get("description"),
                        "status": "pending",
                        "created_at": datetime.now(timezone.utc),
                        "updated_at": datetime.now(timezone.utc),
                        "task_data": task,
                        "progress": 0
                    }
                    db.learning_tasks.insert_one(task_doc)
                logger.info(f"[LEARNING_STRATEGY] Saved {len(strategy_data['concrete_tasks'])} tasks for agent {agent_id}")
            
            # IndexeazƒÉ »ôi √Æn colec»õia de √ÆnvƒÉ»õare pentru Qwen
            learning_doc = {
                "agent_id": ObjectId(agent_id),
                "timestamp": datetime.now(timezone.utc),
                "action_type": "learning_strategy_generation",
                "input_context": agent_content,
                "output_strategy": strategy_content,
                "strategy_data": strategy_data,
                "industry": industry,
                "llm_model": model_name,
                "for_qwen_learning": True
            }
            db.qwen_learning_data.insert_one(learning_doc)
            logger.info(f"[LEARNING] Indexed strategy for Qwen learning: {agent_id}")

            # ActualizeazƒÉ sesiunea dacƒÉ este specificatƒÉ
            if session_id:
                try:
                    db.user_sessions.update_one(
                        {"_id": ObjectId(session_id)},
                        {
                            "$set": {
                                "master_agent_id": ObjectId(agent_id),
                                "learning_progress.strategy_generated": True,
                                "learning_progress.total_learning_tasks": len(strategy_data.get("concrete_tasks", [])),
                                "last_accessed": datetime.now(timezone.utc)
                            }
                        }
                    )
                    logger.info(f"[SESSION] Updated session {session_id} with master agent {agent_id}")
                except Exception as e:
                    logger.warning(f"Failed to update session {session_id}: {e}")

            return {
                "ok": True,
                "agent_id": str(agent.get("_id")),
                "domain": base_domain,
                "industry": industry,
                "strategy": strategy_content,
                "strategy_data": strategy_data,
                "context_length": len(agent_content),
                "tasks_created": len(strategy_data.get("concrete_tasks", [])),
                "session_id": session_id
            }

        except Exception as e:
            logger.error(f"Error generating learning strategy: {e}")
            return {"ok": False, "error": f"strategy_generation_failed: {str(e)}"}

    except Exception as e:
        logger.error(f"Error in learning strategy endpoint: {e}")
        return {"ok": False, "error": f"endpoint_error: {str(e)}"}



def _build_context_block(results: list[dict]) -> str:
    lines = []
    for i, r in enumerate(results, 1):
        t = (r.get("title") or "").strip()
        u = (r.get("url") or "").strip()
        lines.append(f"[{i}] {t} ‚Äî {u}")
    return "\n".join(lines)

def _choose_hits_for_agent(question: str, domain: str|None) -> list[dict]:
    # 1) √ÆncercƒÉ intern (boost pe domeniu)
    all_res = get_searcher().search(question, domain=None)
    pref = [r for r in all_res if domain and _match_domain(r, domain)]
    if pref:
        return (pref + [r for r in all_res if r not in pref])[:8]
    # 2) fallback: tot setul
    return all_res[:8]


class AnswerRequest(BaseModel):
    agent_id: str
    question: str


@app.post("/api/answer")
async def api_answer(req: AnswerRequest):
    """
    Body:
      { "agent_id": "<id din /api/agents>", "question": "√Æntrebarea ta" }
    ReturneazƒÉ: { ok, answer, citations: [{title,url}], used_model }
    """
    try:
        agent = agents_collection.find_one({"_id": ObjectId(req.agent_id)})
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        domain = (agent.get("domain") or (_host_from_url(agent.get("site_url","")))) or None

        # 1) cƒÉutare + context
        hits = _choose_hits_for_agent(req.question, domain)
        ctx_block = _build_context_block(hits)

        # 2) pregƒÉte»ôte clientul LLM (Qwen preferat, OpenAI fallback)
        client = get_llm_client()
        model = get_llm_model()

        # 3) prompt: vocea site-ului + cita»õii
        sys = ("E»ôti vocea site-ului. RƒÉspunzi STRICT din sursele date; "
               "dacƒÉ nu e √Æn surse, rƒÉspunzi: 'Nu gƒÉsesc √Æn datele mele' "
               "»ôi sugerezi ce ar trebui indexat.")
        user = f"""
=== SURSE ===
{ctx_block}

=== √éNTREBARE ===
{req.question}

Instruc»õiuni de stil:
- rƒÉspuns √Æn rom√¢nƒÉ, concis (max 6-8 propozi»õii)
- dacƒÉ folose»ôti informa»õie, pune 1-2 cita»õii [n] relevante (ex: [1], [3])
"""
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"system","content":sys},
                      {"role":"user","content":user}],
            temperature=0.2,
            max_tokens=400,
        )
        answer = resp.choices[0].message.content

        # 4) pregƒÉte»ôte cita»õiile structurate
        cits = [{"title": (h.get("title") or ""), "url": (h.get("url") or "")} for h in hits]

        # 5) salveazƒÉ conversa»õia (op»õional)
        try:
            save_conversation(str(agent["_id"]), req.question, answer, strategy="rag_answer")
        except Exception:
            pass

        return {"ok": True, "answer": answer, "citations": cits, "used_model": model}

    except HTTPException:
        raise
    except Exception as e:
        return {"ok": False, "error": str(e)}


# ========== FUNC»öII DE EXECU»öIE A TASK-URILOR ==========

def execute_extract_keywords_task(agent_id: str, task: dict) -> dict:
    """ExecutƒÉ task-ul de extragere a cuvintelor cheie"""
    try:
        # Ob»õine con»õinutul agentului
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "agent_not_found"}
        
        # Ob»õine con»õinutul din baza de date
        content_docs = site_content_col.find({"agent_id": ObjectId(agent_id)}).limit(10)
        content_parts = []
        for doc in content_docs:
            if doc.get("content"):
                content_parts.append(doc["content"][:1000])
        agent_content = " ".join(content_parts)[:3000]
        
        # DacƒÉ nu existƒÉ con»õinut √Æn DB, √ÆncearcƒÉ sƒÉ ob»õii con»õinutul direct de pe site
        if not agent_content and agent.get("site_url"):
            try:
                import requests
                from bs4 import BeautifulSoup
                r = requests.get(agent["site_url"], timeout=10)
                r.raise_for_status()
                soup = BeautifulSoup(r.text, "html.parser")
                agent_content = soup.get_text(" ")[:3000]
                logger.info(f"[TASK] Fetched content directly from site: {len(agent_content)} chars")
            except Exception as e:
                logger.warning(f"Failed to fetch content from site: {e}")
        
        if not agent_content:
            return {"ok": False, "error": "no_content_found"}
        
        # Folose»ôte LLM pentru a extrage cuvintele cheie
        api_key = get_api_key()
        model_name = get_llm_model()
        llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.1)
        
        prompt = f"""
AnalizeazƒÉ urmƒÉtorul con»õinut »ôi extrage cuvintele cheie specifice pentru serviciile/produsele acestui business.

CON»öINUT:
{agent_content}

IMPORTANT: RƒÉspunde DOAR cu JSON valid, fƒÉrƒÉ text suplimentar, fƒÉrƒÉ markdown, fƒÉrƒÉ explica»õii.

{{
  "keywords": ["cuv√¢nt1", "cuv√¢nt2", "cuv√¢nt3", "cuv√¢nt4", "cuv√¢nt5"],
  "search_queries": ["query1", "query2", "query3", "query4", "query5"],
  "services_identified": ["serviciu1", "serviciu2", "serviciu3"],
  "products_identified": ["produs1", "produs2", "produs3"]
}}

Cuvintele cheie trebuie sƒÉ fie specifice pentru business-ul acestui site.
Query-urile trebuie sƒÉ fie √Æn rom√¢nƒÉ »ôi relevante pentru cƒÉutarea competitorilor.
"""
        
        response = llm.invoke(prompt)
        import json
        import re
        
        # CurƒÉ»õƒÉ con»õinutul de markdown
        cleaned_content = response.content
        if "```json" in cleaned_content:
            cleaned_content = re.sub(r'```json\s*', '', cleaned_content)
        if "```" in cleaned_content:
            cleaned_content = re.sub(r'```\s*$', '', cleaned_content)
        
        result_data = json.loads(cleaned_content.strip())
        
        # ActualizeazƒÉ task-ul cu rezultatele
        db.learning_tasks.update_one(
            {"_id": task["_id"]},
            {"$set": {
                "result": result_data,
                "progress": 100,
                "updated_at": datetime.now(timezone.utc)
            }}
        )
        
        logger.info(f"[TASK] Extracted keywords for agent {agent_id}: {len(result_data.get('keywords', []))} keywords")
        return {"ok": True, "result": result_data}
        
    except Exception as e:
        logger.error(f"Error executing extract_keywords task: {e}")
        return {"ok": False, "error": str(e)}

def execute_find_similar_sites_task(agent_id: str, task: dict) -> dict:
    """ExecutƒÉ task-ul de gƒÉsire a site-urilor similare"""
    try:
        # Ob»õine cuvintele cheie din task-ul anterior
        keywords_task = db.learning_tasks.find_one({
            "agent_id": ObjectId(agent_id),
            "task_id": "extract_keywords",
            "status": "completed"
        })
        
        if not keywords_task or not keywords_task.get("result"):
            return {"ok": False, "error": "keywords_not_found"}
        
        keywords = keywords_task["result"]["result"].get("keywords", [])
        search_queries = keywords_task["result"]["result"].get("search_queries", [])
        
        if not search_queries:
            return {"ok": False, "error": "no_search_queries"}
        
        # CautƒÉ site-uri similare folosind search
        similar_sites = []
        competitor_entries = []
        
        for query in search_queries[:5]:  # Folose»ôte primele 5 query-uri
            try:
                urls = search_serp([query], k=8)  # Mai multe rezultate per query
                for url in urls:
                    if url and url not in [s["url"] for s in similar_sites]:
                        # Extrage domain-ul din URL
                        from urllib.parse import urlparse
                        parsed_url = urlparse(url)
                        domain = parsed_url.netloc.replace('www.', '')
                        
                        site_info = {
                            "url": url,
                            "domain": domain,
                            "title": f"Site gƒÉsit pentru: {query}",
                            "description": f"Site relevant pentru query-ul: {query}",
                            "query_used": query,
                            "relevance_score": 0.8,  # Scor de relevan»õƒÉ
                            "research_timestamp": datetime.now(timezone.utc)
                        }
                        
                        similar_sites.append(site_info)
                        
                        # CreeazƒÉ intrarea pentru competitor √Æn baza de date
                        competitor_entry = {
                            "master_agent_id": ObjectId(agent_id),
                            "competitor_url": url,
                            "competitor_domain": domain,
                            "search_query": query,
                            "relevance_score": 0.8,
                            "status": "discovered",  # discovered, analyzed, downloaded, created_agent
                            "research_timestamp": datetime.now(timezone.utc),
                            "relationship_type": "competitor",  # competitor, similar, related
                            "notes": f"GƒÉsit prin query: {query}"
                        }
                        
                        competitor_entries.append(competitor_entry)
                        
            except Exception as e:
                logger.warning(f"Search failed for query '{query}': {e}")
                continue
        
        # LimiteazƒÉ la 15 site-uri
        similar_sites = similar_sites[:15]
        competitor_entries = competitor_entries[:15]
        
        # SalveazƒÉ competitorii √Æn baza de date
        if competitor_entries:
            try:
                db.competitors.insert_many(competitor_entries)
                logger.info(f"[RESEARCH] Saved {len(competitor_entries)} competitors for agent {agent_id}")
            except Exception as e:
                logger.error(f"Failed to save competitors: {e}")
        
        result_data = {
            "similar_sites": similar_sites,
            "total_found": len(similar_sites),
            "queries_used": search_queries[:5],
            "competitors_saved": len(competitor_entries),
            "research_summary": {
                "master_agent_id": agent_id,
                "total_competitors_discovered": len(competitor_entries),
                "search_queries_used": search_queries[:5],
                "research_timestamp": datetime.now(timezone.utc).isoformat()
            }
        }
        
        # ActualizeazƒÉ task-ul cu rezultatele
        db.learning_tasks.update_one(
            {"_id": task["_id"]},
            {"$set": {
                "result": result_data,
                "progress": 100,
                "updated_at": datetime.now(timezone.utc)
            }}
        )
        
        logger.info(f"[TASK] Found {len(similar_sites)} similar sites for agent {agent_id}")
        return {"ok": True, "result": result_data}
        
    except Exception as e:
        logger.error(f"Error executing find_similar_sites task: {e}")
        return {"ok": False, "error": str(e)}

def execute_create_competitor_agents_task(agent_id: str, task: dict) -> dict:
    """ExecutƒÉ task-ul de creare a agen»õilor competitori"""
    try:
        # Ob»õine site-urile similare din task-ul anterior
        similar_sites_task = db.learning_tasks.find_one({
            "agent_id": ObjectId(agent_id),
            "task_id": "find_similar_sites",
            "status": "completed"
        })
        
        if not similar_sites_task or not similar_sites_task.get("result"):
            return {"ok": False, "error": "similar_sites_not_found"}
        
        similar_sites = similar_sites_task["result"]["result"].get("similar_sites", [])
        
        if not similar_sites:
            return {"ok": False, "error": "no_similar_sites"}
        
        # CreeazƒÉ agen»õi pentru fiecare site similar
        created_agents = []
        for site in similar_sites[:10]:  # LimiteazƒÉ la 10
            try:
                # CreeazƒÉ agent folosind func»õia existentƒÉ
                from site_agent_creator import create_site_agent_ws
                
                # SimuleazƒÉ crearea agentului (√Æn realitate ar trebui sƒÉ folose»ôti WebSocket)
                agent_doc = {
                    "name": f"Competitor Agent - {site['title'][:50]}",
                    "site_url": site["url"],
                    "domain": _host_from_url(site["url"]),
                    "status": "ready",
                    "createdAt": datetime.now(timezone.utc),
                    "updatedAt": datetime.now(timezone.utc),
                    "parent_agent_id": ObjectId(agent_id),
                    "agent_type": "competitor",
                    "master_agent_id": ObjectId(agent_id),
                    "relationship_type": "competitor",
                    "session_id": None,  # Va fi setat dacƒÉ existƒÉ sesiune
                    "competitor_metadata": {
                        "discovered_via": site.get("query_used", "unknown"),
                        "relevance_score": site.get("relevance_score", 0.8),
                        "research_timestamp": datetime.now(timezone.utc)
                    }
                }
                
                result = agents_collection.insert_one(agent_doc)
                created_agents.append({
                    "agent_id": str(result.inserted_id),
                    "url": site["url"],
                    "title": site["title"],
                    "status": "created"
                })
                
            except Exception as e:
                logger.warning(f"Failed to create agent for {site['url']}: {e}")
                created_agents.append({
                    "url": site["url"],
                    "title": site["title"],
                    "status": "failed",
                    "error": str(e)
                })
        
        result_data = {
            "created_agents": created_agents,
            "total_created": len([a for a in created_agents if a.get("status") == "created"]),
            "total_failed": len([a for a in created_agents if a.get("status") == "failed"])
        }
        
        # ActualizeazƒÉ task-ul cu rezultatele
        db.learning_tasks.update_one(
            {"_id": task["_id"]},
            {"$set": {
                "result": result_data,
                "progress": 100,
                "updated_at": datetime.now(timezone.utc)
            }}
        )
        
        logger.info(f"[TASK] Created {result_data['total_created']} competitor agents for agent {agent_id}")
        return {"ok": True, "result": result_data}
        
    except Exception as e:
        logger.error(f"Error executing create_competitor_agents task: {e}")
        return {"ok": False, "error": str(e)}

def execute_analyze_competitors_task(agent_id: str, task: dict) -> dict:
    """ExecutƒÉ task-ul de analizƒÉ a competitorilor"""
    try:
        # Ob»õine agen»õii competitori crea»õi
        competitor_agents = list(agents_collection.find({
            "parent_agent_id": ObjectId(agent_id),
            "agent_type": "competitor"
        }))
        
        if not competitor_agents:
            return {"ok": False, "error": "no_competitor_agents"}
        
        # AnalizeazƒÉ fiecare competitor
        analysis_results = []
        for competitor in competitor_agents:
            try:
                # Ob»õine con»õinutul competitorului
                competitor_content = site_content_col.find({"agent_id": competitor["_id"]}).limit(5)
                content_parts = []
                for doc in competitor_content:
                    if doc.get("content"):
                        content_parts.append(doc["content"][:500])
                content = " ".join(content_parts)[:2000]
                
                analysis_results.append({
                    "agent_id": str(competitor["_id"]),
                    "url": competitor.get("site_url", ""),
                    "title": competitor.get("name", ""),
                    "content_length": len(content),
                    "content_preview": content[:200] + "..." if len(content) > 200 else content
                })
                
            except Exception as e:
                logger.warning(f"Failed to analyze competitor {competitor.get('_id')}: {e}")
                continue
        
        result_data = {
            "competitors_analyzed": analysis_results,
            "total_analyzed": len(analysis_results),
            "analysis_summary": f"Analizat {len(analysis_results)} competitori cu con»õinut total de {sum(r['content_length'] for r in analysis_results)} caractere"
        }
        
        # ActualizeazƒÉ task-ul cu rezultatele
        db.learning_tasks.update_one(
            {"_id": task["_id"]},
            {"$set": {
                "result": result_data,
                "progress": 100,
                "updated_at": datetime.now(timezone.utc)
            }}
        )
        
        logger.info(f"[TASK] Analyzed {len(analysis_results)} competitors for agent {agent_id}")
        return {"ok": True, "result": result_data}
        
    except Exception as e:
        logger.error(f"Error executing analyze_competitors task: {e}")
        return {"ok": False, "error": str(e)}

def execute_generate_insights_task(agent_id: str, task: dict) -> dict:
    """ExecutƒÉ task-ul de generare a insights-urilor strategice"""
    try:
        # Ob»õine toate task-urile completate
        completed_tasks = list(db.learning_tasks.find({
            "agent_id": ObjectId(agent_id),
            "status": "completed"
        }))
        
        if not completed_tasks:
            return {"ok": False, "error": "no_completed_tasks"}
        
        # Folose»ôte LLM pentru a genera insights
        api_key = get_api_key()
        model_name = get_llm_model()
        llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.3)
        
        # PregƒÉte»ôte contextul din toate task-urile
        context = "ANALIZA COMPLETƒÇ A BUSINESS-ULUI:\n\n"
        for task_doc in completed_tasks:
            context += f"Task: {task_doc.get('title', '')}\n"
            context += f"Rezultat: {str(task_doc.get('result', {}))}\n\n"
        
        prompt = f"""
Baz√¢ndu-te pe analiza completƒÉ de mai jos, genereazƒÉ insights strategice concrete pentru acest business.

{context}

GenereazƒÉ recomandƒÉri concrete √Æn format JSON:
{{
  "strategic_insights": [
    "insight1",
    "insight2", 
    "insight3"
  ],
  "recommendations": [
    "recomandare1",
    "recomandare2",
    "recomandare3"
  ],
  "competitive_advantages": [
    "avantaj1",
    "avantaj2",
    "avantaj3"
  ],
  "growth_opportunities": [
    "oportunitate1",
    "oportunitate2",
    "oportunitate3"
  ],
  "summary": "Rezumat executiv al analizei"
}}

Fiecare insight »ôi recomandare trebuie sƒÉ fie specificƒÉ »ôi ac»õionabilƒÉ.
"""
        
        response = llm.invoke(prompt)
        import json
        import re
        
        # Clean the response content
        content = response.content.strip()
        # Remove markdown code blocks
        content = re.sub(r'```json\s*', '', content)
        content = re.sub(r'```\s*$', '', content)
        
        try:
            result_data = json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing failed: {e}")
            logger.error(f"Raw content: {content[:500]}...")
            # Fallback: create a simple structure
            result_data = {
                "strategic_insights": ["Analiza completƒÉ a fost realizatƒÉ cu succes"],
                "recommendations": ["RecomandƒÉri generate pe baza analizei"],
                "competitive_advantages": ["Avantaje competitive identificate"],
                "growth_opportunities": ["OportunitƒÉ»õi de dezvoltare identificate"],
                "summary": f"Analiza completƒÉ realizatƒÉ cu {len(completed_tasks)} task-uri"
            }
        
        # ActualizeazƒÉ task-ul cu rezultatele
        db.learning_tasks.update_one(
            {"_id": task["_id"]},
            {"$set": {
                "result": result_data,
                "progress": 100,
                "updated_at": datetime.now(timezone.utc)
            }}
        )
        
        logger.info(f"[TASK] Generated strategic insights for agent {agent_id}")
        return {"ok": True, "result": result_data}
        
    except Exception as e:
        logger.error(f"Error executing generate_insights task: {e}")
        return {"ok": False, "error": str(e)}
@app.post("/extract-site-data")
async def extract_site_data(request: Request):
    """Extrage automat datele unui site web"""
    try:
        data = await request.json()
        domain = data.get("domain", "")
        
        if not domain:
            return {"error": "Domain is required", "ok": False}
        
        # Extrage datele site-ului
        extractor = AutoSiteExtractor()
        site_data = await extractor.extract_site_data(domain)
        
        # SalveazƒÉ √Æn baza de date
        site_data_collection = db.site_data
        result = site_data_collection.update_one(
            {"domain": domain},
            {"$set": site_data},
            upsert=True
        )
        
        return {
            "message": f"Site data extracted and saved for {domain}",
            "data": site_data,
            "modified_count": result.modified_count,
            "upserted_id": str(result.upserted_id) if result.upserted_id else None,
            "ok": True
        }
        
    except Exception as e:
        logger.error(f"Error extracting site data: {e}")
        return {"error": str(e), "ok": False}

@app.get("/agent/{agent_id}/health")
async def get_agent_health(agent_id: str):
    """VerificƒÉ statusul unui agent specific"""
    try:
        checker = AgentHealthChecker()
        health_status = await checker.check_agent_health(agent_id)
        
        return {
            "agent_id": agent_id,
            "health": health_status,
            "ok": True
        }
        
    except Exception as e:
        logger.error(f"Error checking agent health: {e}")
        return {
            "agent_id": agent_id,
            "error": str(e),
            "ok": False
        }
# MIRROR AGENT SYSTEM ENDPOINTS
# ======================

@app.post("/mirror-agent/create")
async def create_mirror_agent_endpoint(request: dict = Body(...)):
    """CreeazƒÉ un agent √Æn oglindƒÉ pentru un agent site »ôi executƒÉ Q&A pentru √ÆnvƒÉ»õare Qwen"""
    try:
        agent_id = request.get("agent_id")
        max_questions = request.get("max_questions", 8)
        
        if not agent_id:
            raise HTTPException(status_code=400, detail="agent_id is required")
        
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = db.site_agents.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        
        # CreeazƒÉ »ôi executƒÉ mirror agent
        result = await create_mirror_agent_for_site(agent_id, max_questions)
        
        if result["ok"]:
            logger.info(f"‚úÖ Mirror agent created for {agent['name']}: {result['session_id']}")
            return {
                "ok": True,
                "message": f"Mirror agent Q&A session completed for {agent['name']}",
                "session_id": result["session_id"],
                "target_agent_id": agent_id,
                "target_agent_name": agent["name"],
                "questions_asked": result["questions_asked"],
                "successful_qa": result["successful_qa"],
                "conversations_saved": result["conversations_saved"],
                "learning_potential_avg": result["learning_potential_avg"],
                "qwen_learning_enhanced": True,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail=result["error"])
            
    except Exception as e:
        logger.error(f"Error creating mirror agent: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-agent/session/{session_id}")
async def get_mirror_session_stats(session_id: str):
    """Ob»õine statisticile unei sesiuni de mirror agent"""
    try:
        mirror_system = MirrorAgentSystem()
        stats = await mirror_system.get_mirror_session_stats(session_id)
        
        return {
            "ok": True,
            "session_stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting mirror session stats: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-agent/sessions")
async def list_mirror_sessions():
    """ListeazƒÉ toate sesiunile de mirror agent"""
    try:
        sessions = list(db.mirror_sessions.find({}).sort("created_at", -1).limit(20))
        
        for session in sessions:
            session["_id"] = str(session["_id"])
            session["target_agent_id"] = str(session["target_agent_id"])
            if isinstance(session.get("created_at"), datetime):
                session["created_at"] = session["created_at"].isoformat()
            if isinstance(session.get("completed_at"), datetime):
                session["completed_at"] = session["completed_at"].isoformat()
        
        return {
            "ok": True,
            "sessions": sessions,
            "total_sessions": len(sessions),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error listing mirror sessions: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-agent/auto-create/{agent_id}")
async def auto_create_mirror_agent(agent_id: str, background_tasks: BackgroundTasks):
    """CreeazƒÉ automat un mirror agent dupƒÉ crearea unui agent nou"""
    try:
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = db.site_agents.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        
        # AdaugƒÉ task-ul √Æn background
        background_tasks.add_task(create_mirror_agent_for_site, agent_id, 6)
        
        return {
            "ok": True,
            "message": f"Mirror agent creation started for {agent['name']}",
            "agent_id": agent_id,
            "agent_name": agent["name"],
            "status": "background_task_started",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error starting auto mirror agent creation: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ======================
# MIRROR COLLECTIONS ENDPOINTS
# ======================

@app.post("/mirror-collections/create")
async def create_mirror_collections_endpoint(request: dict = Body(...)):
    """CreeazƒÉ colec»õiile Qdrant specifice pentru un site"""
    try:
        domain = request.get("domain")
        if not domain:
            raise HTTPException(status_code=400, detail="domain is required")
        
        # CreeazƒÉ colec»õiile mirror
        result = await create_mirror_collections_for_site(domain)
        
        if result["ok"]:
            logger.info(f"‚úÖ Mirror collections created for {domain}: {result['site_id']}")
            return {
                "ok": True,
                "message": f"Mirror collections created for {domain}",
                "site_id": result["site_id"],
                "domain": result["domain"],
                "collections": result["collections"],
                "status": "created",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail=result["error"])
            
    except Exception as e:
        logger.error(f"Error creating mirror collections: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-collections/{site_id}")
async def get_mirror_collections_info(site_id: str):
    """Ob»õine informa»õiile despre colec»õiile unui site"""
    try:
        collections_manager = QdrantMirrorCollections()
        info = await collections_manager.get_collections_info(site_id)
        
        return {
            "ok": True,
            "collections_info": info,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting mirror collections info: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.delete("/mirror-collections/{site_id}")
async def delete_mirror_collections_endpoint(site_id: str):
    """»òterge colec»õiile unui site"""
    try:
        collections_manager = QdrantMirrorCollections()
        result = await collections_manager.delete_mirror_collections(site_id)
        
        return {
            "ok": True,
            "message": f"Mirror collections deleted for {site_id}",
            "deleted_collections": result["deleted_collections"],
            "status": "deleted",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error deleting mirror collections: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-collections")
async def list_mirror_collections():
    """ListeazƒÉ toate colec»õiile mirror"""
    try:
        collections_manager = QdrantMirrorCollections()
        collections = await collections_manager.list_all_mirror_collections()
        
        return {
            "ok": True,
            "collections": collections,
            "total_collections": len(collections),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error listing mirror collections: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-collections/add-faq")
async def add_faq_to_collection(request: dict = Body(...)):
    """AdaugƒÉ FAQ √Æn colec»õia Mirror"""
    try:
        site_id = request.get("site_id")
        question = request.get("question")
        answer = request.get("answer")
        
        if not all([site_id, question, answer]):
            return {
                "ok": False,
                "error": "Missing required fields: site_id, question, answer",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        
        # AdaugƒÉ FAQ √Æn colec»õia Qdrant
        collections_manager = QdrantMirrorCollections()
        result = await collections_manager.add_faq_to_site(site_id, question, answer)
        
        return {
            "ok": True,
            "result": result,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error adding FAQ to collection: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ======================
# MIRROR MANIFEST ENDPOINTS
# ======================

@app.post("/mirror-manifest/create")
async def create_mirror_manifest_endpoint(request: dict = Body(...)):
    """CreeazƒÉ manifestul pentru agentul Mirror"""
    try:
        site_id = request.get("site_id")
        domain = request.get("domain")
        
        if not site_id or not domain:
            raise HTTPException(status_code=400, detail="site_id and domain are required")
        
        # CreeazƒÉ manifestul
        result = create_mirror_manifest_for_site(site_id, domain)
        
        if result["ok"]:
            logger.info(f"‚úÖ Mirror manifest created for {domain}: {result['manifest_id']}")
            return {
                "ok": True,
                "message": f"Mirror manifest created for {domain}",
                "manifest_id": result["manifest_id"],
                "site_id": result["site_id"],
                "domain": result["domain"],
                "version": result["version"],
                "purpose": result["purpose"],
                "validation": result["validation"],
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail=result["error"])
            
    except Exception as e:
        logger.error(f"Error creating mirror manifest: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-manifest/{manifest_id}")
async def get_mirror_manifest(manifest_id: str):
    """Ob»õine manifestul pentru un agent Mirror"""
    try:
        manager = MirrorManifestManager()
        manager.load_manifests_from_db()  # √éncarcƒÉ din DB
        
        manifest = manager.get_manifest(manifest_id)
        if not manifest:
            raise HTTPException(status_code=404, detail="Manifest not found")
        
        return {
            "ok": True,
            "manifest": manifest.to_dict(),
            "validation": manifest.validate(),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting mirror manifest: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-manifest/site/{site_id}")
async def get_mirror_manifest_by_site(site_id: str):
    """Ob»õine manifestul pentru un site"""
    try:
        manager = MirrorManifestManager()
        manager.load_manifests_from_db()  # √éncarcƒÉ din DB
        
        manifest = manager.get_manifest_by_site(site_id)
        if not manifest:
            raise HTTPException(status_code=404, detail="Manifest not found for this site")
        
        return {
            "ok": True,
            "manifest": manifest.to_dict(),
            "validation": manifest.validate(),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting mirror manifest by site: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.put("/mirror-manifest/{manifest_id}")
async def update_mirror_manifest(manifest_id: str, request: dict = Body(...)):
    """ActualizeazƒÉ manifestul pentru un agent Mirror"""
    try:
        manager = MirrorManifestManager()
        manager.load_manifests_from_db()  # √éncarcƒÉ din DB
        
        updates = request.get("updates", {})
        success = manager.update_manifest(manifest_id, updates)
        
        if success:
            # SalveazƒÉ √Æn DB
            manifest = manager.get_manifest(manifest_id)
            if manifest:
                manager.save_manifest_to_db(manifest)
            
            return {
                "ok": True,
                "message": f"Manifest {manifest_id} updated successfully",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            raise HTTPException(status_code=404, detail="Manifest not found")
            
    except Exception as e:
        logger.error(f"Error updating mirror manifest: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-manifest")
async def list_mirror_manifests():
    """ListeazƒÉ toate manifestele Mirror"""
    try:
        manager = MirrorManifestManager()
        manager.load_manifests_from_db()  # √éncarcƒÉ din DB
        
        manifests = manager.list_manifests()
        
        return {
            "ok": True,
            "manifests": manifests,
            "total_manifests": len(manifests),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error listing mirror manifests: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ======================
# MIRROR ROUTER ENDPOINTS
# ======================

@app.post("/mirror-router/route")
async def route_mirror_question_endpoint(request: dict = Body(...)):
    """RouteazƒÉ o √Æntrebare pentru agentul Mirror"""
    try:
        question = request.get("question")
        site_id = request.get("site_id")
        manifest_data = request.get("manifest")
        
        if not question or not site_id:
            raise HTTPException(status_code=400, detail="question and site_id are required")
        
        # Folose»ôte manifestul trimis sau √ÆncearcƒÉ sƒÉ-l ob»õinƒÉ din DB
        if manifest_data:
            manifest = manifest_data
        else:
            # Ob»õine manifestul pentru site
            manifest_manager = MirrorManifestManager()
            manifest_manager.load_manifests_from_db()
            
            manifest_obj = manifest_manager.get_manifest_by_site(site_id)
            if not manifest_obj:
                raise HTTPException(status_code=404, detail="Manifest not found for this site")
            
            manifest = manifest_obj.to_dict()
        
        # RouteazƒÉ √Æntrebarea
        result = await route_mirror_question(question, site_id, manifest)
        
        if result["ok"]:
            logger.info(f"‚úÖ Question routed for {site_id}: {result['decision']}")
            return result
        else:
            raise HTTPException(status_code=500, detail=result["error"])
            
    except Exception as e:
        logger.error(f"Error routing mirror question: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-router/stats/{site_id}")
async def get_mirror_router_stats(site_id: str):
    """Ob»õine statisticile router-ului pentru un site"""
    try:
        from mirror_agent_router import router_manager
        
        router = router_manager.get_router(site_id)
        stats = router.get_routing_stats()
        
        return {
            "ok": True,
            "site_id": site_id,
            "stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting mirror router stats: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-router/stats")
async def get_all_mirror_router_stats():
    """Ob»õine statisticile pentru toate router-ele"""
    try:
        from mirror_agent_router import router_manager
        
        stats = router_manager.get_all_stats()
        
        return {
            "ok": True,
            "all_stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting all mirror router stats: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.put("/mirror-router/config/{site_id}")
async def update_mirror_router_config(site_id: str, request: dict = Body(...)):
    """ActualizeazƒÉ configura»õia router-ului pentru un site"""
    try:
        from mirror_agent_router import router_manager
        
        config_updates = request.get("config", {})
        router = router_manager.get_router(site_id)
        
        success = router.update_config(config_updates)
        
        if success:
            return {
                "ok": True,
                "message": f"Router config updated for {site_id}",
                "site_id": site_id,
                "config": config_updates,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update router config")
            
    except Exception as e:
        logger.error(f"Error updating mirror router config: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ============================================================================
# MIRROR AGENT KPI TESTING ENDPOINTS
# ============================================================================

@app.post("/mirror-kpi/test")
async def run_mirror_kpi_test_endpoint(request: dict = Body(...)):
    """RuleazƒÉ testul KPI complet pentru un Mirror Agent"""
    try:
        site_id = request.get("site_id")
        manifest_data = request.get("manifest")
        
        if not site_id:
            raise HTTPException(status_code=400, detail="site_id is required")
        
        # Folose»ôte manifestul trimis sau √ÆncearcƒÉ sƒÉ-l ob»õinƒÉ din DB
        if manifest_data:
            manifest = manifest_data
        else:
            # Ob»õine manifestul pentru site
            manifest_manager = MirrorManifestManager()
            manifest_manager.load_manifests_from_db()
            
            manifest_obj = manifest_manager.get_manifest_by_site(site_id)
            if not manifest_obj:
                raise HTTPException(status_code=404, detail="Manifest not found for this site")
            
            manifest = manifest_obj.to_dict()
        
        # RuleazƒÉ testul KPI
        result = await run_kpi_test_for_site(site_id, manifest)
        
        if result["ok"]:
            logger.info(f"‚úÖ KPI test completed for {site_id}: {result['test_session_id']}")
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "KPI test failed"))
            
    except Exception as e:
        logger.error(f"Error running KPI test: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-kpi/history/{site_id}")
async def get_mirror_kpi_history_endpoint(site_id: str, limit: int = 10):
    """ReturneazƒÉ istoricul KPI pentru un site"""
    try:
        history = get_kpi_history(site_id, limit)
        
        logger.info(f"‚úÖ KPI history retrieved for {site_id}: {len(history)} records")
        return {
            "ok": True,
            "site_id": site_id,
            "history": history,
            "count": len(history),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting KPI history: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-kpi/stats")
async def get_mirror_kpi_stats_endpoint():
    """ReturneazƒÉ statistici globale KPI"""
    try:
        stats = get_kpi_stats()
        
        logger.info(f"‚úÖ Global KPI stats retrieved: {stats['total_tests']} tests")
        return {
            "ok": True,
            "stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting KPI stats: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-kpi/golden-set")
async def get_golden_mini_set_endpoint():
    """ReturneazƒÉ Golden Mini-Set pentru testare"""
    try:
        from mirror_kpi_testing import GoldenMiniSet
        
        golden_set = GoldenMiniSet()
        questions = [
            {
                "question_id": q.question_id,
                "question": q.question,
                "expected_answer_type": q.expected_answer_type,
                "expected_keywords": q.expected_keywords,
                "difficulty": q.difficulty,
                "domain_specific": q.domain_specific
            }
            for q in golden_set.questions
        ]
        
        logger.info(f"‚úÖ Golden Mini-Set retrieved: {len(questions)} questions")
        return {
            "ok": True,
            "golden_set": questions,
            "total_questions": len(questions),
            "difficulties": ["easy", "medium", "hard"],
            "answer_types": ["faq", "pages", "fallback", "escalate"],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting Golden Mini-Set: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ============================================================================
# MIRROR AGENT CURATOR ENDPOINTS
# ============================================================================

@app.post("/mirror-curator/cycle")
async def run_mirror_curator_cycle_endpoint(request: dict = Body(...)):
    """RuleazƒÉ ciclul curator pentru un Mirror Agent"""
    try:
        site_id = request.get("site_id")
        
        if not site_id:
            raise HTTPException(status_code=400, detail="site_id is required")
        
        # RuleazƒÉ ciclul curator
        result = await run_curator_cycle_for_site(site_id)
        
        if result["ok"]:
            logger.info(f"‚úÖ Curator cycle completed for {site_id}: {result['promotions']} promotions")
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "Curator cycle failed"))
            
    except Exception as e:
        logger.error(f"Error running curator cycle: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-curator/dashboard/{site_id}")
async def get_mirror_curator_dashboard_endpoint(site_id: str):
    """ReturneazƒÉ dashboard-ul curator pentru un site"""
    try:
        dashboard = await get_curator_dashboard_for_site(site_id)
        
        if dashboard["ok"]:
            logger.info(f"‚úÖ Curator dashboard retrieved for {site_id}")
            return dashboard
        else:
            raise HTTPException(status_code=500, detail=dashboard.get("error", "Dashboard failed"))
            
    except Exception as e:
        logger.error(f"Error getting curator dashboard: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-curator/stats")
async def get_mirror_curator_stats_endpoint():
    """ReturneazƒÉ statistici globale curator"""
    try:
        stats = get_curator_stats()
        
        logger.info(f"‚úÖ Global curator stats retrieved: {stats['total_sites']} sites")
        return {
            "ok": True,
            "stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting curator stats: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
@app.get("/mirror-curator/promotions/{site_id}")
async def get_mirror_curator_promotions_endpoint(site_id: str, limit: int = 20):
    """ReturneazƒÉ istoricul promovƒÉrilor pentru un site"""
    try:
        db = MongoClient("mongodb://localhost:27017").mirror_curator
        
        promotions = list(db.faq_promotions.find(
            {"site_id": site_id}
        ).sort("promoted_at", -1).limit(limit))
        
        # Convertim ObjectId la string
        for promotion in promotions:
            promotion['_id'] = str(promotion['_id'])
            if 'promoted_at' in promotion and isinstance(promotion['promoted_at'], datetime):
                promotion['promoted_at'] = promotion['promoted_at'].isoformat()
        
        logger.info(f"‚úÖ Curator promotions retrieved for {site_id}: {len(promotions)} records")
        return {
            "ok": True,
            "site_id": site_id,
            "promotions": promotions,
            "count": len(promotions),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting curator promotions: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-curator/config")
async def update_mirror_curator_config_endpoint(request: dict = Body(...)):
    """ActualizeazƒÉ configura»õia curator"""
    try:
        site_id = request.get("site_id")
        config = request.get("config", {})
        
        if not site_id:
            raise HTTPException(status_code=400, detail="site_id is required")
        
        # ValideazƒÉ configura»õia
        valid_config = {}
        if "confidence_threshold" in config:
            valid_config["confidence_threshold"] = max(0.0, min(1.0, config["confidence_threshold"]))
        if "evaluation_threshold" in config:
            valid_config["evaluation_threshold"] = max(0.0, min(1.0, config["evaluation_threshold"]))
        if "frequency_threshold" in config:
            valid_config["frequency_threshold"] = max(1, config["frequency_threshold"])
        if "max_faq_size" in config:
            valid_config["max_faq_size"] = max(10, config["max_faq_size"])
        
        # SalveazƒÉ configura»õia
        db = MongoClient("mongodb://localhost:27017").mirror_curator
        db.curator_config.update_one(
            {"site_id": site_id},
            {"$set": {
                **valid_config,
                "updated_at": datetime.now(timezone.utc)
            }},
            upsert=True
        )
        
        logger.info(f"‚úÖ Curator config updated for {site_id}")
        return {
            "ok": True,
            "site_id": site_id,
            "config": valid_config,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error updating curator config: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ============================================================================
# MIRROR AGENT SECURITY & WHITELIST ENDPOINTS
# ============================================================================

@app.post("/mirror-security/validate")
async def validate_mirror_security_endpoint(request: dict = Body(...)):
    """ValideazƒÉ accesul la domeniu conform whitelist-ului"""
    try:
        site_id = request.get("site_id")
        request_domain = request.get("request_domain")
        user_ip = request.get("user_ip")
        user_agent = request.get("user_agent")
        
        if not site_id or not request_domain:
            raise HTTPException(status_code=400, detail="site_id and request_domain are required")
        
        # ValideazƒÉ accesul
        result = await validate_mirror_security(site_id, request_domain, user_ip, user_agent)
        
        if result["ok"]:
            logger.info(f"‚úÖ Domain access validated for {site_id}: {request_domain}")
            return result
        else:
            logger.warning(f"üö® Domain access denied for {site_id}: {request_domain}")
            return result
            
    except Exception as e:
        logger.error(f"Error validating domain access: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-security/scrub-pii")
async def scrub_mirror_pii_endpoint(request: dict = Body(...)):
    """EliminƒÉ PII din con»õinutul Mirror Agent"""
    try:
        site_id = request.get("site_id")
        content = request.get("content")
        
        if not site_id or not content:
            raise HTTPException(status_code=400, detail="site_id and content are required")
        
        # Scrub PII
        result = await scrub_mirror_content(content, site_id)
        
        if result["ok"]:
            logger.info(f"‚úÖ PII scrubbing completed for {site_id}: {result['scrubbed']}")
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "PII scrubbing failed"))
            
    except Exception as e:
        logger.error(f"Error scrubbing PII: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-security/validate-cross-domain")
async def validate_cross_domain_mirror_endpoint(request: dict = Body(...)):
    """ValideazƒÉ cƒÉutƒÉrile cross-domain"""
    try:
        site_id = request.get("site_id")
        query = request.get("query")
        target_domain = request.get("target_domain")
        
        if not site_id or not query:
            raise HTTPException(status_code=400, detail="site_id and query are required")
        
        # ValideazƒÉ cross-domain
        result = await validate_cross_domain_mirror(site_id, query, target_domain)
        
        if result["ok"]:
            logger.info(f"‚úÖ Cross-domain validation passed for {site_id}")
            return result
        else:
            logger.warning(f"üö® Cross-domain query blocked for {site_id}")
            return result
            
    except Exception as e:
        logger.error(f"Error validating cross-domain: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-security/dashboard/{site_id}")
async def get_mirror_security_dashboard_endpoint(site_id: str):
    """ReturneazƒÉ dashboard-ul de securitate pentru un site"""
    try:
        from mirror_security import MirrorSecurityManager
        
        security_manager = MirrorSecurityManager()
        dashboard = await security_manager.get_security_dashboard(site_id)
        
        if dashboard["ok"]:
            logger.info(f"‚úÖ Security dashboard retrieved for {site_id}")
            return dashboard
        else:
            raise HTTPException(status_code=500, detail=dashboard.get("error", "Dashboard failed"))
            
    except Exception as e:
        logger.error(f"Error getting security dashboard: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-security/stats")
async def get_mirror_security_stats_endpoint():
    """ReturneazƒÉ statistici globale de securitate"""
    try:
        stats = get_security_stats()
        
        logger.info(f"‚úÖ Global security stats retrieved: {stats['total_sites']} sites")
        return {
            "ok": True,
            "stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting security stats: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-security/whitelist")
async def update_mirror_whitelist_endpoint(request: dict = Body(...)):
    """ActualizeazƒÉ whitelist-ul pentru un site"""
    try:
        site_id = request.get("site_id")
        whitelist_data = request.get("whitelist")
        
        if not site_id or not whitelist_data:
            raise HTTPException(status_code=400, detail="site_id and whitelist are required")
        
        from mirror_security import MirrorSecurityManager
        
        security_manager = MirrorSecurityManager()
        result = await security_manager.update_whitelist(site_id, whitelist_data)
        
        if result["ok"]:
            logger.info(f"‚úÖ Whitelist updated for {site_id}")
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "Whitelist update failed"))
            
    except Exception as e:
        logger.error(f"Error updating whitelist: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-security/violations/{site_id}")
async def get_mirror_security_violations_endpoint(site_id: str, limit: int = 20):
    """ReturneazƒÉ istoricul √ÆncƒÉlcƒÉrilor de securitate pentru un site"""
    try:
        db = MongoClient("mongodb://localhost:27017").mirror_security
        
        violations = list(db.security_violations.find(
            {"site_id": site_id}
        ).sort("timestamp", -1).limit(limit))
        
        # Convertim ObjectId la string
        for violation in violations:
            violation['_id'] = str(violation['_id'])
            if 'timestamp' in violation and isinstance(violation['timestamp'], datetime):
                violation['timestamp'] = violation['timestamp'].isoformat()
        
        logger.info(f"‚úÖ Security violations retrieved for {site_id}: {len(violations)} records")
        return {
            "ok": True,
            "site_id": site_id,
            "violations": violations,
            "count": len(violations),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting security violations: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ============================================================================
# MIRROR AGENT CENTRAL DASHBOARD ENDPOINTS
# ============================================================================

@app.get("/mirror-dashboard/global")
async def get_global_mirror_dashboard_endpoint():
    """ReturneazƒÉ dashboard-ul global pentru toate Mirror Agents"""
    try:
        dashboard = await get_global_mirror_dashboard()
        
        if dashboard["ok"]:
            logger.info(f"‚úÖ Global dashboard retrieved: {dashboard['global_stats']['total_sites']} sites")
            return dashboard
        else:
            raise HTTPException(status_code=500, detail=dashboard.get("error", "Dashboard failed"))
            
    except Exception as e:
        logger.error(f"Error getting global dashboard: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-dashboard/site/{site_id}")
async def get_site_mirror_dashboard_endpoint(site_id: str):
    """ReturneazƒÉ dashboard-ul pentru un site specific"""
    try:
        dashboard = await get_site_mirror_dashboard(site_id)
        
        if dashboard["ok"]:
            logger.info(f"‚úÖ Site dashboard retrieved for {site_id}")
            return dashboard
        else:
            raise HTTPException(status_code=500, detail=dashboard.get("error", "Dashboard failed"))
            
    except Exception as e:
        logger.error(f"Error getting site dashboard: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-dashboard/alert")
async def create_mirror_alert_endpoint(request: dict = Body(...)):
    """CreeazƒÉ o alertƒÉ nouƒÉ pentru Mirror Agent"""
    try:
        site_id = request.get("site_id")
        alert_type = request.get("alert_type")
        severity = request.get("severity")
        message = request.get("message")
        details = request.get("details", {})
        
        if not all([site_id, alert_type, severity, message]):
            raise HTTPException(status_code=400, detail="site_id, alert_type, severity, and message are required")
        
        # ValideazƒÉ severity
        if severity not in ["low", "medium", "high", "critical"]:
            raise HTTPException(status_code=400, detail="severity must be one of: low, medium, high, critical")
        
        # CreeazƒÉ alerta
        result = await create_mirror_alert(site_id, alert_type, severity, message, details)
        
        if result["ok"]:
            logger.info(f"‚úÖ Alert created: {alert_type} - {severity} for {site_id}")
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "Alert creation failed"))
            
    except Exception as e:
        logger.error(f"Error creating alert: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/mirror-dashboard/alert/{alert_id}/resolve")
async def resolve_mirror_alert_endpoint(alert_id: str):
    """RezolvƒÉ o alertƒÉ Mirror Agent"""
    try:
        result = await resolve_mirror_alert(alert_id)
        
        if result["ok"]:
            logger.info(f"‚úÖ Alert resolved: {alert_id}")
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "Alert resolution failed"))
            
    except Exception as e:
        logger.error(f"Error resolving alert: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-dashboard/alerts")
async def get_mirror_alerts_endpoint(site_id: Optional[str] = None, resolved: bool = False):
    """ReturneazƒÉ alertele Mirror Agents"""
    try:
        from mirror_dashboard import MirrorDashboardManager
        
        dashboard_manager = MirrorDashboardManager()
        
        if site_id:
            alerts = await dashboard_manager._get_site_alerts(site_id)
        else:
            alerts = await dashboard_manager._get_active_alerts()
        
        # FiltreazƒÉ dupƒÉ status rezolvat
        if not resolved:
            alerts = [alert for alert in alerts if not alert.resolved]
        
        logger.info(f"‚úÖ Alerts retrieved: {len(alerts)} alerts")
        return {
            "ok": True,
            "alerts": [alert.to_dict() for alert in alerts],
            "count": len(alerts),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting alerts: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-dashboard/health")
async def get_mirror_health_endpoint():
    """ReturneazƒÉ health score-ul global pentru Mirror Agents"""
    try:
        from mirror_dashboard import MirrorDashboardManager
        
        dashboard_manager = MirrorDashboardManager()
        
        # Ob»õine toate site-urile
        sites = await dashboard_manager._get_all_sites()
        
        # Ob»õine alertele active
        alerts = await dashboard_manager._get_active_alerts()
        
        # CalculeazƒÉ health score
        health_score = await dashboard_manager._calculate_health_score(sites, alerts)
        
        # CalculeazƒÉ statistici de health
        total_sites = len(sites)
        active_sites = sum(1 for site in sites if site.status == "active")
        critical_alerts = sum(1 for alert in alerts if alert.severity == "critical")
        high_alerts = sum(1 for alert in alerts if alert.severity == "high")
        
        health_status = "excellent" if health_score >= 0.9 else "good" if health_score >= 0.7 else "warning" if health_score >= 0.5 else "critical"
        
        logger.info(f"‚úÖ Health score calculated: {health_score} ({health_status})")
        return {
            "ok": True,
            "health_score": health_score,
            "health_status": health_status,
            "total_sites": total_sites,
            "active_sites": active_sites,
            "critical_alerts": critical_alerts,
            "high_alerts": high_alerts,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting health score: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ============================================================================
# MIRROR AGENT Q/A GENERATION ENDPOINTS
# ============================================================================

@app.post("/mirror-qa/generate")
async def generate_mirror_qa_endpoint(request: dict = Body(...)):
    """GenereazƒÉ set Q/A pentru Mirror Agent"""
    try:
        domain = request.get("domain")
        site_id = request.get("site_id")
        num_questions = request.get("num_questions", 20)
        
        if not domain or not site_id:
            raise HTTPException(status_code=400, detail="domain and site_id are required")
        
        # GenereazƒÉ setul Q/A
        result = await generate_mirror_qa_set(domain, site_id, num_questions)
        
        if result["ok"]:
            logger.info(f"‚úÖ Q/A set generated for {domain}: {result['qa_count']} items")
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "Q/A generation failed"))
            
    except Exception as e:
        logger.error(f"Error generating Q/A set: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-qa/stats")
async def get_mirror_qa_stats_endpoint():
    """ReturneazƒÉ statistici pentru generarea Q/A"""
    try:
        stats = get_qa_generation_stats()
        
        logger.info(f"‚úÖ Q/A generation stats retrieved: {stats['total_qa_sets']} sets")
        return {
            "ok": True,
            "stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting Q/A stats: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/mirror-qa/set/{site_id}")
async def get_mirror_qa_set_endpoint(site_id: str):
    """ReturneazƒÉ setul Q/A pentru un site"""
    try:
        db = MongoClient("mongodb://localhost:27017").mirror_qa_generation
        
        qa_set = db.qa_sets.find_one({"site_id": site_id})
        
        if qa_set:
            # Convertim ObjectId la string
            qa_set['_id'] = str(qa_set['_id'])
            
            logger.info(f"‚úÖ Q/A set retrieved for {site_id}")
            return {
                "ok": True,
                "site_id": site_id,
                "qa_set": qa_set,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            return {
                "ok": False,
                "error": f"Q/A set not found for site {site_id}",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error getting Q/A set: {e}")
        return {
            "ok": False,
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

async def get_master_content(agent):
    """Ob»õine con»õinutul agentului master din MongoDB site_content"""
    try:
        agent_id = agent["_id"]
        
        # CautƒÉ con»õinutul √Æn MongoDB site_content
        content_docs = list(db.site_content.find({'agent_id': agent_id}).limit(20))
        
        content = ""
        if content_docs:
            logger.info(f"üîç Found {len(content_docs)} content documents in MongoDB")
            for doc in content_docs:
                if doc.get('content'):
                    content += f"URL: {doc.get('url', 'Unknown')}\n"
                    content += f"Title: {doc.get('title', 'Unknown')}\n"
                    content += f"Content: {doc.get('content', '')}\n\n"
        else:
            # Fallback: cautƒÉ con»õinut generic din site_content
            logger.warning(f"‚ö†Ô∏è No content found for agent {agent_id}, searching for generic content")
            generic_docs = list(db.site_content.find({}).limit(10))
            if generic_docs:
                logger.info(f"üîç Found {len(generic_docs)} generic content documents")
                for doc in generic_docs:
                    if doc.get('content'):
                        content += f"URL: {doc.get('url', 'Unknown')}\n"
                        content += f"Title: {doc.get('title', 'Unknown')}\n"
                        content += f"Content: {doc.get('content', '')}\n\n"
        
        # Fallback la metadata agentului
        if not content:
            logger.warning(f"‚ö†Ô∏è No content found anywhere, using agent metadata")
            content = f"Nume: {agent.get('name', '')}\n"
            content += f"Domeniu: {agent.get('domain', '')}\n"
            content += f"URL: {agent.get('site_url', '')}\n"
            content += f"Industrie: {agent.get('industry', 'necunoscutƒÉ')}\n"
        else:
            logger.info(f"‚úÖ Retrieved {len(content)} characters of content from MongoDB")
        
        return content
        
    except Exception as e:
        logger.error(f"Error getting master content: {e}")
        return f"Nume: {agent.get('name', '')}\nDomeniu: {agent.get('domain', '')}\nURL: {agent.get('site_url', '')}"

@app.post("/agents/{agent_id}/strategy-chat")
async def strategy_chat(agent_id: str, message: dict):
    """Chat cu GPT-5 pentru discu»õii despre strategie"""
    try:
        # Ob»õine agentul master
        agent = db.site_agents.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "Agent not found"}
        
        # Ob»õine con»õinutul agentului din Qdrant
        master_content = await get_master_content(agent)
        
        # Folose»ôte GPT-5 pentru chat strategic
        llm = ChatOpenAI(
            model_name="gpt-5",  # GPT-5 pentru analiza strategicƒÉ
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0.3
        )
        
        prompt = f"""E»ôti GPT-5, expert √Æn strategii de business »ôi competitor analysis. 

BUSINESS MASTER:
- Nume: {agent.get('name', '')}
- Domeniu: {agent.get('domain', '')}
- URL: {agent.get('site_url', '')}

CON»öINUT SITE MASTER:
{master_content[:2000]}

MESAJUL UTILIZATORULUI:
{message.get('message', '')}

SARCINA TA:
RƒÉspunde ca un consultant strategic expert, oferind:
1. Analize concrete bazate pe con»õinutul site-ului
2. Sugestii strategice specifice pentru business-ul acestuia
3. RecomandƒÉri pentru √ÆmbunƒÉtƒÉ»õirea pozi»õiei competitive
4. Insights despre pia»õƒÉ »ôi oportunitƒÉ»õi

RƒÉspunde √Æn rom√¢nƒÉ, profesional »ôi constructiv."""
        
        response = llm.invoke([HumanMessage(content=prompt)])
        
        return {
            "ok": True,
            "response": response.content,
            "agent_name": agent.get('name', ''),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error in strategy chat: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/generate-industry-sites")
async def generate_industry_sites(agent_id: str, request: dict = Body(...)):
    """GenereazƒÉ lista completƒÉ de site-uri din industria competitivƒÉ pentru indexare"""
    try:
        # VerificƒÉ dacƒÉ agentul existƒÉ
        agent = agents_collection.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            return {"ok": False, "error": "Agent not found"}
        
        strategy = request.get('strategy', {})
        business_analysis = strategy.get('business_analysis', {})
        
        # Folose»ôte GPT-5 pentru generarea listei de site-uri din industrie
        llm = ChatOpenAI(
            model_name="gpt-5",
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0.2
        )
        
        prompt = f"""E»ôti GPT-5, expert √Æn analiza industriei »ôi identificarea competitorilor.

CONTEXTUL BUSINESS-ULUI MASTER:
- Servicii: {business_analysis.get('services_identified', [])}
- Pia»õa »õintƒÉ: {business_analysis.get('target_market', '')}
- Tehnologii: {business_analysis.get('technologies_used', [])}
- Pozi»õia √Æn pia»õƒÉ: {business_analysis.get('market_position', '')}
- Puncte forte: {business_analysis.get('strengths', [])}

SARCINA: GenereazƒÉ o listƒÉ COMPLETƒÇ de site-uri din industria competitivƒÉ pentru indexare.

ReturneazƒÉ DOAR JSON valid cu aceastƒÉ structurƒÉ:
{{
    "sites": [
        {{
            "domain": "exemplu.ro",
            "url": "https://exemplu.ro",
            "category": "Direct Competitor",
            "type": "Competitor",
            "priority": "High",
            "relevance_score": 95,
            "description": "Descrierea site-ului »ôi de ce este relevant"
        }}
    ],
    "categories": {{
        "Direct Competitor": 5,
        "Indirect Competitor": 8,
        "Supplier": 3,
        "Technology Partner": 2
    }},
    "industry_summary": "Rezumat al industriei identificate"
}}

CATEGORII DE SITE-URI:
1. Direct Competitor - competitori direc»õi cu acelea»ôi servicii
2. Indirect Competitor - competitori cu servicii similare/√Ænrudite
3. Supplier - furnizori de materiale/tehnologii
4. Technology Partner - parteneri tehnologici
5. Industry Leader - lideri √Æn industrie
6. Regional Player - jucƒÉtori regionali
7. Niche Specialist - speciali»ôti √Æn ni»ôe specifice

PRIORITATE:
- High: Competitori direc»õi, lideri √Æn industrie
- Medium: Competitori indirec»õi, jucƒÉtori regionali
- Low: Furnizori, parteneri tehnologici

GenereazƒÉ 30-50 de site-uri relevante din Rom√¢nia »ôi interna»õionale."""

        response = llm.invoke([HumanMessage(content=prompt)])
        response_content = response.content.strip()
        
        # Extrage JSON din rƒÉspuns
        if response_content.startswith('```json'):
            json_start = response_content.find('```json') + 7
            json_end = response_content.find('```', json_start)
            if json_end > json_start:
                response_content = response_content[json_start:json_end].strip()
        elif response_content.startswith('```'):
            json_start = response_content.find('```') + 3
            json_end = response_content.find('```', json_start)
            if json_end > json_start:
                response_content = response_content[json_start:json_end].strip()
        
        industry_sites = json.loads(response_content)
        
        logger.info(f"üè≠ Generated industry sites list: {len(industry_sites.get('sites', []))} sites")
        
        return {
            "ok": True,
            "industry_sites": industry_sites,
            "agent_id": agent_id,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error generating industry sites: {e}")
        return {"ok": False, "error": str(e)}

# ===================== EXPERT ADVISOR MODE - SPRINT 1 ENDPOINTS =====================

@app.post("/agents/{agent_id}/expert-advisor/initialize")
async def initialize_expert_advisor(agent_id: str):
    """Ini»õializeazƒÉ Expert Advisor Mode pentru un agent"""
    try:
        logger.info(f"ü§ñ Initializing Expert Advisor Mode for agent {agent_id}")
        
        # 1. Ini»õializeazƒÉ Knowledge Architecture
        knowledge_arch = get_knowledge_architecture()
        
        # 2. Ini»õializeazƒÉ AI Roles Orchestrator
        ai_orchestrator = get_ai_roles_orchestrator()
        
        # 3. Ini»õializeazƒÉ Industry Aggregator
        industry_agg = get_industry_aggregator()
        
        # 4. Ini»õializeazƒÉ Gap Analyzer
        gap_analyzer = get_gap_analyzer()
        
        # 5. RuleazƒÉ procesul complet
        # a) Agregare industrie
        industry_result = await industry_agg.aggregate_industry_knowledge(agent_id)
        
        # b) AnalizƒÉ gap-uri
        gaps_result = await gap_analyzer.analyze_gaps(agent_id)
        
        # c) GenereazƒÉ hinturi
        knowledge_arch = get_knowledge_architecture()
        hints = await knowledge_arch.generate_hints(agent_id)
        
        return {
            "ok": True,
            "message": "Expert Advisor Mode initialized successfully",
            "results": {
                "industry_aggregation": industry_result,
                "gap_analysis": gaps_result,
                "hints_generated": len(hints)
            }
        }
        
    except Exception as e:
        logger.error(f"Error initializing Expert Advisor Mode: {e}")
        return {"ok": False, "error": str(e)}
@app.get("/agents/{agent_id}/expert-advisor/status")
async def get_expert_advisor_status(agent_id: str):
    """Ob»õine statusul Expert Advisor Mode"""
    try:
        # Ob»õine statusul AI-urilor
        ai_orchestrator = get_ai_roles_orchestrator()
        ai_status = await ai_orchestrator.get_ai_status(agent_id)
        
        # Ob»õine statusul sincronizƒÉrii
        sync_status = await ai_orchestrator.get_sync_status(agent_id)
        
        # Ob»õine quick wins
        knowledge_arch = get_knowledge_architecture()
        quick_wins = await knowledge_arch.get_quick_wins(agent_id)
        
        # Ob»õine groundedness score
        groundedness = await knowledge_arch.get_groundedness_score(agent_id)
        
        return {
            "ok": True,
            "status": {
                "ai_status": ai_status,
                "sync_status": sync_status,
                "quick_wins_count": len(quick_wins),
                "groundedness_score": groundedness,
                "expert_advisor_active": True
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting Expert Advisor status: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/agents/{agent_id}/expert-advisor/hints")
async def get_expert_hints(agent_id: str):
    """Ob»õine hinturile generate de Expert Advisor"""
    try:
        knowledge_arch = get_knowledge_architecture()
        
        # Ob»õine toate hinturile
        hints = list(db.hints.find({"agent_id": agent_id}).sort("created_at", -1))
        
        # Converte»ôte ObjectId la string
        for hint in hints:
            hint["_id"] = str(hint["_id"])
            if "created_at" in hint:
                hint["created_at"] = hint["created_at"].isoformat()
        
        # Ob»õine quick wins
        quick_wins = await knowledge_arch.get_quick_wins(agent_id)
        
        return {
            "ok": True,
            "hints": hints,
            "quick_wins": [asdict(win) for win in quick_wins],
            "total_hints": len(hints)
        }
        
    except Exception as e:
        logger.error(f"Error getting expert hints: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/expert-advisor/feedback")
async def submit_expert_feedback(agent_id: str, request: dict = Body(...)):
    """Trimite feedback pentru hinturi"""
    try:
        hint_id = request.get("hint_id")
        feedback = request.get("feedback", "")
        admin_action = request.get("action", "pending")  # "accepted", "dismissed", "needs_evidence"
        
        knowledge_arch = get_knowledge_architecture()
        result = await knowledge_arch.process_feedback(hint_id, feedback, admin_action)
        
        if result:
            return {
                "ok": True,
                "message": f"Feedback processed: {admin_action}",
                "hint_id": hint_id
            }
        else:
            return {"ok": False, "error": "Failed to process feedback"}
            
    except Exception as e:
        logger.error(f"Error submitting feedback: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/agents/{agent_id}/expert-advisor/industry-map")
async def get_industry_map(agent_id: str):
    """Ob»õine Industry Map-ul pentru agent"""
    try:
        industry_agg = get_industry_aggregator()
        industry_map = await industry_agg.get_industry_map(agent_id)
        
        if industry_map:
            # Converte»ôte ObjectId la string
            industry_map["_id"] = str(industry_map["_id"])
            if "created_at" in industry_map:
                industry_map["created_at"] = industry_map["created_at"].isoformat()
            if "updated_at" in industry_map:
                industry_map["updated_at"] = industry_map["updated_at"].isoformat()
        
        return {
            "ok": True,
            "industry_map": industry_map
        }
        
    except Exception as e:
        logger.error(f"Error getting industry map: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/agents/{agent_id}/expert-advisor/gaps")
async def get_expert_gaps(agent_id: str):
    """Ob»õine gap-urile identificate de Expert Advisor"""
    try:
        gaps = list(db.gaps.find({"agent_id": agent_id}).sort("total_score", -1))
        
        # Converte»ôte ObjectId la string
        for gap in gaps:
            gap["_id"] = str(gap["_id"])
            if "created_at" in gap:
                gap["created_at"] = gap["created_at"].isoformat()
        
        # CategorizeazƒÉ gap-urile
        by_priority = {"high": [], "medium": [], "low": []}
        quick_wins = []
        
        for gap in gaps:
            priority = gap.get("priority", "low")
            by_priority[priority].append(gap)
            
            if gap.get("quick_win", False):
                quick_wins.append(gap)
        
        return {
            "ok": True,
            "gaps": gaps,
            "by_priority": by_priority,
            "quick_wins": quick_wins,
            "total_gaps": len(gaps)
        }
        
    except Exception as e:
        logger.error(f"Error getting expert gaps: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/expert-advisor/force-sync")
async def force_expert_sync(agent_id: str):
    """For»õeazƒÉ sincronizarea Expert Advisor"""
    try:
        # For»õeazƒÉ sincronizarea AI-urilor
        ai_orchestrator = get_ai_roles_orchestrator()
        sync_result = await ai_orchestrator.get_sync_status(agent_id)
        
        # For»õeazƒÉ re-analiza gap-urilor
        gap_analyzer = get_gap_analyzer()
        gaps_result = await gap_analyzer.analyze_gaps(agent_id)
        
        return {
            "ok": True,
            "message": "Expert Advisor sync completed",
            "sync_result": sync_result,
            "gaps_updated": gaps_result
        }
        
    except Exception as e:
        logger.error(f"Error in force sync: {e}")
        return {"ok": False, "error": str(e)}

# ===================== DUAL ORCHESTRATOR ENDPOINTS =====================

@app.post("/agents/{agent_id}/dual-report")
async def generate_dual_report(agent_id: str):
    """GenereazƒÉ raport dual GPT-5 + Qwen"""
    try:
        orchestrator = get_dual_orchestrator()
        report = await orchestrator.generate_dual_report(agent_id)
        
        return {
            "ok": True,
            "report": {
                "agent_id": report.agent_id,
                "gpt5_insights": report.gpt5_insights,
                "qwen_insights": report.qwen_insights,
                "industry_map": report.industry_map,
                "learning_progress": report.learning_progress,
                "recommendations": report.recommendations,
                "timestamp": report.timestamp.isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"Error generating dual report: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/agents/{agent_id}/dual-status")
async def get_dual_status(agent_id: str):
    """Ob»õine statusul sistemului dual GPT-5 + Qwen"""
    try:
        orchestrator = get_dual_orchestrator()
        status = await orchestrator.get_dual_status(agent_id)
        
        return {
            "ok": True,
            "status": status
        }
        
    except Exception as e:
        logger.error(f"Error getting dual status: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/force-sync")
async def force_sync_dual(agent_id: str):
    """For»õeazƒÉ sincronizarea √Æntre GPT-5 »ôi Qwen"""
    try:
        orchestrator = get_dual_orchestrator()
        sync_result = await orchestrator.force_sync(agent_id)
        
        return {
            "ok": True,
            "sync_result": sync_result
        }
        
    except Exception as e:
        logger.error(f"Error in force sync: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/agents/{agent_id}/add-learning")
async def add_to_learning_queue(agent_id: str, request: dict = Body(...)):
    """AdaugƒÉ con»õinut √Æn learning queue pentru Qwen"""
    try:
        orchestrator = get_dual_orchestrator()
        
        content = request.get('content', '')
        source = request.get('source', 'user')
        metadata = request.get('metadata', {})
        
        # AdaugƒÉ agent_id √Æn metadata
        metadata['agent_id'] = agent_id
        
        await orchestrator.add_to_learning_queue(content, source, metadata)
        
        return {
            "ok": True,
            "message": "Content added to learning queue",
            "agent_id": agent_id
        }
        
    except Exception as e:
        logger.error(f"Error adding to learning queue: {e}")
        return {"ok": False, "error": str(e)}

@app.get("/agents/{agent_id}/learning-queue")
async def get_learning_queue(agent_id: str):
    """Ob»õine learning queue pentru un agent"""
    try:
        entries = list(db.learning_queue.find({
            "agent_id": agent_id
        }).sort("timestamp", -1).limit(50))
        
        # Converte»ôte ObjectId la string
        for entry in entries:
            entry['_id'] = str(entry['_id'])
            if 'timestamp' in entry:
                entry['timestamp'] = entry['timestamp'].isoformat()
        
        return {
            "ok": True,
            "entries": entries,
            "total": len(entries)
        }
        
    except Exception as e:
        logger.error(f"Error getting learning queue: {e}")
        return {"ok": False, "error": str(e)}