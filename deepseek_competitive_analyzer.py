#!/usr/bin/env python3
"""
DeepSeek Competitive Analyzer - PrimeÈ™te TOT contextul agentului È™i genereazÄƒ analizÄƒ strategicÄƒ
"""

import json
import logging
from typing import Dict, List, Any
from pymongo import MongoClient
from bson import ObjectId

# Import modulele existente
from qdrant_context_enhancer import get_context_enhancer
from llm_orchestrator import get_orchestrator

logger = logging.getLogger(__name__)

class DeepSeekCompetitiveAnalyzer:
    """
    Analizator competitiv care foloseÈ™te DeepSeek cu TOT contextul disponibil
    """
    
    def __init__(self):
        self.mongo_client = MongoClient("mongodb://localhost:27017/")
        self.db = self.mongo_client["ai_agents_db"]
        self.context_enhancer = get_context_enhancer()
        self.llm = get_orchestrator()  # ğŸ­ LLM Orchestrator cu DeepSeek + fallback
        
    def get_full_agent_context(self, agent_id: str) -> Dict[str, Any]:
        """
        ObÈ›ine TOATE datele despre agent din MongoDB + Qdrant
        
        Returns:
            Dict cu:
            - agent_info: date de bazÄƒ
            - content_chunks: conÈ›inut complet din MongoDB
            - vector_context: context semantic din Qdrant
            - services: servicii identificate
            - contact_info: informaÈ›ii de contact
        """
        agent = self.db.site_agents.find_one({"_id": ObjectId(agent_id)})
        if not agent:
            raise ValueError(f"Agent {agent_id} not found")
        
        # 1. Date de bazÄƒ agent
        agent_info = {
            "domain": agent.get("domain"),
            "site_url": agent.get("site_url"),
            "name": agent.get("name"),
            "business_type": agent.get("business_type", "general"),
            "status": agent.get("status"),
            "validation_passed": agent.get("validation_passed")
        }
        
        # 2. ConÈ›inut complet din MongoDB
        content_chunks = list(self.db.site_content.find({"agent_id": ObjectId(agent_id)}))
        total_content = "\n\n".join([
            chunk.get("content", "") 
            for chunk in content_chunks 
            if chunk.get("content")
        ])
        
        # 3. Context semantic din Qdrant (topics relevante)
        vector_context = {}
        try:
            # ConstruieÈ™te numele corect al collection-ului
            domain = agent.get("domain", "").replace(".", "_")
            collection_name = f"construction_{domain}"
            
            logger.info(f"ğŸ” Folosesc Qdrant collection: {collection_name}")
            
            topics = [
                "servicii È™i produse principale",
                "puncte forte È™i avantaje",
                "clienÈ›i È™i piaÈ›Äƒ È›intÄƒ",
                "domenii de activitate",
                "expertize È™i specializÄƒri"
            ]
            
            for topic in topics:
                try:
                    contexts = self.context_enhancer.get_context_for_query(
                        query=topic,
                        collection_name=collection_name,
                        top_k=3
                    )
                    vector_context[topic] = contexts
                except Exception as topic_error:
                    logger.warning(f"Could not get context for topic '{topic}': {topic_error}")
                    vector_context[topic] = []
                
        except Exception as e:
            logger.warning(f"Could not get vector context: {e}")
            vector_context = {}
        
        # 4. Servicii
        services = agent.get("services", [])
        
        # 5. Contact info
        contact_info = agent.get("contact_info", {})
        
        # 6. Statistici
        stats = {
            "total_chunks": len(content_chunks),
            "total_characters": len(total_content),
            "services_count": len(services),
            "has_vector_context": bool(vector_context)
        }
        
        return {
            "agent_info": agent_info,
            "content_full": total_content[:50000],  # LimitÄƒ pentru token efficiency
            "content_chunks_count": len(content_chunks),
            "vector_context": vector_context,
            "services": services,
            "contact_info": contact_info,
            "stats": stats
        }
    
    def analyze_for_competition_discovery(self, agent_id: str) -> Dict[str, Any]:
        """
        TASK 1: Descompune site-ul Ã®n subdomenii È™i genereazÄƒ cuvinte cheie pentru Google search
        
        Returns:
            {
                "subdomains": [
                    {
                        "name": "...",
                        "description": "...",
                        "keywords": ["...", "..."]
                    }
                ],
                "overall_keywords": ["...", "..."],
                "industry": "...",
                "target_market": "..."
            }
        """
        logger.info(f"ğŸ¯ Analizez agent {agent_id} pentru descoperire competiÈ›ie...")
        
        # ObÈ›ine TOT contextul
        full_context = self.get_full_agent_context(agent_id)
        
        # ConstruieÈ™te prompt pentru DeepSeek
        prompt = self._build_competition_discovery_prompt(full_context)
        
        # Trimite la DeepSeek
        logger.info(f"ğŸ“¤ Trimit context complet cÄƒtre DeepSeek ({len(prompt)} caractere)")
        
        try:
            # ğŸ­ Folosim Orchestrator cu DeepSeek + fallback
            response = self.llm.chat(
                messages=[
                    {
                        "role": "system",
                        "content": "EÈ™ti un expert Ã®n analizÄƒ competitivÄƒ È™i strategii de marketing. "
                                   "Analizezi site-uri web È™i identifici domenii de activitate, servicii, "
                                   "È™i generezi cuvinte cheie pentru descoperirea concurenÈ›ei pe Google."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=4000,
                temperature=0.7
            )
            
            # LLMOrchestrator.chat() returneazÄƒ STRING direct, nu dict
            if isinstance(response, str):
                result = self._parse_deepseek_response(response)
            elif isinstance(response, dict) and response.get("success"):
                # Fallback pentru cazul Ã®n care ar returna dict
                result = self._parse_deepseek_response(response["content"])
            else:
                raise Exception(f"LLM failed: Invalid response type {type(response)}")
            
            # SalveazÄƒ analiza Ã®n MongoDB
            self._save_analysis(agent_id, result)
            
            logger.info(f"âœ… AnalizÄƒ completÄƒ pentru {agent_id}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Eroare la analiza DeepSeek: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _build_competition_discovery_prompt(self, context: Dict[str, Any]) -> str:
        """ConstruieÈ™te prompt structurat pentru DeepSeek"""
        
        agent_info = context["agent_info"]
        content = context["content_full"]
        services = context["services"]
        vector_context = context["vector_context"]
        
        # ConstruieÈ™te secÈ›iunea de servicii
        services_text = ""
        if services:
            services_text = "\n**SERVICII IDENTIFICATE:**\n"
            for i, svc in enumerate(services[:20], 1):
                if isinstance(svc, dict):
                    name = svc.get("service_name") or svc.get("name", "")
                    desc = svc.get("description", "")
                    services_text += f"{i}. {name}\n"
                    if desc:
                        services_text += f"   Descriere: {desc[:200]}\n"
                else:
                    services_text += f"{i}. {svc}\n"
        
        # ConstruieÈ™te context semantic
        semantic_context = ""
        if vector_context:
            semantic_context = "\n**CONTEXT SEMANTIC DIN QDRANT (cele mai relevante informaÈ›ii):**\n"
            for topic, contexts in vector_context.items():
                if contexts:
                    semantic_context += f"\nâ€¢ {topic.upper()}:\n"
                    for ctx in contexts[:2]:
                        semantic_context += f"  - {ctx['text'][:300]}...\n"
        
        prompt = f"""AnalizeazÄƒ acest site web pentru a identifica subdomenii de activitate È™i cuvinte cheie pentru Google search.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**INFORMAÈšII DESPRE SITE:**
- Domeniu: {agent_info['domain']}
- URL: {agent_info['site_url']}
- Nume: {agent_info['name']}
- Tip business: {agent_info['business_type']}

{services_text}

{semantic_context}

**CONÈšINUT COMPLET AL SITE-ULUI:**
{content[:30000]}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**TASK: DESCOMPUNERE ÃN SUBDOMENII È˜I GENERARE CUVINTE CHEIE**

Te rog sÄƒ analizezi site-ul È™i sÄƒ returnezi un JSON cu urmÄƒtoarea structurÄƒ:

{{
  "industry": "numele industriei principale (ex: protecÈ›ie la foc, construcÈ›ii, etc)",
  "target_market": "piaÈ›a È›intÄƒ principalÄƒ",
  "subdomains": [
    {{
      "name": "Nume subdomeniu (ex: ProtecÈ›ie pasivÄƒ la foc)",
      "description": "Descriere frumoasÄƒ È™i detaliatÄƒ a subdomeniului, ce servicii include, pentru cine e destinat, ce probleme rezolvÄƒ (2-3 propoziÈ›ii)",
      "main_services": ["serviciu1", "serviciu2"],
      "keywords": [
        "cuvÃ¢nt cheie 1 pentru Google search",
        "cuvÃ¢nt cheie 2 pentru Google search",
        "cuvÃ¢nt cheie 3 pentru Google search"
      ]
    }}
  ],
  "overall_keywords": [
    "cuvinte cheie generale pentru toatÄƒ industria",
    "folosite pentru a gÄƒsi competitori generali"
  ],
  "competitive_positioning": "Cum se poziÈ›ioneazÄƒ compania Ã®n piaÈ›Äƒ (1-2 propoziÈ›ii)"
}}

**INSTRUCÈšIUNI:**
1. IdentificÄƒ 3-7 subdomenii principale de activitate
2. Pentru fiecare subdomeniu, scrie o descriere clarÄƒ È™i atrÄƒgÄƒtoare
3. GenereazÄƒ 5-10 cuvinte cheie SPECIFICE pentru fiecare subdomeniu
4. Cuvintele cheie trebuie sÄƒ fie:
   - Specifice domeniului (nu generice)
   - Utile pentru Google search (combinÄƒ serviciu + industrie + locaÈ›ie dacÄƒ e relevant)
   - Ãn romÃ¢nÄƒ (dacÄƒ site-ul e Ã®n romÃ¢nÄƒ)
   - VariaÈ›ii: singular/plural, sinonime
5. AdaugÄƒ 10-15 cuvinte cheie generale pentru toatÄƒ industria

**EXEMPLU DE CUVINTE CHEIE BUNE:**
- "protecÈ›ie la foc structuri metalice BucureÈ™ti"
- "termoprotecÈ›ie vopsea intumescentÄƒ"
- "ignifugare lemn certificatÄƒ"
- "sisteme antiincendiu pasive"

**RETURNEAZÄ‚ DOAR JSON-UL, FÄ‚RÄ‚ MARKDOWN SAU ALT TEXT!**
"""
        
        return prompt
    
    def _parse_deepseek_response(self, response: Any) -> Dict[str, Any]:
        """ParseazÄƒ rÄƒspunsul de la DeepSeek"""
        
        # Extrage conÈ›inutul
        content = ""
        if isinstance(response, dict):
            if "data" in response:
                choices = response["data"].get("choices", [])
                if choices:
                    content = choices[0].get("message", {}).get("content", "")
            elif "content" in response:
                content = response["content"]
            else:
                content = str(response)
        else:
            content = str(response)
        
        # CurÄƒÈ›Äƒ JSON (eliminÄƒ markdown code blocks dacÄƒ existÄƒ)
        content = content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()
        
        # ParseazÄƒ JSON
        try:
            result = json.loads(content)
            return result
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            logger.error(f"Content: {content[:500]}")
            # ReturneazÄƒ structurÄƒ minimÄƒ
            return {
                "industry": "unknown",
                "target_market": "unknown",
                "subdomains": [],
                "overall_keywords": [],
                "competitive_positioning": "AnalizÄƒ incompletÄƒ",
                "_raw_response": content[:1000]
            }
    
    def _save_analysis(self, agent_id: str, analysis: Dict[str, Any]):
        """SalveazÄƒ analiza Ã®n MongoDB"""
        from datetime import datetime, timezone
        
        doc = {
            "agent_id": ObjectId(agent_id),
            "analysis_type": "competition_discovery",
            "analysis_data": analysis,
            "created_at": datetime.now(timezone.utc),
            "status": "completed"
        }
        
        # SalveazÄƒ Ã®n colecÈ›ia competitive_analysis
        self.db.competitive_analysis.update_one(
            {
                "agent_id": ObjectId(agent_id),
                "analysis_type": "competition_discovery"
            },
            {"$set": doc},
            upsert=True
        )
        
        # âœ… FIX: Extrage È™i salveazÄƒ keywords Ã®n documentul agentului
        all_keywords = []
        subdomains_list = []
        keywords_per_subdomain = {}
        
        # Extrage keywords din subdomenii
        for subdomain in analysis.get("subdomains", []):
            subdomain_name = subdomain.get("name", "")
            subdomain_keywords = subdomain.get("keywords", [])
            
            if subdomain_name:
                subdomains_list.append(subdomain_name)
                keywords_per_subdomain[subdomain_name] = subdomain_keywords
                all_keywords.extend(subdomain_keywords)
        
        # AdaugÄƒ keywords generale
        overall_keywords = analysis.get("overall_keywords", [])
        all_keywords.extend(overall_keywords)
        
        # Update agent cu keywords, subdomenii È™i industrie
        update_doc = {
            "keywords": all_keywords,
            "subdomains": subdomains_list,
            "keywords_per_subdomain": keywords_per_subdomain,
            "overall_keywords": overall_keywords,
            "industry": analysis.get("industry", ""),
            "target_market": analysis.get("target_market", ""),
            "competitive_positioning": analysis.get("competitive_positioning", ""),
            "keywords_generated_at": datetime.now(timezone.utc),
            "status": "keywords_generated"
        }
        
        # Update Ã®n ambele colecÈ›ii (site_agents È™i agents)
        result = self.db.site_agents.update_one(
            {"_id": ObjectId(agent_id)},
            {"$set": update_doc}
        )
        
        if result.matched_count == 0:
            # ÃncearcÄƒ Ã®n colecÈ›ia agents dacÄƒ nu e Ã®n site_agents
            self.db.agents.update_one(
                {"_id": ObjectId(agent_id)},
                {"$set": update_doc}
            )
        
        logger.info(f"ğŸ’¾ AnalizÄƒ salvatÄƒ Ã®n MongoDB pentru agent {agent_id}")
        logger.info(f"âœ… Salvate {len(all_keywords)} keywords ({len(subdomains_list)} subdomenii) Ã®n agent!")


# Factory function
def get_analyzer():
    """Get or create analyzer instance"""
    return DeepSeekCompetitiveAnalyzer()
