# ğŸ¯ PLAN ÃMBUNÄ‚TÄ‚ÈšIRE PROCES CREARE AGENÈšI

## Obiectiv
Proces robust de creare agenÈ›i care:
- âœ… FuncÈ›ioneazÄƒ pe ORICE tip de site
- âœ… Are verificÄƒri la fiecare pas
- âœ… GaranteazÄƒ cÄƒ DeepSeek primeÈ™te date de calitate
- âœ… Se auto-valideazÄƒ Ã®nainte de marcare ca "ready"

---

## ğŸ“‹ ETAPE IMPLEMENTARE

### 1. ÃMBUNÄ‚TÄ‚ÈšIRE CRAWLING (site_agent_creator.py)
**Status:** ğŸ”„ IN PROGRESS

**Probleme actuale:**
- âŒ Unele site-uri dau 0 chunks (protectiaantifoc.ro, romstal.ro)
- âŒ Crawler se blocheazÄƒ pe site-uri dinamice
- âŒ Nu detecteazÄƒ cÃ¢nd un site e down/inaccesibil
- âŒ Nu extrage corect serviciile

**SoluÈ›ii:**
- âœ… AdaugÄƒ multiple strategii de scraping:
  1. Playwright (actual)
  2. Requests + BeautifulSoup (fallback)
  3. Selenium (fallback final)
- âœ… DetecteazÄƒ tip site (static, SPA, WordPress, etc.)
- âœ… Timeout-uri mai scurte (5s per paginÄƒ)
- âœ… Retry logic pentru fiecare paginÄƒ
- âœ… Verificare dupÄƒ fiecare paginÄƒ: text_length > 100

**Cod de adÄƒugat:**
```python
async def smart_crawl_with_fallbacks(url, websocket):
    """
    ÃncearcÄƒ mai multe strategii de crawling
    """
    strategies = [
        ("playwright", crawl_with_playwright),
        ("requests", crawl_with_requests),
        ("selenium", crawl_with_selenium)
    ]
    
    for name, func in strategies:
        try:
            result = await func(url, websocket)
            if result['chunks'] >= 2:  # Minim 2 chunks
                return result
        except Exception as e:
            logger.warning(f"Strategy {name} failed: {e}")
            continue
    
    raise ValueError("Toate strategiile de crawling au eÈ™uat")
```

---

### 2. VERIFICÄ‚RI OBLIGATORII LA FIECARE PAS
**Status:** â³ PENDING

**VerificÄƒri de adÄƒugat:**

#### A. DupÄƒ Auto Site Extractor:
```python
# Verificare: Avem minim 5 servicii?
if len(extracted_services) < 2:
    logger.warning("Prea puÈ›ine servicii extrase, Ã®ncerc analiza manualÄƒ")
    # Fallback: AnalizeazÄƒ conÈ›inut cu Qwen pentru a detecta servicii
```

#### B. DupÄƒ Crawling:
```python
# Verificare: Avem minim 2 pagini cu conÈ›inut?
valid_pages = [p for p in pages if len(p['text']) > 100]
if len(valid_pages) < 2:
    raise ValueError(f"Site incomplet: doar {len(valid_pages)} pagini valide")
```

#### C. DupÄƒ Salvare MongoDB:
```python
# Verificare: Datele sunt Ã®n DB?
saved_chunks = db.site_content.count_documents({"agent_id": agent_id})
if saved_chunks < 2:
    raise ValueError(f"Salvare incompletÄƒ: doar {saved_chunks} chunks Ã®n DB")
```

#### D. DupÄƒ Creare Qdrant:
```python
# Verificare: Vectorii sunt Ã®n Qdrant? (dacÄƒ disponibil)
if qdrant_available:
    vector_count = check_qdrant_vectors(collection_name)
    if vector_count < saved_chunks - 1:  # TolerÄƒm 1 diferenÈ›Äƒ
        logger.warning(f"Vectori incompleÈ›i: {vector_count}/{saved_chunks}")
```

---

### 3. ASIGURARE CALITATE PENTRU DEEPSEEK
**Status:** â³ PENDING

**CerinÈ›e minime pentru un agent valid:**
```python
AGENT_QUALITY_REQUIREMENTS = {
    "min_content_chunks": 2,
    "min_total_characters": 1000,
    "min_services": 1,
    "required_fields": ["domain", "site_url", "status", "created_at"],
    "required_metadata": ["business_type", "contact_info"]
}

def validate_agent_quality(agent_id):
    """
    VerificÄƒ dacÄƒ agentul respectÄƒ cerinÈ›ele minime
    """
    agent = db.site_agents.find_one({"_id": ObjectId(agent_id)})
    content = list(db.site_content.find({"agent_id": ObjectId(agent_id)}))
    
    checks = {
        "chunks": len(content) >= 2,
        "content_length": sum(len(c.get('content', '')) for c in content) >= 1000,
        "services": len(agent.get('services', [])) >= 1,
        "fields": all(agent.get(f) for f in ["domain", "site_url", "status"]),
        "created_at": agent.get('created_at') is not None
    }
    
    return all(checks.values()), checks
```

---

### 4. RETRY LOGIC & FALLBACK-URI
**Status:** â³ PENDING

**Puncte critice cu retry:**

#### A. Crawling cu Playwright:
```python
@retry(max_attempts=3, delay=2)
async def crawl_page_with_retry(page, url):
    """Retry pentru fiecare paginÄƒ"""
    return await page.goto(url, timeout=10000)
```

#### B. Salvare MongoDB:
```python
@retry(max_attempts=5, delay=0.5)
def save_to_mongo_with_retry(collection, data):
    """Retry pentru operaÈ›ii MongoDB"""
    return collection.insert_one(data)
```

#### C. Qdrant (dacÄƒ disponibil):
```python
def create_qdrant_with_fallback(chunks, collection_name):
    """ÃncearcÄƒ Qdrant, continuÄƒ fÄƒrÄƒ el dacÄƒ eÈ™ueazÄƒ"""
    try:
        return create_qdrant_collection(chunks, collection_name)
    except Exception as e:
        logger.warning(f"Qdrant failed: {e}, continuÄƒm fÄƒrÄƒ vectori")
        return {"success": False, "reason": str(e)}
```

---

### 5. TESTARE AUTOMATÄ‚ CU DIVERSE SITE-URI
**Status:** â³ PENDING

**Lista site-uri de test:**
```python
TEST_SITES = [
    # Site-uri simple
    "https://example.com",
    "https://www.ropaintsolutions.ro",
    
    # Site-uri WordPress
    "https://www.tehnica-antifoc.ro",
    
    # Site-uri SPA (React/Vue)
    "https://www.rezistentlafoc.ro",
    
    # Site-uri cu anti-scraping
    "https://www.emag.ro",
    
    # Site-uri cu Cloudflare
    "https://www.pcgarage.ro"
]
```

**Script de testare:**
```python
async def test_agent_creation_pipeline():
    results = {"success": [], "failed": []}
    
    for url in TEST_SITES:
        try:
            agent = await create_agent_logic(url)
            is_valid, checks = validate_agent_quality(agent['agent_id'])
            
            if is_valid:
                results["success"].append(url)
            else:
                results["failed"].append({"url": url, "checks": checks})
        except Exception as e:
            results["failed"].append({"url": url, "error": str(e)})
    
    return results
```

---

### 6. VALIDARE AUTOMATÄ‚ ÃNAINTE DE "READY"
**Status:** â³ PENDING

**Modificare Ã®n `create_agent_logic`:**
```python
async def create_agent_logic(url, ...):
    # ... procesare ...
    
    # â­ VERIFICARE FINALÄ‚ ÃNAINTE DE MARCARE CA READY
    is_valid, checks = validate_agent_quality(agent_id)
    
    if not is_valid:
        logger.error(f"Agent {agent_id} NU respectÄƒ cerinÈ›ele: {checks}")
        db.site_agents.update_one(
            {"_id": ObjectId(agent_id)},
            {"$set": {
                "status": "incomplete",
                "validation_errors": checks
            }}
        )
        raise ValueError(f"Agent incomplet: {checks}")
    
    # Doar dacÄƒ e valid, marcheazÄƒ ca ready
    db.site_agents.update_one(
        {"_id": ObjectId(agent_id)},
        {"$set": {"status": "ready"}}
    )
```

---

## ğŸ“Š METRICI DE SUCCES

Un agent este considerat **VALID** dacÄƒ:
- âœ… Are minimum 2 chunks de conÈ›inut
- âœ… Total caractere > 1000
- âœ… Are minimum 1 serviciu detectat
- âœ… Are `created_at` timestamp
- âœ… Are `domain`, `site_url`, `status`
- âœ… DeepSeek poate genera o strategie (nu eÈ™ueazÄƒ)

---

## ğŸš€ PLAN DE EXECUÈšIE

1. âœ… Creare document plan (ACEST FIÈ˜IER)
2. ğŸ”„ Implementare `smart_crawl_with_fallbacks`
3. â³ AdÄƒugare verificÄƒri la fiecare pas
4. â³ Implementare `validate_agent_quality`
5. â³ Testare cu 10 site-uri diverse
6. â³ AjustÄƒri finale bazate pe rezultate
7. â³ DocumentaÈ›ie actualizatÄƒ

---

**Data Ã®nceput:** 2025-11-07
**Target finalizare:** 2025-11-07 (acelaÈ™i task session)

